id,date,text
4,2023-06-10T23:22:16,"qlora efficient finetuning of quantized llms первый пост будет посвящен работе, относительно свежей по меркам deep learning, где, как известно, статьи отличаются небольшим сроком годности. в последние несколько лет мы стали свидетелями бешеного прогресса глубокого обучения во всех его ипостасях, проявлениях и приложениях, и флагманом прогресса выступает область nlp natural language processing . разрастающиеся в ширину и глубину модели, обученные на колосалльных обьемах данных, дополненные современными плюшками и прибамбасами в виде instruction finetuning, rlhf reinforcement learning on human feedback и прочими другими, демонстируют способности, которые еще не так давно казались бы чем-то из разряда научной фантастики. однако выдающиеся возможности языковых моделей не даются за бесплатно, всему есть цена. характерный размер современных языковых моделей используемых в новомодных чатботах по типу alpaca, vicuna и прочих представителей животного мира исчисляется порядка десятков миллиардов параметров, что делает традиционный инференс на пользовательских gpu и любимом колабе затруднительным . отсюда возникает естественный вопрос на сжатие таких моделей, и квантизация представление чисел в более низкой точности на текущий момент, по всей видимости, является наиболее ходовым решением. но если хочется пользоваться не просто готовой моделькой, а еще дообучить на какую-то специфичную задачу? как бы много данных не видела сеть в процессе своего обучения, для достижения хорошего качества в конкретном приложении обыкновенно требуется дообучать модель, и существует множество способов и вариантов эффективного по памяти и вычислительным ресурсам техник по дообучению большихх сетей - различные адаптеры среди них наиболее популярна lora, заслуживающая отдельного разбора , prompt tuning, in context learning и иже с ними. и в статье, которой посвящен сегодняшний выпуск, предложили эффективный алгоритм квантования весов и последующую процедуру дообучения модели под чатбота, в результате которой получается весьма годный ассистент guanaco, который, как утверждается, бьет всех open-source конкурентов. если перечислить списком основные результаты статьи, выходит что-то такое 1. новый тип для хранения весов 4-bit normalfloat nf4 . 2. двойное квантование double quantization . 3. paged отпимизаторы 4. экстенстивная валидация модели на разных бенчмарках и исследование самих бенчмарков"
5,2023-06-10T23:26:08,"теперь пройдемся по каждому из пунктов подробнее. 1 при квантовании весов, каждый параметр принимает одно из небольшого количества возможных значений. скажем, если квантование в 4 бита, то у нас 2 4 16 вариантов. но, как выбрать эти значения? самая очевидная и по существу используемая на практике стратегия - задать максимальное и минимальное значение, которое может принимать вес, и разбить интервал на одинаковые промежутки. и если точки распределены равномерно, то это и правда оптимальный выбор. однако на практике веса скорее распределены в соотвествии с нормальным распределением, имеющим колоколообразную форму. то есть веса обычно кучкуются ближе к центру интервала и вероятность их встретить убывает к краям интервала. отсюда возникает идея выбрать интервалы таким образом, чтобы в каждый интервал попадала примерно одна и та же доля весов. на языке высокой науки значения, которые случайная величина не превышает с заданной вероятностью называется квантилями распределения. например, 10 квантиль - такой значение, что слева от него лежит 10 массы распределения. для некоторых случайных величин квантили ищутся легко, как для равномерного распределения, но для нормального распределения это табличные значения, которые приходится получать численными методами."
7,2023-06-10T23:26:45,"авторы наглядно демонстрируют, что normalfloat4 дает заметно большее качество по сравнению с обычным float4."
9,2023-06-10T23:27:03,"2 следующее ноу-хау данной работы - double quantization двойное квантование из квантовой механики здесь не причем! . при квантовании группы значений каждое число представляется в виде целого числа, домноженного на некоторое вещественное, называемое масштабом scale . со сдвигом от некоторого значения zero point в случае ассиметричной квантизации в данной статье рассматривается симметричная квантизация ."
10,2023-06-10T23:27:47,вот так мы квантуем получаем целые числа
12,2023-06-10T23:28:30,а вот так получаем значения с плавающей точкой из квантованных
14,2023-06-10T23:28:57,"группы для квантования могут быть различного размера - можно зафиксировать один масштаб на весь тензор, но так как разные измерения тензора могут сильно отличаться масштабом, то шаг квантования разность между ближайшими возможными значениями может быть слишком велик для точного представления весов, посему выгодно брать меньшие группы, фиксируя масштаб для одной размености, скажем, либо даже для меньшей группы соседних весов. однако, если брать слишком маленькие группы - то память, выделяемая на хранение scale, cтановится довольно существенной. скажем, если хранить scale в fp32, то для при квантовании в группы из 64 весов, скейлы уже будут давать дополнительные 32 64 0.5 бита в среднем. возникает идея - а почему бы и не заквантовать сами скейлы и так до бесконечности, но авторы решили ограничиться двумя квантованиями . в данной работе scale квантуется группами по 256 весов в 8 бит, что снижает накладные расходы на их использование до 8 64 32 64 256 0.127 бит в среднем на параметр. данная процедура не приводит к просадке качества, но дает существенную экономию по памяти."
15,2023-06-10T23:29:08,"3 и еще одно нововведение - так называемые paged optimizers. у нвидиевских gpu есть опция автоматической перекачки памяти с gpu на cpu ram, если память видеокарты забивается. причем данная перекачка не приводит к заметному замедлению процесса обучения. весьма полезная фича, учитывая сколько нейронов в голове программистов и исследователей убивает ежесекудно сообщение cuda error out of memory. низкоранговый адаптер lora, заслуживающая отдельного выпуска не квантуется и хранится в bf16. но учитывая его мизерный вес по сравнению со всей моделью - это и не требуется. итого, в конечном виде обученный на целевой задаче имеет следующий вид"
18,2023-06-10T23:31:14,"где первый член - это дважды квантованный вес, а второй - низкоранговая добавка. double dequant - расквантование скейлов с последующим расквантованием весов."
19,2023-06-10T23:31:27,"4 отдельного упоминания и уважения заслуживает валидация их модели и конкуретных на ряде бенчмарков. сначала авторы демонстрируют, что при finetuning, qlora имеет примерно такое же качество, как и дообучение всей модели и lora для модели с весами с плавающей точкой без квантования ."
22,2023-06-10T23:32:00,"далее авторы дообучают квантованную сеть на различных датасетах, и замеряют качество работы на бенчмарках, тестирующих language understanding и качество ответов модели на чатбот-бенчмарках, замеренное с помощью gpt-4 и человеками. среди датасетов берутся доступные публично данные - self-instruct - longform - chip2 - hh-rlhf - unnatural instruct - oasst1 и конечная модель guanaco обучается именно на нем - alpaca - flan v2"
23,2023-06-10T23:32:10,для замеров способности модели понимать языки используется стандартный mmlu бенчмарк. при обучении на датасете flan v2 здоровенная смесь из множества разнообразных задач на инструкции конечная модель выдает наилучшее качество вероятно из-за количества данных и их структуры .
25,2023-06-10T23:32:31,потом авторы проводят замеры на vicuna benchmark наборе инструкций против chatgpt используя в качестве арбитра gpt4. в этом случае небольшой oasst1 оказывается наилучшим для instruction finetuning. самая большая модель практически сравнивается по качеству с chatgpt.
27,2023-06-10T23:32:47,"однако делается примечание, что разброс довольно существеннен от запуска к запуски и ранжирование у gpt-4 и людей не всегда совпадает, хоть корреляция и существенна. далее качество работы оценивается на запросах из vicuna и openassistant с агреграцией оценок через elo score методики, придуманной для шамхатных турниров уже непосредственно кожаными мешками . и версии guanaco- 33b,65b опережают конкуретные open-source модели и chatgpt."
29,2023-06-10T23:33:48,"таким образом, с одной стороны авторы получают довольно качественного чатбота. и заодно делают вывод про то, что в разные датасеты заточены под разные свойства модели. скажем, flanv2, лучший для прокачки модели под понимание языка, не столь хорош для создания чатбота, и наоборот для openassistant. и самое приятное, для обучения самой большой версии guanaco не требуется значительных ресурсов. всего день на одной rtx a6000 хоть и много больше бесплатного колаба . есть демка модели на huggingface и несколько нотбуков на странице проекта. примечателен ноутбук с файнтьюном gptneox-20b на колабовской t4 c 16gib, которая так-то весит 40 гигов или 20 в 8-битной квантизации . введеный в работе формат квантизации имплементирован в библиотеке bitsandbytes и некоторые модели не только лишь все можно подгружать в данном формате"
30,2023-06-10T23:34:13,"model_id eleutherai gpt-neox-20b bnb_config bitsandbytesconfig load_in_4bit true, bnb_4bit_use_double_quant true, квантовать ли скейлы bnb_4bit_quant_type nf4 , использовать nf4 bnb_4bit_compute_dtype torch.bfloat16 тип в котором проводятся вычисления model automodelforcausallm.from_pretrained model_id, quantization_config bnb_config, device_map 0"
31,2023-06-10T23:34:26,"ах, да, маленький нюанс. формат используется для хранения весов, а в момент вычислений веса материализуется в обычный тип с плавающей точкой fp16, bf16 . потому ускорения пока нет, и даже некоторые накладные расходы на квантизацию и деквантизацию, но в будущем, вероятно, с развитием карточек и разработкой ядер, возможно, и добавится еще и ускорение. в общем, весьма сильный и практически полезный результат."
32,2023-06-10T23:36:42,статья репозиторий qlora демо finetuning bitsandbytes блогпост huggingface
33,2023-06-12T10:47:52,memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization tl dr в данной работе авторы предложили простой и дешевый эффективный по памяти способ дообучения квантованной модели для восстановления качества и под instruction finetuning. https teletype.in spiridon_sun_rotator uj6lixasxjn
34,2023-06-17T21:33:21,intriguing properties of quantization at scale tl dr в данной статье авторы исследуют влияние факторов и гиперпараметров обучения на изменение качества квантованной модели по сравнению с ее исходной версией в числах с плавающей точкой. https teletype.in spiridon_sun_rotator ybdfcach98r
35,2023-06-21T11:04:02,"understanding optimization of deep learning данный пост будет в необычном формате. обзор на обзор, он же обзор 2-го порядка. https teletype.in spiridon_sun_rotator w08b5vufrro"
36,2023-06-24T16:45:09,"snap diffusion text-to-image diffusion model on mobile devices within two seconds история про диффузионку, генерирующую картинки на уровне stable diffusion, менее чем за 2с на мобилке. https teletype.in spiridon_sun_rotator sa5t3j0p7eu"
37,2023-07-02T20:00:29,https teletype.in spiridon_sun_rotator zodh4frsx7e еще одна статья про квантование llm на основе фишерского приближения гессиана и разбиения весов на выбросы и не-выбросы.
38,2023-07-05T19:34:31,"fast segment anything есть просто сэм причем даже не один , есть дядюшка сэм, есть серьезный сэм, и на днях вышел еше быстрый сэм - fastsam. напомню, что в оригинальной работе sam segment anything model была предложена методология сегментации обьектов из широкого набора категорий на основе различных типов запросов 1 point prompt. пользователь делает тык и нейросеть выделяет самый примечательный обьект, содержащий данную точку. 2 box prompt. пользователь выделяет прямоугольник с предполагаемым обьектом и нейросеть ищет маску уточняет контуры . 3 text prompt. пользователь пишет текстом название желаемого обьекта, и он выделяется маской. помимо прочего, авторы выпустили самый большой датасет по сегментации, собранный частично вручную, а частично размеченный автоматически - sa-1b. работа вызвала большой ажиотаж, появилось много последователей. однако, у всей этой красоты есть один большой недостаток - основная моделька, используемая в работе, увесистый vit-huge с 630m, который довольно нелегко инферить даже на карточках, что уж говорить про мобильный инференс. и авторы задались вопросом - можно ли достичь сопоставимого качества, используя значительно меньше вычислений? и предложенное решение работает в 50 раз быстрее при сопоставимом качестве на rtx3090 . суть идеи в том, чтобы использовать легковесную модель. и не утруждая себя долгими раздумьями, авторы взяли известное рабочее решение - yolov8-seg c yolact для сегментации. модель обучалась на малой части данных sa-1b - всего 2 миллионах картинках из миллиарда. обладая значительно большими inductive biases и специализированными модулями, yolo сходится значительно быстрее и требует куда меньше данных. энкодеры промтов те же, что и в исходном sam. модельку валидируют на разных бенчмарках по сегментации в zero-shot. fastsam выделяет границы примерно так же хорошо, как и sam. на coco и lvisv1 примерно на одном уровне с sam, не сильно уступая finetuned vitdet-h. и еще в ряде приложений вышло неплохо - salient object detection выделении самого примечательного обьекта и anomaly segmentation. метод все же слегка уступает sam, основные проблемы возникают с маленькими обьектами. утверждается, что box confidence score, предсказываемый yolo не всегда соотвествует качеству маски. маски для маленьких обьектов выходят чрезмерно квадратными. статья код"
39,2023-07-05T19:37:07,результаты
40,2023-07-05T19:37:49,сегментация с текстовыми промптами
41,2023-07-05T19:38:29,anomaly detection - какую-то бяку нашли на таблетке
42,2023-07-05T19:39:51,"схематичный рисунок фреймворка - feature pyramid, с box и mask head nms модули."
43,2023-07-05T19:40:24,работает реально быстро
44,2023-07-05T19:40:49,failure cases
45,2023-07-09T07:34:42,"статья код на днях stability.ai выкатили новую диффузионку stable diffusion xl. как можно догадаться из названия, моделька заметно набрала в мышечной массе по сравнению с исходной моделью расшумляющий unet вырос в 3 раза в размере, стал длиньше и толще. остальные изменения носят скорее инкрементальный характер, но тем не менее любопытны. архитектура выше было сказано, сеть увеличилась в размерах. при этом убрали self-attention на максимальном разрешении из-за большого количества вычислений , и всего 2 стадии уменьшения разрешения в 2 раза вместо 3 в исходной sd т.е в середине unet пространственное разрешение в 4 раза меньше, чем на входе и выходе . в середину напихали аж 10 блоков attention. conditioning size conditionining картинки в датасетах бывают разных размеров. из-за двухстадийной структуры - диффузии в латентном пространстве и sr каскада, stable diffusion имеет жесткие ограничения на размер данных sd v1.4 обучалась на картинках, где меньшая из сторон имеет размер 512 . а существенная доля данных 39 имеет размер менее 256 минимальный принимаемый sdxl . можно было бы интерполировать до минимального приемлемого размера - но полученные картинки будут размытыми, и сеть будет считать, что так и надо. потому авторы предложили при обучении и генерации обуславливать на размер пару значений высота ширина - если надо мыльную картинку, получай мыльную, надо резкую - получай резкую. модификация улучшает метрики fid и is . crop conditioning stable diffusion при генерации по промпту нередко выдает кропы картинок, которые смотрятся неэстетично. по всей видимости, причина в том, что подобная аугментация использовалась на обучении модели. решение простое - снова condition на положение кропа нормализованные от 0,1 координаты верхнего левого угла . в итоге при condition на 0,0 выдаются более центрированные картинки, захватывающие объект целиком. оба типа conditioning добавляют фурье-эмбеддинги в процесс генерации. multi-aspect training на практике интересна генерация не только квадратных картинок, но и прямоугольных, потому сеть дообучают на генерацию с разными соотношениями ширины и высоты. чтобы обучение было эффективным, в один батч собирают картинки с похожим aspect ratio bucketing . improved autoencoder потюнили параметры обучения и улучшили реконструкцию из латентного пространства. refinement стадия после базового unet для диффузии в латентном пространстве добавили еще один unet для улучшения полученных представлений. подобная модификация, как утверждается, помогает генерировать более мелкие детали. результаты stable diffusion xl, по мнению большинства пользователей, всегда или почти всегда лучше стандартной stable diffusion, причем версия refinement стадией выглядит более предпочтительной, чем без нее. что интересно, метрики fid и clip score, на coco - стандартном бенчмарке для оценки качества генерации по промптам, даже просели по сравнению с sd v1.5, v2.1 , но на них, как известно, следует ориентировать с некоторой опаской. sdxl сравнили с midjourney v5.1 на partiprompts p2 , и на ряде категорий, пользователи чаще отдавали предпочтение sdxl, что выглядит солидно, учитывая, что midjourney считается флагманом по генерации. есть однако подозрения у знающих людей, что саму sdxl файнтьюнили на генерациям от midjourney. сама модель настолько же опенсорсная, как и llama. веса выдаются только с одобрения авторов. ждем пираток, короче."
46,2023-07-09T07:35:53,пользовательские предпочтения
47,2023-07-09T07:36:25,архитектура. новый модуль - предложенный refiner
48,2023-07-09T07:37:06,распределение обучающей выборки по ширине и высоте
49,2023-07-09T07:37:36,эффект от condition на размер
50,2023-07-09T07:38:13,эффект от сondition на crop
51,2023-07-09T07:39:08,refinement позволяет лучше прорисовывать мелкие детали
52,2023-07-09T07:39:57,stable diffusion-xl против midjourney v5.1
53,2023-07-09T07:42:03,"fid-clip кривые для stable diffusion xl хуже, чем для sd v1.5 v2.1 напомню, что согласно этим метрикам генерация тем лучше - чем ниже и левее находится кривая формы эмблемы компании nike ."
54,2023-07-19T07:29:00,"flash attention-2 faster attention with better parallelism and work partitioning статья код на днях tri dao это имя и фамилия, а не три пути из китайской философии выпустил сиквел знаменитого блокбастера flash attention - flash attention 2. обновленная версия flash attention примерно в два раза быстрее своей предшественницы и местами до 10 раз бывает быстрее наивной реализации pytorch. введение потребность в работе с длинным контекстом возникает в ряде приложений - задачах связанных с написанием или пониманием книг, обработкой видео, аудио. наивная реализация attention требует квадратичного по длине последовательности объема памяти и вычислений, из-за чего на практике редко используют контекст более 1k токенов. в свое время много работ и усилий исследователей было потрачено на разработку альтернатив алгоритму внимания в его исходной формулировке, но нельзя сказать, чтобы один из множества подходов оказался конкурентоспособен на широком наборе задач. в работе по flash attention добились значительного ускорения attention и снижения пикового расхода памяти за счет оптимизации входящих в него операции. при этом сам алгоритм математически эквивалентен up to numerical precision исходному attention. напомню, что в основе оригинальной работы по flash attention лежат следующие наблюдения 1 большую часть времени занимают не вычисления, а работа с памятью. 2 для подсчета attention не обязательно материализовывать всю матрицу attention, квадратичную по длине последовательности целиком. можно считать ее поблочно, а затем агрегировать результат. flash attention уменьшает количество операций по перекачке данных с hbm памяти в кэши gpu за счет выполнения нескольких математических операций сразу в пределах одного блока kernel fusion . при обратном проходе и подсчете градиентов по параметрам матрица attention пересчитывается снова поблочно. flash attention делает больше вычислений, чем исходный алгоритм, но так как вычисления в разы быстрее перекачки памяти туда-сюда, получается значительный выигрыш в производительности. однако, даже оптимизированный flash attention сильно недоиспользует возможности современных ускорителей вычислений, достигая всего-лишь 25-40 от теоретической максимальной производительности, в то время как матричные умножения могут достигать 80-90 ."
55,2023-07-19T07:29:44,"метод в работе flash attention 2 по существу еще слегка подкрутили процедуру вычисления и повысили степень параллелизма самой операций. алгоритм вычисления автор заметил, что операции, не являющиеся матричным умножением, выполняются куда медленнее в 16 раз , чем матричные умножения, потому переписал алгоритм так, чтобы уменьшить их количество. казалось бы, их количество невелико, но тем не менее, они занимают существенную часть общего времени работы. кроме того, при авторегрессионной генерации нужна лишь верхнетреугольная часть матрицы attention, и вместо того, чтобы считать ее, а затем занулять, ее просто не считают. вот так вот! благодаря перечисленным выше нововведениям удается добиться ускорения 2-3x. параллелизм flash attention-1 параллелизует вычисления по размеру батча и числу голов в трансформере, но если батч не слишком большой или трансформер не очень огромный, то многие streaming multiprocessors sm простаивают. и чтобы не оставлять их без дела, предлагается паралеллизовывать вычисления и по длине последовательности. на прямом проходе ряды матрицы attention можно считать независимо, а на обратном проходе - колонки. и каждый поток обрабатывает свой токен. кроме того, для уменьшения коммуникации между варпами группами потоков , оказывается целесообразным держать куски матриц ключей key и значений values общими для групп поток, а query свою на варп в flash attention-1 было наоборот . уменьшение количество операций чтения записи приводит к дополнительному ускорению. результаты flash-attention-2 сравнивается с flash-attention из оригинального репозитория, реализации на triton и xformers. для замеров рассматривают последовательности длиной от 512 до 16k токенов, и слой attention со скрытой размерностью 2048 64 или 128 голов . flashattention-2 в 1.3-1.5x быстрее на прямом проходе, и до 2x быстрее на обратном проходе по сравнению с flash-attention - 1 особенно велик выигрыш при использовании causal mask . flash-attention - 2 использует до 72 теоретической производительности a100. на h100 разница еще заметнее. выводы данная история поучительна тем, что одна и та же математическая операция в зависимости от реализации, может выполняться принципиально разное время. замечательный пример того, что насколько учет особенностей железа, время работы различных компонент, сильных и слабых сторон ускорителя вычислений важен при проектировании алгоритмов."
56,2023-07-19T07:31:09,алгоритм прямого прохода
57,2023-07-19T07:31:27,алгоритм обратного прохода
58,2023-07-19T07:32:11,параллелизм по длине последовательности на прямом и обратном проходе
59,2023-07-19T07:33:00,"распределение матриц q, k, v по варпам"
60,2023-07-19T07:34:00,сравнение скорости forward pass для различных реализаций attention
61,2023-07-19T07:34:36,сравнение скорости forward backward pass для различных реализаций attention
62,2023-07-19T07:43:47,attention forward backward на h100
63,2023-07-21T20:09:57,"stack more layers differently high-rank training through low-rank updates статья код обучение всех параметров больших языков моделей весьма прожорливо по памяти из-за необходимости хранить кроме самой тяжеловесной модели еще и состояния оптимизатора 8 байт на параметр . lora, один из самых ходовых методов peft, заключающийся в обучении низкоранговых добавок к весам позволяет сильно сэкономить по памяти, демонстрируя при этом хорошее качество при обучении предобученной модели на downstream задачах. но низкоранговые представления имеют место при дообучении, в то время как для эффективного предобучения на разнообразных данных желательно использовать все имеющуюся в распоряжении емкость сети - то есть обучение должно быть высокоранговым. в данной статье авторы предлагают метод последовательного обучения низкоранговых добавок к весам линейных слоев нейронной сети с последующим их слиянием с основными весами. и как утверждается, подобная процедура для достаточно больших сетей самая большая обученная сеть имеет 350m параметров - сущий пустяк по современным меркам , работает ненамного хуже стандартной полноранговой процедуры обучения. метод ранг суммы двух и более матриц ограничен сверху суммой рангов матриц. если низкоранговые матрицы в достаточной мере взаимно независимы, то их сумма может иметь значительно больший ранг чем каждое слагаемое по отдельности. последовательно обучая низкоранговые добавки возможно в итоге добиться высокорангового изменения весов матрицы, в этом и суть метода. однако, чтобы метод заработал, авторам пришлось учесть ряд нюансов и применить пару трюков. во-первых, используемый при обучении трансформеров adam хранит скользящие статистики градиентов, и при переходе к обучению новой низкоранговой добавки, если не предпринимать никаких действий, оптимизация будет проводиться в том же подпространстве, что и у предыдущей lora добавки, нивелируя всякий смысл в итеративной процедуре. для предотвращения такого сценария, авторы зануляют 99 состояний оптимизатора с меньшей абсолютной величиной почему не все? почему не любую другую долю? при инициализации новой добавки. кроме того, learning rate в момент начала обучения новой добавки зануляется и потом быстро разогревается до примерно того же значения, с которым закончила обучение прошлая добавка используется cosine annealing learning rate . без короткой warmup фазы обучение расходится. предложенная cтратегия именуется relora."
64,2023-07-21T20:09:57,"эксперименты авторы обучают семейство декодерных моделей моделей от 60 до 350m типичный размер языковых моделей в 18-19 году на данных из c4. архитектура модели повторяет llama. процедура обучения состоит из первоначальной фазы полнорангового обучения т.е обучения всех параметров модели в течение 5k шагов и 3 циклов обучения низкоранговых добавок на протяжении тех же 5k шагов с warmup фазой в 100 шагов при переходе к новой lora . пиковый расход памяти такой же, как и в стандартной процедуре обучения. в качестве бейзлайнов используются стандартное обучение обучение меньшей модели с таким же количеством обучаемых параметров, как с lora control lora метод ожидаемо бьет lora, обладая большей выразительностью, и меньшую сеть с тем же числом обучаемых параметров за исключением самой маленькой модели , при этом несколько уступая стандартной процедуре обучения. авторы анализируют спектральное разложение обученных матриц, и у relora оно больше напоминает изменение весов при обучении всех параметров по сравнению с lora , хоть все еще заметно отличается. ablation показывает, что все компоненты метода важны для приемлемого результата - первичная процедура стандартного обучения, зануление состояний отпимизатора и warmup. заключение довольно интересный и разумный подход. применимость его в качестве претрейна, по моему мнению, ограничена, из-за необходимости фазы высорангового обучения в начале, из-за чего большие llm-ки какое-то время придется обучать на множестве хостов. основной выигрыш может быть при файнтьюнинге на достаточно больших и разнообразных задачах, где выразительности низкоранговых добавок недостаточно."
65,2023-07-21T20:11:53,"кривые обучения стандартного обучения синяя , relora оранжевая , и меньшей модели control"
66,2023-07-21T20:12:44,learning rate schedule из статьи на графике 4 цикла relora вместо 3-х
67,2023-07-21T20:13:44,ключевое тождество из статьи
68,2023-07-21T20:14:03,алгоритм
69,2023-07-21T20:14:59,сингулярное разложение разницы весов на 20k шагов конец обучения и 5 k шагов конец warmup фазы
70,2023-07-21T20:15:26,relora против бейзлайнов
71,2023-07-24T07:02:29,"llama-2 open foundation and fine-tuned chat models статья код не прошло и года и даже половины года , как запрещенная в россии экстремистская организация meta выпустила новую версию всем полюбившейся llm-ки llama-2. первая версия модели стала настоящим хитом среди исследователей, практиков, да и простых обывателей, будучи наиболее качественной языковой моделью среди находящихся в публичном доступе. llama стала основой для множества чатботов, получила множество интеграций для запуска на чем угодно начиная от продвинутых gpu и заканчивая калькуляторами и микроволновками. нововведения в плане архитектуры. и процедуры предобучения llama-2 не претерпела значительных изменений. вместо стандартного attention блока, где количество голов в query, key, value проекциях одинаково, и каждому query соотвествует отдельный key и value, используется grouped query attention c 8 проекциями вместо num_heads т.е каждая key, value активация спаривается с num_heads 8 головами query . делать полный multi query с 1 проекций на все головы не стали по двум соображениям - 1 на инференсе они параллелизует вычисления между 8 gpu, и пришлось бы все равно копировать key, value между всеми устройствами 2 multi query просаживается по качеству по сравнению с исходным attention, а grouped query имеет примерно то же качество. данное изменение полезно при авторегрессионой генерации с использованием key, value кэшей, так как приводит к заметной экономии в памяти при той же длине последовательности экономия в num_heads 8 рвз . длину контекста увеличили до 4k токенов. rope позиционные энкодинги могут работать и с более длинным контекстом. данные отфильтровали более тщательно, увеличили в размере на 40 и обучили все модели на 2t токенов вместо 1t. итоговая модель на common sense reasoning, question answering, world knowledge, и т.д оказывается лучше прошлой версии и всех других моделей в открытом доступе, но уступает флагманским закрытым - gpt-4, palm-2-l. куда более занимательна ей же и уделено основное внимание процедура instruction-finetuning и получения чатбота из языковой модели. instruction-finetuning процедура состоит из 2 стадий 1 sft - supervised finetuning 2 rlhf - reinforcement learning авторы собирают свой собственный датасет из инструкций, в котором акцент был сделан не на количество инструкций, а на их качество и разнообразие актуальные работы утверждают, что для instruction finetuning данных много и не требуется . в полученном датасете 27540 инструкций. на первом стадии sft модель обучают на causal lm, как на этапе преобучения на датасете инструкций. промты и ответы контатенируют с одну последовательность, разделяя специальным токеном. данные для обучения reward модели собирали с помощью человекоподобных разметчиков. каждый респодент выбирает между двумя вариантами с градацией разницы significantly better, better, slightly better, or negligibly better unsure. для максимизации разнообразия варианты ответов генерируются случайно выбранными моделями из семейства llama-2 с разной температурой."
72,2023-07-24T07:02:29,"обучают две reward модели 1 helpfullness полезность 2 safety безопасность для моделирования reward используются предобученные чекпоинты с 1-го этапа. в качестве функции потерь используется бинарная ранжировочная функция потерь из instruct gpt с добавкой, зависящей от степени увереннности в ответе, чтобы разница в оценках для ответа с большей уверенностью была больше, чем для менее уверенного ответа. полученные reward модели сравнивают с теми, что получаются при обучении на других instruction датасетах и gpt4. и по отдельности reward модели оказываются лучше безйлайнов на своих и прочих датасетах но для gpt4 нет данных на других instruction датасетах . затем исследуется scaling поведение от количества данных и размеров модели. ожидаемо, большие модели и большее количество данных улучшает качество reward модели. с ростом количества полученных данных от аннотаторов авторы итеративно дообучают reward модель 5-версий с использованием proximal policy optimization ppo и rejection sampling. нередко перед чатботом ставится задача следовать некоторой инструкции или парадигме поведения на протяжении нескольких раундов вопрос-ответ или всего диалога. чтобы поддерживать в модели подобный сценарий поведения, авторы статьи используют метод gatt ghost attention . ко всем запросам пользователя добавляется целевая инструкция, но чтобы не нарушать распределение данных диалог, где пользователь повторяет одну и ту же инструкцию много раз смотрится неестественно , лосс от прошлых сообщений в диалоге не учитывается. данная модификация действительно способствует следованию ассистентом целевой инструкции."
74,2023-07-24T07:03:37,"результаты llama-2-chat уверенно побеждает чатботов, основанных на моделях в открытом доступе, сопоставимых размеров, и с небольшим отрывом оказывается лучше с точки зрения человеческих предпочтений чем chatgpt при оценке helpfulness на собранных meta 4k инструкциях. при обучении на safety данных, с ростом количества safety данных стабильно уменьшается доля небезопасных ответов без просадки по метрике полезности. по safety доле небезопасных ответов и общему рейтингу полезности и безопасности llama-2 чатботы опережают конкуретных открытых чатботов и chatgpt palm при оценке на собственном бенчмарке из 2k промптов. из дополнительных экспериментов авторы показывают, что модель можно научить действовать корректно подав инструкцию относящуюся к заданному времени например, модель не будет знать ответ на то, кто побелил во второй мировой войне, если бы запрос был адресован в 1940 году и хорошо взаимодействует с toolformer. итог llama-2 - новая sota среди моделей в открытом доступе, и с учетом бешеного прогресса в области, большого интереса в dl-сообществе, за несколько дней с выпуска, народ уже успел изрядно поиграться с моделью, покрутить и повертеть ее. данная работа - труд скорее инженерный, чем научный, но, безусловно, полезный и важный. приятное отличие от первой версии, где месяцами можно было ждать одобрения на скачивание весов хотя все кому надо воспользовались пиратками , в том, что запрос на llama-2 удовлетворяется оперативно обычно в течение пары часов ."
75,2023-07-24T07:05:00,сравнение llama-2-chat против других чатботов по метрике helpfulness
76,2023-07-24T07:06:26,сравнение llama-2-chat против других чатботов по метрике safety
77,2023-07-24T07:07:21,схема процедуры обучения llama-2-chat
78,2023-07-24T07:07:49,кривые обучения моделей из семейства llama-2
79,2023-07-24T07:08:21,сравнение на стандартных бенчмарках для lm
80,2023-07-24T07:09:02,сравнение на academic бенчмарках
81,2023-07-24T07:09:51,сравнение reward моделей
82,2023-07-24T07:10:35,скейлинг-кривые для reward моделей
83,2023-07-24T07:11:21,демонстрация пользы от gatt в multi-turn диалоге
84,2023-07-24T07:12:27,скейлинг по safety от количества safety данных
85,2023-07-24T07:13:50,"time awareness у llama-2 при подаче инструкции, где чатбот должен ответить, использую актуальные на то время знания человечества о мире"
86,2023-07-25T07:18:59,"no train no gain revisiting efficient training algorithms for transformer-based language models статья код с течением времени запрос на обучение языковых моделях, основанных на трансформерах, растет быстрее, чем имеющиеся в распоряжении вычислительные ресурсы. потому и возникает запрос на эффективное обучение моделей. коллективный разум dl-сообщества предложил великое множество стратегий ускорения обучения модификации процедуры обучения с использованием только части слоев, использованием части данных и алгоритмами отпимизации. каждая статья заявляет в той или иной форме, что предложенный метод эффективнее базового решения. но что означает эффективнее? ответ на этот вопрос не столь тривиален. сравнение эффективности алгоритмов количество шагов алгоритмов может быть плохим показателем - так как время одного шага может существенно различаться между алгоритмами. алгоритм делающий в два раза меньше шагов, но с пятикратной стоимостью шага не слишком полезен. время работы зависит от используемой конфигурации вычислительных ресурсов. число операций с плавающей точкой зачастую не отражает специфику реализации математических операций на железе, время доступа к памяти, накладные расходы на коммуникацию . потому авторы предлагают использовать относительное время работы, привязанное к конкретному железу. для некоторой конфигурации видеокарты, cpu, озу фиксируется вычислительный бюджет, и при запуске на другом сервере надо замерить отношение работы алгоритма к исходной конфигурации и с поправкой на этот фактор проводить все замеры. т.е если новый сервер в 2 раза быстрее - то эксперимент должен быть в два раза короче."
87,2023-07-25T07:23:07,"архитектурные модификации layer stacking обучаем меньшую модель некоторое число шагов, а затем дублируем слои и обучаем далее уже вдвое большую модель. предполагаемый выигрыш за счет того, что меньшая модель делает большее число шагов при фиксированном размере. layer dropping в архитектурах с residual connections в целях регуляризации иногда пропускают вычисления части слоев например mha или feedforward block . кроме того, сия процедура дает некоторую экономию в количестве вычислений. обыкновенно в начале обучения вероятность пропуска слоя равна нулю и с течением времени растет до некоторого максимального значения. кроме того, вероятность растет от входа модели, к ее выходу согласно народной мудрости в начале обучаются универсальные и общие представления, а ближе к выходу более специфичные . отбор данных selective backprop на обратном проходе градиенты считаются только по примерам с наибольшим значением функции потерь. на прямом проходе считается лосс, а на обратном сэмплируются примеры отранжированные по недавней истории функции потерь. rho loss предложенная в работе метод уменьшает вес примеров с большими значениями train лосса, так как они скорее всего соотвествуют шумным или некорректно размеченным данным. оптимизаторы lion в качестве альтернативы общеупотребимому adam-у в начале этого года был предложен отпимизатор lion, найденный с помощью reinforcement learning. теоретических гарантий на его превосходство нет, но в ряде работ утверждается, что он немного накидывает по сравнению с adam. при этом сам алгоритм достаточно прост. sophia все новое - это хорошо забытое старое, как известно. sophia - по существу тот же adam, но использующий другой способ оценки кривизны для preconditioning. в исходной работе использовались две формулировки - через hutchinson estimator и gauss-newton-bartlett. в экспериментах здесь используется вторая, так как ее реализация была опубликована и работала немного лучше в cтатье по sophia ."
88,2023-07-25T07:23:55,"эксперименты для сравнения различных методик эффективного обучения авторы берут bert и t5 модели, обучают на задачи mlm и span-corrupting mlm, соответственно. для предобучения используется c4. для каждого метода параметры подстраиваются на основе некоторой сетки. алгоритмы сравниваются на вычислительных бюджетах в 6,12,24 часов на одной rtx 3090 a100 для т5 . для оценки качества модели bert валидируют результат файтьюнинга bert на задачах из glue superglue бенчмарков. кроме того, оценивается качество работы mlm на валидации для bert и t5. и для t5 модель проверяют на super natural instructions. как оказалось, ни один из перечисленных методов оптимизации процедуры обучения не дают стабильного выигрыша по сравнению со стандартной процедуро обучения adam без наворотов . на коротких отрезках - 6-12 часов некоторый профит имеет место от layer stacking, практически исчезающий при более длительном обучении. остальные алгоритмы из рассмотренных не дают выигрыша против бейзлайна ни при каких бюджетах обучения. методы отбора данных при заданном ограничении на время работают хуже и на mlm и на glue. новомодные отпимизаторы lion и t5 сходятся хуже по времени со сравнению с baseline. итоги результат данной статьи в очередной раз подтверждает важность аккуратного и честного сравнения методов друг с другом. статьи по deep learning выходят все чаще и чаще, потому спрос на санитаров леса будет только расти с течением времени. наука на то и наука, что фальсифицируема."
90,2023-08-01T15:31:45,"retentive network a successor to transformer for large language models статья код каждый исследователь или практик алкает следующих качеств от архитектуры нейронной сети 1 эффективное обучение 2 хорошее качество 3 дешевый инференс архитектура трансформер, доминирующая во многих областях и приложениях deep learning, удовлетворяет первым двух из требований, но, к сожалению, довольно тяжеловесна и прожорлива до вычислений. множество работ предлагали различные альтернативы и приближения attention, но ни один из них не стал общеупотребимым на практике. аналогично, варианты с внедрением рекуррентных механизмов и state-space моделей s4, h3, hyena хоть и получили признание, но не составили полноценной конкуренции attention. и в этой статье авторы делают довольно громкое заявление о том, что они смогли воплотить невозможный треугольник в реальность, создав архитектуру, обладающую всеми желаемыми аттрибутами - retentive network retnet . все прошлые архитектуры можно теперь со спокойной душой отправить на свалку истории."
91,2023-08-01T15:32:35,"метод предложенный механизм рассматривается в качестве альтернативы стандартному attention в трансформер блоках, то есть чередуется с feedforward слоями и применяется на последовательностях токенов. retention является по существу версией rnn, c обновляемым вектором состояния, где каждое последующее состояние получается как взвешенная матрично сумма прошлого состояния и текущего элемента последовательности, и с выходной проекцией, превращающей скрытое состояние в выход слоя. все проекции - взвешивающая скрытое состояние, текущий элемент последовательности и выход, получаются линейным преобразованием входа - полный аналог query, key, value проекций в attention. в частном случае необучаемых проекций архит далее авторы диагонализуют матрицу, преобразующую скрытое состояние, перепараметризуют веса и приходят в итоге к форме, удобной для вычисления. полученная операция включает в себе attention без softmax, causal masking, и экспоненциальное затухание по длине последовательности. замечательным свойством retention является возможность представить его в 3 ипостасях 1 параллельная версия оптимальная для обучения на gpu 2 последовательная версия бюджетная на инференсе 3 чанковая реализация trade-off между двумя первыми в первой версии одновременно обрабатывается вся последовательность, во второй один элемент, в третьей - блок некоторого размера. далее, как и в оригинальном attention, предлагается многоголовая версия retention. однако, если обучаемыми разными головами паттерны в multiheadattention имеют случайное поведение, то здесь разные головы отвечают разным масштабам. коэффициент затухания γ определяет характерный масштаб длину контекста , с которой работает данная голова. на выход multiheadretention довешивается groupnorm. для повышения выразительности операции выход multiheadretention умножается на swish xw_g - gating механизм, и пропускается через выходную проекцию w_o."
92,2023-08-01T15:34:18,"эксперименты предложенную архитектуру retnet обучают на задаче causallm на смеси из the pile, c4, the stack. в качестве бейзлайна берут стандартный трансформер. для оценки качества модели используют 0-shot и few-shot бенчмарки из lm evaluation harness. retention network достигает меньшей перплексии на валидации по сравнению с transformer на размерах 2.7b, 6.7b, и по форме кривой авторы утверждают, что retention network масштабируется лучше трансформера сильноватое заявление на основе графика из 3-х точек . аналогично, retention network превосходит transformer на 0-shot и few-shot. далее следуют замеры расхода памяти и скорости инференса, и retnet оказывается быстрее и экономичнее по расходу gpu ram даже оптимизированной версии flash attention на тритоне. расход памяти, при работе в рекуррентном режиме не растет с размером последовательности, как и скорость генерации одного токена не зависит от длины последовательности. при сравнении на ряде бенчмарков по language modelling retnet превосходит другие альтернативы трансформера - state-space модели, rwkv, линейный трансформер. удивительно, что нет сравнения на long range arena, которое является признанным мерилом способности сети работать с длинным контекстом, каким бы данный бенчмарк ограниченным не несовершенным ни был. авторы проводят ablation компонент архитектуры и все компоненты - groupnorm, multi-head, экспоненциальный спад γ с разным коэффициентом в разных головах оказывается важен. хоть разница, не сказать, чтобы кардинальная для каждой из компонент."
93,2023-08-01T15:34:40,"итог интересная идея с понятным value proposition. однако, валидация выглядит на текущий момент слишком ограниченной и специфичной, чтобы утверждать про рождение преемника трансформера. наличие экспоненциального затухания, пусть и на разных масштабах, ограничивает длину обрабатываемого контекста. устремление же γ- 1, по всей видимости, приводит к нестабильности в обучении. код модели недавно появился в открытом доступе, так что ждем независимых проверок, а можем и сами убедиться в чудесных свойствах предложенной архитектуры. но за подачу материала, тем не менее следует отдать комплимент авторам. красивая реклама - важная составляющая не только в коммерции, но и в науке."
94,2023-08-15T11:52:26,"fastervit fast vision transformers with hierarchical attention очередной гипер-мега-ультра эффективный гибрид трансформера и сверточной архитектуры - fastervit от nvidia. согласно кривой парето throughput vs top-1 imagenet accuracy, представленной в работе, fastervit заметно опережает всех предшественников при замерах на a100. в чем же секрет такого чудесного быстродействия? архитектура работа по ускорению и оптимизации vision и не только трансформеров активно ведется примерно со времени их младенчества. исходный трансформер очень гибок и универсален, и в то же обладая меньшим набором inductive biases, менее заточен под задачи компьютерного зрения, поэтому не столь эффективен. с 2021 года появилось множество работ, совмещающих архитектурные элементы из трансформера и более привычных сверточных архитектур - swin, convnext, poolformer, crossvit, coat, mobilevit, efficientformer, nextvit, и многие другие. серьёзным недостатком трансформера с точки зрения вычислительной сложности является квадратичная сложность по длине последовательности. потому приходится либо ограничиваться attention на низких разрешениях, либо нарезать на крупные патчи. в литературе был предложен ряд способов удешевить attention за счет его локализации в окне swin , факторизации на локальный и глобальный attention twins . в этой работе предложили по сути новую версию факторизации attention. заводят на каждое окно аналогичное таковому в swin некоторое количество carrier токенов которых гораздо меньше, чем патчей в окне . и в каждом hat hierarchical attention происходит обмен информацией между carrier токенами из разных окон, а затем carrier токены обмениваются информацией с патчами в окнах. после одного или несколько раундов hat carrier токены сливаются с исходными патчами для передачи глобальной информации. кроме того, популярные в мобильных архитектурах depthwise свертки не эффективны по скорости, из-за memory-bound природы верхних слоев, поэтому используются обычные 3x3 свертки без факторизации. дизайн архитектуры довольно стандартный для эффективных гибридных архитектур на первых двух стадиях высоком разрешении сверточные блоки. на меньшем разрешении работают hat блоки."
95,2023-08-15T11:52:46,"результаты fastervit оказывается значительно эффективнее альтернатив по throughput, хоть и не по количеству параметров. кроме того, архитектура неплохо себя показывает в качестве бэкбоуна на задачах сегментации и детекции. далее в статье есть ablation компонент, подтверждающий необходимость того или иного решения. эффект от каждой из них по отдельности, честного говоря, небольшой. выводы если верить представленным результатам, то вышло довольно эффективное решение, с относительно простой структурой. вопрос в том, насколько ускорение на gpu будет переноситься на мобильные архитектуры, кои скорее всего и будут являться целевой аудиторией. комбинация локальной и глобальной агрегации признаков - выглядит универсальным подходом в разработке современных архитектур, и дальнейший прогресс носит скорее инкрементальный характер. кроме того, процедура обучения, если присмотреться, включает некоторый тюнинг гиперпараметров learning rate, dropout, drop path , по сравнению со стандарнтыми рецептами, и неизвестно насколько выигрыш обусловлен самой архитектурой, а насколько удачным оптимизационным рецептом. отдельный вопрос, насколько наработки в области эффективных архитектур переносятся на clip-модели и self-supervised претрейны. есть ли какая-то польза от специфичных архитектур, или базовый трансформер всех победит?"
96,2023-08-26T11:54:43,"on architectural compression of text-to-image diffusion models статья код главным недостатком любимых всеми диффузионных моделей для генерации чего-либо является их скорость. каждый процесс генерации представляет собой итеративное применение довольно увесистой модели. и потому есть два направления для ускорения 1 уменьшение количества шагов сэмплирования 2 удешевление одного шага сэмплирования сегодняшний сюжет будет относиться ко второму направлению. постановка эксперимента за основу берется stable diffusion v1.4. как известно, данная модель состоит из unet модели, генерирующей в латентном пространстве, текстового энкодера, и вариационного кодировщика, погружающего в скрытое пространство и из него. но так как почти все вычисления приходятся на unet целесообразно сжимать в первую очередь его. unet stable diffusion состоит из сверточных resnet блоков и transformer блоков. первое наблюдение, которое делают авторы, оказывается, что из unet stable diffusion можно вытащить среднюю часть с наименьшим разрешением карт активаций с довольно небольшой просадкой по качеству и метрикам без какого либо дообучения, что любопытно. выходит, что данный участок сети не выполняет серьезной работы, либо сеть, как человеческая печень, продолжает функционировать в штатном режиме при удалении части от нее. далее авторы предлагают три варианта уменьшенной stable diffusion 1 base. меньше блоков в верхних слоях. 2 small. без блоков в середине модели. 3 tiny. укорачивание на промежуточных разрешениях. обучать с нуля это хозяйство оригинальная stable diffusion обучалась 150к gpu часов на a100 - удел богатых. и авторы прибегают к дистилляции. лосс состоит из трех компонент 1 исходного denoising loss 2 mse с учителем 3 mse между активациями на выходах каждой стадии перед понижением повышением разрешения . существенная деталь, от которого зависит итоговое качество всей процедуры, на каких данных следует дистиллировать в условиях ограниченного бюджета? авторы используют подвыборку из 220k примеров из laion-aesthetics с наибольшими оценками по эстетике."
97,2023-08-26T11:54:44,"эксперименты процедура дистилляции выходит относительно бюджетной по нынешним меркам обучение самой большой модели bk-sdm base занимает 300 часов на одной a100. для оценки качества генераций модели используется стандартный бенчмарк ms-coco на реалистичность изображений и соответствие описанию картинки. дистиллированые bk-sdm несколько просаживаются по метрикам по сравнению с материнской моделью, тем не менее, обладают все еще неплохим качеством генераций. что интересно, оптимальное качество по fid достигается еще до последней итерации. авторы демонстрируют, что bk-sdm генерирует лучше ganов, хотя осмысленность подобного сравнения без сопоставления времени генераций выглядит сомнительной. полученные модели позволяют генерировать картинку на 30-40 в зависимости от размера , быстрее чем исходная sd v1.4. далее авторы проводят ablation. инициализация модели-ученика весами stable diffusion работает на порядок лучше, чем с нуля, что ожидаемо, учитывая короткое время обучения. оба лосса в дистилляции - на выходе модели и на уровне промежуточных активаций полезны, и улучшают качество. больший размер батча немного лучше, чем меньший в среднем по метрикам. на специализированной генерации с dreambooth дистиллированные модели почти не уступают базовой stable diffusion. заключение данная статья достигает довольно неплохих результатов по сжатию моделей в условиях ограниченных ресурсов, используя стандартные методы из дистилляции трансформеров. совсем просадки по качеству избежать не удалось, и, сжатые модели, по всей видимости, более специализированы под генерацию в стиле laion aestethics, и, скорее всего, проседают более заметно на промптах из другого распределения. однако, сама возможность восстановить качество, близкое к исходному, за счет отбрасывания некоторых блоков, говорит о том, что есть некоторая свобода и простор в направлении оптимизации архитектур для диффузионных моделей. классификаторы imagenet-1k и бэкбоуны для детекции сегментации на mscoco несколько приелись."
98,2023-08-27T21:47:02,"dataset quantization статья код вопреки обыкновению, данная статья не про квантование нейронных сетей, и даже не про квантование векторов, а про квантование датасетов. как известно, чем больше данных, тем лучше итоговая модель. однако, обучение на большом датасете требует значительных затрат по времени. да и данные надо где-то хранить. потому возникает естественный вопрос - можно ли как-то уменьшить количество данных, не потеряв при этом существенно в качестве? существующие методы отбирают примеры либо на основе градиентов по примерам для фиксированной архитектуры, либо model-agnostic способами на основе некоторых эвристик. проблема первого семейства подходов, что они не обобщаются на другие модели и архитектуры, а качество работы второго класса методов обычно оставляет желать лучшего. кроме того, первый класс методов требует весьма значительных вычислительных затрат. и в данной работе предлагают метод, который, с одной стороны, не привязан к конкретной модели, и с хорошим качеством. метод хочется, чтобы полученный набор данных был как можно более разнообразным. за основу берут метод graphcut, который стартует с произвольно выбранного примера, и каждый следующий пример подбирают так, чтобы он был как можно дальше от выбранных ранее, и ближе к еще не выбранным. однако, проблема исходной постановки в том, что пока примеров выбрано мало по сравнению с размером всех данных, будут браться примеры, наиболее близкие к центроиде еще не выбранных, и разнообразие примеров выйдет довольно ограниченным. авторы предлагают пройтись по примерам в порядке, определенном graphcut, и добавлять разбить примеры на несколько корзин. и затем равномерно выбирать примеры из каждой полученной корзины. утверждается, что образованный таким образом датасет будет обладать достаточным разнообразием и репрезентативностью. для дальнейшего сжатия, авторы оценивают информативность патчей с помощью модифицированной версии gradcam и заменяют на черные квадраты наименее информативные."
99,2023-08-27T21:48:18,"эксперименты качество работы метода проверяется на cifar-10, imagenet-1k, ade20k, ms coco и инструкциях из alpaca. предложенный метод заметно превосходит альтернативы и при этом оказывается обобщаемым на другие архитектуры. оптимальная доля маскируемых патчей изображения - 25 , отбор с помощью gradcam лучше случайного выбора. здесь однако, возникает вопрос, насколько полезно такое сжатие - так как количество операций при прогонке что исходной, что маскированной картинки для большинства архитектур - одна и та же. кроме того, за счет уменьшения количества цветов, прочих алгоритмов сжатия, можно добиться более сильного сжатия. оптимальное число корзин в районе 10. для задач из компьютерного зрения без просадки в качестве удается сжать датасет на 20 , 40 . it aint much but it s honest work. датасет с инструкциями же удается сжать куда лучше 80-98 . вероятно, потому, что рассмотренные задачи для компьютерного зрения требуют существенной перестройки модели, а instruction tuning - небольшая корректировка весов модели под желаемый паттерн поведения. кроме того, ранее было показано, что удачно выбранное подмножество из alpaca-set лучше всего alpaca. кроме того, метод работает за разумное время, от силы час но непонятно, для какого датасета . итог задача полезная и востребованная. однако сам метод не имеет каких-то строгих теоретических гарантий и имеет ограниченные возможности по сжатию датасетов, хоть и превосходит альтернативы. было бы интересно прогнать на flanv2, там как раз больше миллиона инструкций. и для генеративных моделей."
101,2023-09-08T11:31:40,"falcon-180b статьи нет, но обещают блог-пост на hf свершилось! тысячелетний сокол расправил крылья и явился широкой публике во всей красе! самая большая модель в публичном доступе, побеждающая все открытые модели на бенчмарках и кое-где даже проприентарные! охладим бурю эмоций и перейдем к сути дела. модель и правда самая большая - 180 лярдов параметров, чуть больше bloom и opt-175b. обучалась эта здоровенная птица на 3.5т токенов против 2т у различных версий llama-2 из refinedweb и других источников - диалогов, статей, кода. датасет настолько велик, что даже это немалое число меньше, чем одна эпоха. длину контекста 2k токенов не меняли. возникает вопрос о compute-оптимальности модели, ибо масштабирование по размеру модели больше, чем по количеству данных, а chincilla law предписывает масштабировать модель и данные примерно одинаково. на стандартных бенчмарках falcon-180b опережает все иные модели из находящихся в публичном доступе без instruction-finetuning . хотя памятуя о том, что ранее была выявлена лажа с валидацией меньшей версии модели 1 , требуется независимая экспертиза научного сообщества для проверки справедливости заявлений. архитетурно большой сокол не отличается существенно от меньших соколиков, за исключением multi-query attention, по аналогии с llama-2. учитывая, колосалльный размер модели, ее инференс и файнтьюнинг представляет определенные сложности. если самую большую llama llama-2 можно без проблем засунуть на одну a100 80gb , а в 4-бита через bitsnandbytes gptq интеграцию и на a100 40gb rtx a6000, то falcon 80gb не влезает целиком ни на одну gpu стандартными методами, без квантизации в менее чем в 4 бита. блог содержит информацию о требуемых ресурсах для запуска модели. однако, есть вопросы к приведенным в таблице цифрам, ибо qlora вряд ли требует меньше памяти, чем инференс с gptq с той же битностью. с 4 битной квантизацией, на батче размера 1 и последовательностью длины порядка 1k должно выходить 90 gb. без информации о длине последовательности непонятен расход память на kv-кэши. некоторые утверждают 2 , что falcon-180b лучше справляется со сложными промптами, чем gpt-3.5 и llama-2. возможно, статья приоткроет интересные подробности об обучении модели, хотя скорее всего размер модели и данных играют определяющую роль. статья про llama-2 примечательна в первую очередь дообучением модели под чатбота и заточкой ее под полезность и безопасность. предполагаю, что нечто подобное будет предьявлено и для falcon-180b. 1 https twitter.com francis_yao_ status 1666833311279517696 2 обсуждение на reddit"
102,2023-09-10T22:01:14,"memory-efficient selective fine-tuning статья кода нет введение с ростом размера моделей, используемых в разных приложениях, все острее встает проблема о том, как обучать нейросети в условиях ограниченной памяти gpu. на gpu надо как-то разместить модель состояния оптимизатора, и промежуточные активации, необходимые для большинства операций на обратном проходе. для экономии памяти на состояниях оптимизатора существуют различные методы peft. но память, расходуемая на хранение активаций, тоже бывает весьма существенной, особенно при обучении на больших батчах и длинных последовательностей. есть давно известный способ - gradient checkpointing - позволяющий экономить расход по памяти за счет дополнильных вычислений, но в идеале хотелось бы обойтись без этого. метод в рассматриваемой статье предлагается делать backprogation только через часть выбранных случайным образом токенов одинаковых для всех блоков трансформера , а для остальных можно высвобождать активации после выполнения операции. при длине последовательности l и количестве выбранных токенов k экономия по памяти l k раз. предложенный подход применяется к задачам классификации и в качестве конечного состояния трансформера, подающегося в классификатор, используют усредненный эмбеддинг выбранных токенов. результаты авторы проверяют свой метод на бенчмарках из glue, используя маленькую по современным меркам bert-large модель. при одинаковых гиперпараметрах процедуры обучения за исключением подбираемого индивидуально под каждый метод learning rate selective finetuning c k 16 не сильно уступает full-finetuning и слегка опережает lora. далее авторы показывают, что их метод гораздо лучше масштабируется с размером батча по сравнению с альтернативами. кроме того, selective finetuning можно применять в связке с lora для экономии на активациях и состояниях оптимизатора одновременно. потом приводится ablation качества с количеством выбранных токенов. первоначально качество модели сильно улучшается с ростом k, но затем выходит почти на плато. вывод идея проста в реализации и довольно интуитивна. интересно, насколько хорошо полученные в статье результаты будут обобщаться на другие задачи, например, пресловутый instructing finetuning. да и glue содержит преимущественно последовальности длиной в несколько десятков токенов. идея об экономии памяти за счет использования части токенов не нова, например, работы по token pruning и merging dynamicvit, differentiable patch selection, tome и др предлагают оставлять только часть или объединять в один несколько токенов по ходу прогонки последовательности через трансформер. здесь же длина последовательности фиксирована и экономия достигается за счет backward pass. для полноты картины нехватает сравнения с gradient checkpointing по скорости шага обучения и расходу памяти."
103,2023-09-14T17:32:29,"instaflow one step is enough for high-quality diffusion-based text-to-image generation статья кода нет введение на текущий момент диффузионные модели - безусловный лидер среди различных семейств генеративных моделей. однако, их существенным недостатком является итеративная природа генерации, вынуждающая несколько раз прогонять сеть, чтобы получить выход приемлемого качества. а в идеале бы хотелось сразу из шума и промпта получать конфетку. для уменьшения количества шагов генерации были разработаны продвинутые солверы, способы дистилляции учителя в ученика с меньшим количеством шагов сэмплирования. но при стремлении количества шагов к единице, у всех методов неизбежно проседает качество генераций. в режиме одношаговой генерации новые архитектуры ganов, которые, как известно, не обучаются без жертвоприношений богам хаоса и разрушения, все еще остаются лучшими по качеству. была еще работа по consistency models, но ее не проверяли в практически интересном сценарии. возникает резонный вопрос - хорошее качество генерации при помощи диффузионных недостижимо за один шаг, или наш текущий уровень прогресса в области не позволяет этого достичь? в рассматриваемой предлагается способ одношагового сэмплирования, который, как утверждается, дает одновременно хорошее качество генераций и работает всего за один шаг."
104,2023-09-14T17:33:57,"метод процесс генерации диффузионной моделью можно рассматривать как перемещение в пространстве состояний из шума, обусловленного на что-то, в изображение или любую иную сущность . перемещение в пространстве состояний задает некоторую траекторию, вообще говоря, довольно произвольной формы, описываемую некоторым дифференциальным уравнением. уравнение не имеет аналитического решения, поэтому приходится решать его численно. в пределе бесконечно мало шага, траектория, полученная численными методами совпадает с истинным решением. но на практике количество шагов ограничено, и чем больше шаг, тем сильнее полученная ломаная отклоняется от кривой. отсюда возникает мысль - а что, если вместо того, чтобы улучшать солвер, выпрямить траектории? в пределе идеально прямой траектории от шума до картинки, самый простой солвер будет попадать идеально в яблочко. а чтобы выпрямить траектории, авторы предлагают rectified flow - метод итеративного выпрямления траекторий. суть метода заключается в следующем у нас есть изначальный метод генерации сеть солвер , выдающий некоторые траектории x t . а новая сеть должна за один шаг, стартуя из начальной точки x 0 , попасть в x t , но уже по прямому пути. а затем процесс повторяется с использованием сети с последней итерации для генерации траекторий. таким образом, постепенно траектории становятся все более и более прямыми. фиксированная точка итеративного процесса, когда прошлый генератор траекторий совпадает с текущим - и есть генератор идеально прямых траекторий. но процесс сходится к идеально прямым траекториям только в пределе бесконечного числа итераций, что недостижимо на практике. но оказывается, что уже пары итераций выпрямления достаточно для получения достаточно прямых траекторий. далее, последнюю модель с rectifiedflow дистиллируют в модель, предсказывающую картинку из шума за раз. в качестве функций потерь на данной стадии используют mse и the learned perceptual image patch similarity lpips , более коррелирующий с человеческим представлением о качестве изображения."
105,2023-09-14T17:35:13,"эксперименты за основу авторы берут stablediffusion v1.4 и ее же пробуют наивно дистиллировать по шагам на исходных траекториях и с использованием rectifiedflow. наивная дистилляция приводит к сильной просадке качества и некрасивым, нереалистичным картинкам, в то время как полученные при дистилляции с rectifiedflow картинки выглядят значительно лучше, хоть все еще не дотягивают до уровня базовой модели. сам rectifiedflow при том же числе шагов, что и базовая модель, имеет - то же качество. далее авторы, экспериментируют с разными параметрами и модификациями сети 2 этапов выпрямления достаточно, и большее количество шагов выпрямления уже не отражается существенно на качестве. для повышения выразительности сети, предлагается соединить как два вагона железнодорожного состава исходную сеть с ее копией тоже инициализированной весами stable diffusion . полученная сущность называется - stackedunet. при значительном приросте качества снижении fid с 20 до 13.7 время инференса растет не так сильно c 0.09с до 0.12c на сэмпл . архитектурный stablediffusion, которая учится предсказывать за один шаг - называется instaflow-0.9b. а stacked версия - instaflow-1.7b. конечный пайплайн обучения выглядит следующим образом 1 2 шага rectifiedflow, второй шаг с большим размером батча 2 дистилляция в instaflow с mse лоссом 3 дистилляция в instaflow с lpips лоссом почему дистилляция проводится с разными лоссами, а не их взвешенной комбинацией - не понятно."
106,2023-09-14T17:36:57,"instaflow-0.9b 1.7b в 30 24 раза быстрее базовой stablediffusion, хоть и с некоторой потерей в качестве 13.10 11.83 против 9.62 на замерах fid-30k на ms coco 14 . при том же бюджете на время, модель выдает сопоставимые метрики с ganами превосходя stylegan-t, но проигрывая gigagan . потом авторы сравнивают генерации stablediffusion и instaflow на разном количестве шагов генерации, и stablediffusion ожидаемо дает низкокачественные и мыльные картинки на малом количестве шагов, а instaflow - вполне приемлемого качества. если смотреть на конкретные пиксели при отдельности, то в stablediffusion их значение меняется по сложным траекториям, а у recitifiedflow почти по прямой. кроме того, мало чувствительна к guidance scale, что безусловный плюс и позволяет сэкономить по памяти так как не требуется дополнительно прогонять unconditioned расшумление в classifier-free guidance . авторы статьи утверждают, что не имеют обьяснения данному эффекту. вероятно, classifier-free guidance нужен, чтобы не сбиться с траектории генерации, а так как у нас теперть прямые траектории, то и слететь с них не так просто. вдобавок, полученная модель дает красивые интерполяции в латентном пространстве, по всей видимости, опять из-за выпрямленных траекторий генерации. instaflow из коробки комбинируется с refiner и superresolution в sdxl, и полученные картинки выглядят вполне себе сносно. вся процедура обучения занимает 108 для instaflow-0.9b и 130 для instaflow-1.7b a100 gpu дней, соответственно, что не так много по нынешним меркам stablediffusion v1.4 обучалась 6250 a100 gpu дней . обучение rectifiedflow и дистиллированной модели не успело сойтись до конца, так что при более длительном обучении качество может еще возрасти."
107,2023-09-14T17:37:23,"вывод метод довольно прост и интуитивен, хоть и поди догадайся так сам придумать. просадка по качеству генераций довольно заметна по сравнению с исходными моделями, но при таком ускорении результат все равно довольно впечатляющий. наверняка, кто-то из бигтехов исследовательских лабораторий с большими вычислительными ресурсами попробует воспроизвести идею. генерация картинок за прогон - это движение в сторону к real-time генерации, снижении расходов на генерацию контента, улучшение useer experience. будем посмотреть."
108,2023-09-20T11:16:00,"beyond surface statistics scene representations in a latent diffusion model статья кода нет диффузионные модели в процессе обучения нелегкому искусству расшумления картинок и прочих данных попутно выучивают представления, которые могут быть применены в других приложениях. ранее было показано, что на промежуточных активациях unet-а можно обучить линейный классификатор, который дает неплохое качество на задаче сегментации. кроме того, хоть обучающая выборка не содержит явно информацию о расположении объектов в трехмерном пространстве, диффузионные модели обладают понятием о 3d-геометрии. в ряде работ было продемонстрировано, что сеть для генерации изображений можно без файнтьюнинга превратить в генератор объёмных моделей. в данной работе, авторы находят еще 2 применения внутренним представлениям - salient object distinction и depth estimation. постановка в данной статье используются промежуточные представления stablediffusion. важно, что именно первой версии, ибо вторая обучалась с depth prior и явно получала информацию о картах глубины. тем интереснее, что сеть сама по себе имеет понятие о глубине и расстояниях. предполагая, что внутренние представления сети уже достаточно информативны себя, можно предположить, что даже простой классификатор, линейный слой поверх признаков будет давать неплохое качество. в данной работе берут признаки с разных attention слоев, так как ранее данные feature maps были наиболее полезны в прошлых работах. рассматривают две постановки задачи - discrete binary depth, где предполагается разделять примечательные объекты и фон, и continious depth estimation с вещественными метками."
109,2023-09-20T11:18:30,"эксперименты для обучения линейных классификаторов авторы генерируют датасет из 1000 картинок с помощью stable diffusion и размечают на salient object distinction и depth estimation при помощи моделей tracer и midas. промпты для картинок берут из laion-aesthetics v2. затем из датасета фильтруется непотребный контент и изображения без понятия о глубине. то, что остается, содержит 617 примеров и разбивается на обучающую и тестовую выборку в отношении 246 371 пример. метки имеют разрешение 512x512, а предсказания классификатора - куда меньшее разрешение скрытых представлений stable diffusion, потом последние интерполируют до размера конечного изображения. для оценки качества сегментации salient objects используется dice score и rmse для предсказания глубины. для saliency distinction depth estimation пробуют признаки с разных блоков unet-а и разные шаги диффузии. существенной разницы между выбором блока нет, а вот что примечательно понятие о глубине возникает на ранних шагах расшумления, еще задолго до того, как изображение напоминает что-то осмысленное. то есть расположение обьектов на сцене формируется еще до того, как из шума начинают выделяться обьекты. интересно, что даже карты активаций низкого разрешения позволяют выделить мелкие обьекты. stable diffusion со случайно инициализированными весами ожидаемо дает низкое качество линейного классификатора. авторы проверяют, что глубина заложена в unet-е, а не vq-vae, преобразующем из латентного пространства в пространство изображений. попытки обучить классификатор на признаках из vq-vae приводят к фиаско. вдобавок ко всему прочему, авторы демонстрируют, что с предложенный подход позволяет двигать сцену во время генерации. а именно, обученный классификатор выделяет некоторую исходную маску, есть некоторая желаемая позиция - левее, правее, ниже, выше. и с помощью градиентного спуска по предсказанному шуму, его модифицируют таким образом, чтобы целевой объект находился в желаемой позиции. фон при этом может сильно измениться."
110,2023-09-20T11:19:09,"вывод диффузионные модели, хоть и обучаются явно только на генеративное моделирование, в качестве приятного бонуса содержат в себе кладезь различной полезной информации, и в некотором роде могут рассматриваться как foundation models в области компьютерного зрения. задача предсказания шума является dense-prediction task, потому можно предположить, что признаки из диффузионок более подходят для сегментации детекции и прочих задачах с множественными предсказаниями по сравнению с image-level обучением - классификацией imagenet, clip, разными ssl методами."
111,2023-09-23T11:30:23,"longlora efficient fine-tuning of long-context large language models статья код в ряде приложений - суммаризации и написанных длинных текстов, ответов на сложные вопросы возникает необходимость работы с длинным контекстом. но обучать на длинных последовательностях, как и проводить инференс, очень затратно из-за квадратичной сложности attention, потому языковые модели обычно обучаются на контексте порядка нескольких тысяч токенов от силы. но на более длинных последовательностях качество резко просаживается. в литературе были предложены разные стратегии дообучения на длинные последовательности - но стандартное обучение вычислительно затратно, требует нескольких машин для современных llm. а стратегии, модифицирующие attention, retrieval-based подходы обычно несколько хуже по качеству. да и не все могут использовать преимущества эффективных cuda-кернелов а-ля flash attention. метод метод прост как пробка и относится к разряду - где-то я уже это видел, тем не менее любопытен. токены группируются по окнам некоторого размера порядка длины контекста на обучении и attention делается только в пределах этих окон. чтобы информация протекала между группами, в половине групп маска attention сдвинута на половину размера группы. технически это реализовано следующим образом - в половине голов attention применяются обычные окна, в другой половине - сдвинутые. метод явно вдохновлен работой по swin. группы перегоняются в размерность батча перед операцией attention, поэтому реализация не требует существенных усилий. другой источник дороговизны обучения - состояния оптимизатора, и потому авторы предлагают использовать lora для дообучения. наивное применение, без предложенной выше стратегии работы с attention экономит по памяти на градиентах и моментах adam, но не самих операциях и активациях, и кроме того, не обладает достаточной выразительностью, чтобы выучивать длинный контекст. предложенный же метод по сути работает с исходным attention, поэтому не требует существенного дообучения. для наилучшего качества оказывается полезным дообучать нормализационные слои и эмбеддинги, кои не сильно прибавляют к вычислительной сложности."
112,2023-09-23T11:30:24,"эксперименты предложенный метод валидируется на ряде бенчмарков по языковому моделированию с большим контекстом - pg19, proof-pile, topic retrieval на longchat. для обучения используется redpajama. longlora работает значительно лучше обычного файнтьюна с lora, и не сильно уступает полному дообучению там где это было посильно . сдвиг окон важен для качества, причем, если его делать в головах attention, а не в чередующихся последовательных блоках - качество немного выше. dilated, sparse attention на данных задачах показывают себя плохо. упомянутый ранее тюнинг эмбеддингов и нормализаций, называемый lora , неплохо накидывает. дообученная таким образом llama-2 13b выступает на одном уровне или даже бьет специализированные модели под длинный контекст такие как mpt-storywriter, longchat. вывод простая идея, которую можно быстро применить. не хватает однако сравнения с парой бейзлайнов - lm-infinite, например, не требующей вообще никакого дообучения. да и непонятно, насколько подход универсален."
114,2023-09-29T12:20:17,"pruning vs quantization which is better? статья кода нет прунинг и квантизация - две широко известные стратегии сжатия и ускорения нейронных сетей, с тем или иным успехом применяемые в различных задачах и приложениях. но если стоит выбор между этими двумя - то какой выбор следует сделать? принять красную или синюю таблетку? ранее в литературе не было полноценного сравнения прунинга и квантизации, и в статье авторы пытаются дать ответ на обозначенный выше вопрос. метод в данной работе авторы используют симметричное квантование к ближайшему значению на сетке round-to-nearest и magnitude неструктурированный! pruning, как простые для анализа и самые распространенные на практике. размер и качество модели сравнивается относительно fp16 модели."
115,2023-09-29T12:22:24,"эксперименты в первой части исследования авторы смотрят на норму разницы между исходными и сжатыми весами, или если быть точнее, log snr разницы. сначала рассматривают аналитические распределения - нормальное и обрезанное t-распределение стьюдента, чтобы моделировать тяжелые хвосты. для нормального распределения квадратичная ошибка, нормированная на плотность распределения для квантизации возрастает между узлами решетки, но при этом существенно ниже, чем максимальная ошибка у прунинга при той же степени сжатия. при уменьшении модели в фиксированное число раз ошибка квантизации всегда ниже таковой у прунинга. для t-распределения стьюдента при сильном сжатии и kurtosis меры тяжести хвостов распределения прунинг может быть лучше квантизации, однако такое распределение редко встречается на практике. затем авторы берут 46 моделек из torchvision и вновь смотрят на ошибку приближения весов и почти все оказывается, что при заданном сжатии у квантизации ошибка меньше. далее авторы смотрят на ошибку уже на выходе слоя и рассматривают более продвинутые алгоритмы прунинга и квантизации в сценарии post-training compression то есть без дообучения , оптимизирующие ошибку на уровне слоя. выводы, тем не менее, те же, что и раньше. но сохраняется ли преимущество квантования после обучения сжатой модели? оказывается, что да. авторы берут несколько архитектур - resnet, mobilenet, efficientnet, vit и прогоняют sparse training quantization aware training, соотвественно, сжатых моделей на ряде задач компьютерного зрения - классификации, сегментации, детекции, и почти всегда квантованная модель оказывается лучше запруненной. гиперпараметры процедуры обучения одинаковы в обоих случаях для честного сравнения. здесь стоить важную ремарку, что модели сжимают за один раз, в то время как для прунинга куда оптимальнее сжимать итеративно и тот же mobilenet-v3 вполне реально сжать до 87.5 с умеренной просадкой в качестве . вывод довольно интересное, хоть и сравнительно короткое исследование. наверное, основной вывод естественен, что небольшие пертурбации всех весов влияют на качество модели меньше, чем большие у части. вероятно, результат зависит еще от деталей процедуры обучения, в особенности, weight decay. и было бы интересно посмотреть на аналогичное исследования для языковых моделей. кроме того, прунинг можно комбинировать с квантованием и можно поставить задачу поиска оптимального соотношения между прунингом и квантованием. тем для будущих исследований предостаточно"
116,2023-10-02T14:46:26,"qa-lora quantization-aware low-rank adaptation of large language models статья код давеча коллеги из хуавей подогнали статью про квантование и дообучение больших языковых моделей. как известно, квантование позволяет значительно уменьшать размер модели и ускорять большие языковые модели, а низкоранговые адаптеры, упоминаемая чуть не в каждом посте lorа, дообучать в условиях ограниченных ресурсов. однако, квантование применяется к исходной модели перед ее дообучением, поэтому при вливании низкоранговых адаптеров в модель придется переквантовывать модель, что может привести к заметной просадке качества. и в этой статье авторы исследую причины, приводящие к просадке качества при переквантовании модели и предлагают способ бесшовного слияния lora c весами базовой модели. сразу скажу, что в статье есть несколько некорректных утверждений и ослабления бейзлайнов, по невнимательности или по злому умыслу. наиболее близкая по теме статья - разобранная ранее qlora. напомню, что там модель квантуется в 4 бит, и поверх квантованной модели обучается низкоранговый адаптер на instruction finetuning. авторы qa-lora утверждают, что выгода от этого подхода только во время обучения, так как на инференсе все равно придется сливать веса с адаптерами. но данное утверждение более чем спорно, ибо можно параллельно прогонять вход через квантованные веса и floating-point адаптер, и накладные расходы на последний довольно маленькие так как типичный ранг добавки r в сотни и тысячи раз меньше размерности в сети ."
117,2023-10-02T14:48:44,"метод наивное квантование работает не очень хорошо из-за несовпадения степеней свободы у квантования и низкоранговой добавки. на каждый входной канал приходится один скейл и один zero вообще-то нет , и при этом r чисел в адаптере. но чтобы можно было просто взять и поменять параметры квантизации, нужно чтобы все r чисел в адаптере, соотвествующие конкретному ряду матрицы, были одинаковы. что по сути ограничивает lora одноранговой добавкой. чтобы как-то повысить выразительность, предлагается о боже! квантовать входную размерность малыми группами, и тогда ранг добавки следует делать равным числу групп. и после этого можно сливать добавку без проблем. тут стоит напомнить, что маленькие группы размера 64 , которые дополнительно квантуют, - одна из ключевых идей в qlora. эксперименты чтобы провалидировать предложенный подход авторы квантуют модель с помощью gptq с размером группы 32 и дообучают lora на alpaca и flanv2. для валидации используются mmlu и ряд других стандартных бенчмарков языковых моделей - arc, hellaswag, piqa, замечу, что используемый размер группы дает более, чем 0.5 бит на параметр, что не пренебрежимо мало. qa-lora на 4 битах несколько уступает qlora без вливания но уверенно бьет варианты с вливанием весов и повторным квантованием, как и peqa с дообучением скейлов в квантизации. метод неплохо себя показывает при низких битностях, давая качество статистически выше случайного даже при двух битном квантовании качество случайного классификатора на mmlu - 25 . далее авторы смотрят на эффект от размера группы и ожидаемо меньшие группы дают лучшее качество, так как с одной стороны и приближение исходных весов лучше, и больше обучаемых параметров в lora. размер подвыборки flanv2 заметно влияет на качество, особенно при квантовании в низкую битность. выводы интересная постановка задачи и подход, однако мотивация метода строится сразу на нескольких неверных утверждениях - дороговизне инференса qlora , отсутствия квантования малыми группами в qlora. используемые группы даже меньше, чем в qlora, потому расходы на хранение статистик квантования, как было выше сказано, довольно существенны."
119,2023-10-08T19:58:25,"scaling laws for sparsely-connected foundation models статья кода нет и хрен с ним введение как известно, foundation модели, обученные на колоссальных объёмах данных, демонстрируют scaling laws - сравнительно простые зависимости качества работы с изменением количества данных и размера модели, выражающиеся обычно степенными законами. и на основе этих закономерностей подбирают модели оптимальные по размеру и количеству данных при заданном ограничении на бюджет обучения. прунинг весов - одна из стандартных методик по уменьшению и ускорению моделей, приравнивающая нулю некоторую долю весов. тем самым, при заданной размерности активаций нейронной сети суммарное количество ненулевых параметров меньше, чем у плотной модели. возникает вопрос - может ли случиться так, что разреженная сеть большей размерности будет оптимальнее плотной сети с аналогичным количеством обучаемых параметров с меньшей внутренней размерностью? и в приведенной работе, авторы впервые проводят систематическое исследование по масштабированию спарсных сетей. постановка задачи авторы рассматривают 2 задачи - 1 обучение t5 на корпусе с4 2 обучение vit на jft-4b проприетарный гугловский датасет для vit рассматривают 7 моделей размера от 0.66m до 42.4m параметров, и 4 конфигурации количества шагов обучения, а для t5 - 4 модели от 1.3m до 85m параметров и 3 конфигурации длительности обучения. рассматривают 4 уровня прореживания - 0 , 50 , 75 , 87.5 . менее 50 не целесообразно рассматривать обычно на практике, а выше 87.5 оптимизация становится затруднительной. рассматриваемые датасеты настолько велики, что ни в одном из сценариев, модель не успевает проделать более одной эпохи, тем самым постановка эксперимента удовлетворяет предположению о бесконечности выборки, из которой сэмплируются данные. первые 25 времени обучение обучается плотная модель, следующие 50 времени обучения уровень прореживания постепенно поднимается до целевого значения, и последние 25 модель обучается с постоянным прореживанием."
120,2023-10-08T19:59:54,"эксперименты вывод закона скейлинга авторы отталкиваются от стандартной формулы scaling law по количеству данных и размеру модели. в ней три аддитивных члена 1 спадающий степенным образом с размером выборки. 2 спадающий степенным образом с размером модели. 3 неустранимая ошибка - некоторая константа. однако не очевидно, каким образом sparsity будет входить в конечный закон. важно лишь общее количество параметров или sparsity может дать некоторое преимущество по сравнению с dense моделью того же размера? чтобы угадать форму закона, авторы прогоняют эксперименты с перечисленными выше конфигурациями и обнаруживают, что 1 графики лосса против количества параметров образуют почти параллельные линии 2 чем выше степень прореживания, тем меньше лосс, но выигрыш от прореживания быстро спадает с ростом степени сжатия. 3 форма кривых лосса против количества параметров почти не зависит от количества данных. из наблюдений выше возникает анзац для scaling law c прореживанием. вместо константы помноженной на степень от размера модели, возникает степень доли ненулевых параметров некоторая константа. полученный анзац весьма неплохо согласуется с экспериментальными данными, и кроме того, экстраполируется на большие модели. например, вдобавок к конфигурациям t5-моделей рассмотренных в работе впридачу берут t5-xl, на порядок большую самой большой модели из списка, которая тем не менее хорошо ложится на выведенную зависимость."
121,2023-10-08T20:02:23,"оптимальная степень прореживания исходя из полученных scaling law можно вывести оптимальную степень прореживания при заданном количестве ненулевых параметров и flops, потраченных на обучение. чем больше перебор по длительности обучения по сравнению с chinchilla law, тем выгоднее иметь более сильное прореживание. авторы рассматривают два способа оценки количества flops - 1 в предположении, что железо умеет идеально пропускать операции с нулевыми весами, 2 и в типичном сценарии - где операции со спарсными матрицами требуют столько же вычислений, как и с произвольными. оба метода будут отличаться на некоторый фактор. контуры pretraining flops против non-zero parameters для разных значений sparsity, при которых данная степень прореживания оптимальна, удивительным образом оказываются параллельны scaling law chincilla. однако, даже для chincilla law оптимальным является ненулевое значение sparsity. в предположении идеальной утилизации спарсности при обучении 50 модель требует в примерно 2 раза для vit , 3 раза для t5 больше flops чем предсказание chincilla law для dense модели. а без этого предположения 5x и 70x, соотвественно. кажется, что это слишком много, но на самом деле современные языковые модели обучаются нередко значительно больше, чем compute-optimal значение. например, llama-2-7b обучалась в 14 раз больше. дальше можно задаться вопросом, а какого размера должна быть плотная модель при том же количестве flops, чтобы иметь при том же количестве то же качество, что и sparse модель? для 50 sparse модели соответствующая ей модель должна быть в 1.6x раз больше, и для 75 - 2.16x, т.е преимущество от sparsity уменьшается с увеличением степени прореживания. примечательно, что закономерность примерно одна и та же, для vit и t5, хотя модели и задачи совершенно разные. затем авторы исследуют scaling laws для полуструктурированной n m sparsity n ненулевых значений . данный паттерн более ограничителен, потому закономерно ожидать, что он менее эффективен, чем произвольное расположение нулевых ненулевых весов. 2 4 паттерн поддерживаемый nvidia gpu начиная с ampere не уступает 50 неструктурированной sparsity, а вот 1 4, 2 8 уже заметно хуже с точки зрения масштабирования, чем 75 sparsity."
122,2023-10-08T20:03:18,"прореживание предобученных моделей на практике чаще берут обученную модель и сжимают ее. авторы берут три модели vit s 16, m 16, b 16 s, m, b - small, medium, base, а 16 - размер патча и прореживают их тем же самым способом, что sparse модели в экспериментах выше только прореживая сразу, а не через 25 времени обучения , используя 5.6 бюджета на обучение плотной модели. для 50 , 75 сжатия такой способ в 5 4 раз эффективнее чем обучение sparse модели from scratch, но при большем сжатии выигрыш уменьшается. по всей видимости, причина этого в том, что модель сильно просаживается, по сравнению с исходной плотной. если учитывать бюджет обучения плотной модели в суммарных затратах на создание sparse модели заданного качества, то генерация sparse модели с нуля значительно эффективнее. вывод весьма интересное и нужное исследование, мотивирующее дальнейшую разработку железа и алгоритмов, способных работать с прореженными матрицами. при фиксированной производительности и памяти железа, по всей видимости, оптимальнее всего будет брать большую насколько возможно модель с некоторой долей нулевых весов и квантованную в низкую точность. дальнейшее повышение эффективности могут дать conditional sparsity архитектуры, использующие часть параметров на прямом и обратном проходе как пресловутые смеси экспертов и retrieval-augmented модели."
123,2023-10-09T11:45:15,"подумал я, что длинные полотна, подобные shine on you crazy diamond, с детальным описанием экспериментов выходят чрезмерно многословными и содержат слишком многа букв . и думаю сократить среднюю длину постов, акцентируя внимания только на ключевых моментах и результатах. тем самым заодно можно будет повысить и throughput постов."
125,2023-10-09T20:39:20,"так как голоса, по всей видимости, разделяются поровну - сделаем гибридный формат"
126,2023-10-11T00:17:24,"в pytorch 2.1 завезли 2 4 semi-structured sparsity! напомню, что произвольный паттерн неструтурированного прунинга не поддерживается на gpu, но начиная с поколения ampere реализована аппаратная и программная поддержка 2 4 паттерна, где на 2 позициях из 4 стоят ненулевые веса. в общем случае n m sparsity n ненулевых весов из m. до недавних пор, чтобы воспользоваться 2 4 sparsity нужно было напрямую использовать ядра из cusparse или cutlass или пользоваться скомпилированной с --sparsity моделью в tensorrt. но теперь semi-structured sparsity доступна почти всем желающим. примечание . 2 4 sparsity работает только на ampere hopper. не ожидайте ускорения на колабовской t4. теоретическое ускорение и сжатие, которое дает такой паттерн, 16 9 для half precision и 16 10 для int8. но на практике, увы не все так радужно. реальное ускорение инференса в районе 10-30 в хорошем сценарии . сжатие в 2 4 без дообучения обычно заметно просаживает качество за исключением совсем больших моделей . но его можно быстро восстановить, дообучая на целевой задаче. в приведенном примере c bert качество сжатой модели такое же как у и исходной после дообучения. при маленьких батчах инференс bert c 2 4 sparsity работает медленее, чем dense матричные операции, но с ростом батча выигрыш от разреженности становится заметным и стабилизируется в районе 20-30 . заметим, что в bert сравнительно небольшие матрицы, и на условной llama выигрыш будет наблюдаться уже на батче с 1-2 последовательностями. документация в торче туториал с прунингом bert статья nvidia про 2 4"
127,2023-10-13T11:08:02,"ecoflap efficient coarse-to-fine layer-wise pruning for vision-language models статья код есть, но не почищен большинство методов прунинга сжимают все слои нейронной сети с одинаковой силой . естественно предположить, что слой слою рознь, и какие-то можно сильно проредить, не потеряв заметно в качестве, а спарсификация других же, напротив, будет очень болезненно восприниматься сеткой. потому, при некотором целевом уровне сжатия отпимальным будет скорее всего неоднородное распределение процента нулевых весов по разным слоям. но как найти это распределение? метод современные sota-методы сжатия llm-ок опираются обычно на послойное приближение гессиана, которое ничего не знает о влиянии данного слоя на конечный выход. как вариант можно оценить важность слоя с помощью градиентов модели, в частности, используя диагональ матрицы фишера. но для многомиллиардных моделей подсчет градиента и хранение промежуточных активаций - удовольствие не из дешевых, потому в данной статье предлагается заменить честный градиент его приближением нулевого порядка. а именно, сэмплируется гауссов шум, считается разница значений лосс функций при сдвиге в положительном и отрицательном направлении от данной точки и делится на величину шума - производная по случайному направлению по существу. по-хорошему, требуется много сэмплов, чтобы оценить аккуратно градиент. но в этой работе утверждают, что одного достаточно. вооружившись определенным выше критерием важности слоев, можно для каждого слоя задать свою степень сжатия и применить известный метод прунинга. эксперименты авторы валидируют метод на blip-2 задача vqa , flan-t5 на mmlu , eva-vit на imagenet-1k . не то, чтобы подсчет градиентов был непосильной задачей для моделей этого размера, но зато можно оценить качество приближения нулевого порядка по сравнению с честным фишером. в качестве метода прунинга используется wanda sparsegpt, где заменили послойный гессиан его диагональю и модели не дообучаются после прунинга. предложенный метод неплохо накидывает по сравнению с uniform sparsity при сжатии 50-60 , однако просадка все же довольно существенна по сравнению с плотной моделью во всех трех сценариях. приближение градиента разностью почти не просаживает качество по сравнению с использованием истинного градиента. найденные sparsity распределения тоже близки. метод сильнее прореживает текстовую модель в blip чем vision. большее количество шумов для оценки градиента не влияет на качество. вывод на текущий момент при определении важности и влияния слоя на конечный выход модели человечеству приходится оперировать эвристиками, основанными на величине активаций, градиентов, но исчерпывающего, удовлетворительного решения пока нет. разработка некоторого надежного, подкрепленного теоритическими гарантиями метода позволит сделать большой шаг в сжатии моделей и построении эффективных архитектур. странно, что статья хоть и предлагает метод, экономичный по памяти, не валидируется на условной llama llama-2 ."
128,2023-10-23T22:14:15,"monarch mixer a simple sub-quadratic gemm-based architecture статья код есть, но непричесан введение линейные слои и свертки имеют в общем случае квадратичную сложность по числу каналов в сети. аналогично, операции, обрабатывающие глобальный контекст - attention, свертки с большим ядром, пространственные проекции в mlp-миксерах квадратичны по длине последовательности пространственной размерности. для специальных классов матриц преобразований можно добиться субквадратичной сложности - разреженных, блочно-диагональных или обладающих определенной структурой. и в качестве иллюстративного примера уместно привести быстрое преобразование фурье, имеющее сложность o n log n . однако, логично ожидать, что специальные классы матриц будут обладать меньшей выразительностью по сравнению с линейными преобразованиями общего вида, потому хочется найти такой класс - который бы с одной стороны давал существенную экономию по вычислениям, и в то же время не терял в качестве. и в этой работе, авторы предлагают заменить трансформерные блоки на поканальные и пространственные проекции, параметризованные матрицами monarch. метод матрицы monarch впервые появились в работе. идея довольно простая - каждая матрица параметризуется последовательностью блочно-диагональных матриц, между которыми вставлены матрицы перестановок. перестановочные матрицы эффективно обеспечивают dilation между размерностями матрицы. вообще говоря, можно рассматривать последовательность нескольких dilation вида 1, n 1 p , n p-1 p , но в данной для простоты работают с p 2. количество параметров в такой матрице и сложность вычислений пропорциональна o n 3 2 . в исходной статье про monarch замена стандартных линейных слоев на monarch в трансформерах сработала весьма недурно. а в этой статье пошли дальше и используют монархов для обработки последовательности. m2-миксер состоит из sequence mixer, содержащего длинные свертки и gating, и dimension mixer - обычная mlp с монархами в качестве линейных слоев."
129,2023-10-23T22:15:42,"эксперименты предложенную архитектуру валидируют на ряде задач и архитектур 1 masked language modeling на bert 2 классификации изображений с vit 3 causal language modeling по типу gpt. при тех же скрытых размерностях m2-bert, m2-vit имеет на 25-27 меньше параметров, чем базовая transformer модель, при этом не уступая в качестве. а если увеличить ширину модели, чтобы сравняться по числу параметров, то имеет место даже некоторый прирост. на causal-language modeling предложенная модель тоже показывает себя немного лучше gpt-2. monarch mixer заметно лучше масштабируется с длиной последовательности в соотвествии с теоретическими выкладками. на больших длинах последовательностей прирост порядка 9x против bert-base с vanilla attention и 2x против flash attention. ускорение наблюдается как на gpu, так и на cpu. вывод использование структурных матриц вместо матриц общего вида - интересное направление в разработке нейросетевых архитектур. при том же количестве вычислений возможно работать с картами признаков большей размерности. однако, без крупномасштабных экспериментов на современных больших языковых моделях нельзя доподлинно оценить, обладают ли монархи и иные представители бабочек лучшей масштабируемостью по сравнению с первозданным трансформером. кроме того в сравнениях по throughput везде фигурирует flash-attention-1, а есть flash-attention-2 с еще лушчей утилизацией железа."
130,2023-10-29T18:57:55,"convnets match vision transformers at scale статья кода нет, но у вас все равно нет jft-4b и tpuv4, чтобы воспроизвести экспы народная мудрость гласит, что трансформеры масштабируются лучше, чем сверточные сети при наличии большого количества данных и бюджета на обучение. но убедительного экспериментального подтверждения в поддержку данной гипотезы никто не проводил в convnext-v2 имеется сравнение эффективности mae-предобучения по сравнению с vit, но на меньших масштабах данных исследователи из deepmind прогнали ряд сверточных сеток на большом датасете и продемонстировали, что cnn масштабируются не хуже трансформеров. метод эксперименты в качестве семейства сверточных сетей авторы рассматривают nfnet-ы, одно из последних достижений в разработке cnn, до того как vitы завладели нишей. разные модели семейства отличаются глубиной больше номер - больше глубина , и шириной - более широкая модель . обучается все хозяйство на проприетарном jft-4m и чипах tpuv4. результаты сверточные сети удовлетворяют похожим степенным законам убывания лосса от бюджета обучения, что и трансформеры. при сопоставимых бюджетах сверточные сети выдают сопоставимое качество. авторы учитывают использование более новых tpuv4 против tpuv3 в прошлых работах и потому дают поправку при сравнении бюджетов. самые большие модели из семейства, обученные в течение 100k tpu часов выдают 90 топ-1 точности на imagenet-1k при дообучении на нем. использование repeated augmentation для самой большой модели накидывает аж 0.1 на imagenet-1k. вывод очередной голос в пользу того, что важны не сколько архитектурные детали, сколько количество данных и время обучения. на большом объеме данных inductive bias уже не имеет особого значения, но и трансформеры, вероятно, не обладают чудесными свойствами с точки зрения масштабируемости. тем не менее, для пущей убедительности в данном отчете было бы неплохо увидеть аналогичные кривые лосса против часов обучения для семейства vitов. три точки, отвечающие трансформерам, недостаточно репрезентативны. интересно, если провести подобный эксперимент для lstm, с поправкой на утилизацию железа, будет ли наблюдаться подобная картина?"
132,2023-11-04T19:44:35,"qmoe practical sub-1-bit compression of trillion-parameter models статья код сегодняшний пост можно считать юбилейным. мой скромный канал преодолел психологический рубеж в 100 подписчиков, за что большое спасибо милане milana_shhanukova введение современные алгоритмы post-training-quantization вполне успешно квантуют большие языковые модели в 3-4 бита, благодаря чему многомиллиардные модели становится возможным помещать на одну видеокарту. но, что если поставить еще более амбициозную задачу - уместить на одном хосте модель с триллионом и более параметров, например, гугловский switch transformer c 1.6t весов. сие чудище настолько огроменное, что при квантизации в 4 бита не поместится даже на целую стойку a100, что уж говорить о более скромном сервере из rtx3090. оказывается, что данные модели обладают замечательной сжимаемостью без заметной просадки в качестве их можно квантовать менее чем в один бит, что позволяет поместить самый большой switch transformer на 8 rtx 3090 24 gb или 4 a6000 48 gb . метод рассматриваемый в этой статье класс моделей - смеси экспертов mixture of experts moe - состоят из attention блоков, которые используются для всех входов, и множества реплик от 128 до 2048 в семействе switch transformer полносвязных блоков 2-слойных сетей , называемых экспертами, каждый из которых обрабатывает только часть входных данных. специальная сеть роутер , предсказывает, к какому эксперту направить данный токен. при обработке последовательности, для каждого ее элемента используется только часть параметров, что дает существенную экономию вычислений по сравнению с обычным трансформером эквивалентного размера, сохраняя при этом его выразительность. однако сама модель остается все еще очень большой и занимает много места в памяти. деваться некуда - надо сжимать."
133,2023-11-04T19:46:25,"gptq ранее показал сильный результат при квантовании больших языковых моделей, потому авторы статьи его же создатели берут его за основу. однако, наивное применение сопряжено с определенными сложностями - для оценки гессианов нужно прогнать последовательности через каждого эксперта, которые хоть и небольшие, но их много. авторы прогоняют последовательность через блок, и для каждого эксперта сохраняют в оперативную память активации, нужные для оценки гессиана, и распределение токенов. чтобы повысить скорость алгоритма, квантизация нескольких экспертов проводится параллельно. 16 экспертов, обрабатываемых параллельно дают прирост 6x по сравнению с обработкой каждого эксперта по отдельности. чтобы все завелось, потребовался еще ряд мелких хаков 1 более сильная регуляризация 2 использование округления к ближайшему целому вместо gptq, если матрица, оказалась плохо обусловленной 3 ограничение на число токенов для каждого эксперта, чтобы не создавать большие тензоры. кроме того, полезным оказалось не учитывать специальные токены при калибровке. для сжатия весов используется поканальная тернарная квантизация, когда каждый вес принимает только 3 значения - минимальное, 0 и максимальное. обычно веса имеют нормальное распределение, сконцентрированное около 0, потому значительная доля весов 85-90 становится нулевой. потому возникает идея как-то использовать это свойство. стандартные форматы хранения sparse матриц с метаданными ограничены по сжатию, а кодирование хаффмана с переменной длиной кода не получается эффективно реализовать на gpu. потому авторы предложили свою схему энтропийного кодирования с фиксированной длиной символа, но кодирующее переменное число бит, которое может быть эффективно реализовано на gpu. сжатие не шеннон-оптимально, зато процедура кодирования позволяет эффективно использовать gpu. и далее описывается процедура и реализация cuda kernel."
134,2023-11-04T19:48:12,"эксперименты авторы рассматривают семейство моделей switch transformer - 7b, 26b, 1.6t параметров. по существу это encoder-decoder t5-модель с параллельными feed-forward блоками. switch transformer обучался на masked lm на c4, потому метод тестируют на валидационной выборке из этого датасета, замеряя перплексию. вдобавок, замеряют аналогичным образом качество на нескольких подмножествах из redpajama open-source версии датасета llama . квантуются только эксперты, attention слои, занимающие крохотную долю общего числа параметров держатся в исходной точности. тернарная квантизация с энтропийным кодированием дает 0.8 бита на параметр, при этом полученная модель не сильно просаживается в качестве по сравнению с исходным представлением в bf16. таким образом, имеем сжатие модели примерно в 20 раз. удивительно, что и квантование к ближайшему соседу работает очень неплохо, хоть и заметно хуже gptq. прогонка алгоритма на одной a6000 занимает менее дня для самой большой модели. предложенная процедура кодирования весов дает небольшой оверхед к общему времени вычислений и итоговый прирост времени выполнения - всего 5 по сравнению с тем, что было бы у исходной модели, если ее можно было бы уместить на одну стойку как я понимаю, оценивали скорость на основе одного блока attention полносвязные эксперты . что же делает экспертов настолько легко квантуемыми? по всей видимости, сама процедура обучения и инференса, предполагает, что в большинстве вычислений данный эксперт не будет задействован, потому модель устойчива к отсутствию операций в большинстве слоев. кроме того, накопление ошибки меньше, так как есть эффект от округления только части весов, а используемые во всех вычислениях attention слои неизменны. обсуждение в github issue. вывод крутой и практически полезный результат, мотивирующий дальнейшее развитие и большее широкое применение смесей экспертов. в то время как обычные языковые модели плохо сжимаются менее чем в 3 бита без значительной просадки в качестве, moe допускают куда более агрессивное сжатие. сама концепция moe и условных вычислений кажется более чем естественной для foundation моделей - для решения задачи по алгебре, вряд ли полезно знать название всех персонажей в сильмариллоне или формулу тринитротолуола. gpt-4 по слухам представляет собой смесь экспертов. сдается мне, llama-3 тоже будет moe запомните этот твит ."
135,2023-11-07T11:18:50,"в недавнем ролике канал veritasium затронул важную проблему популяризации науки. научный прогресс носит преимущественно инкрементальный характер, большие прорывы если не говорить про dl происходят редко и ценность многих результатов трудно обьяснить неподготовленной публике. потому популяризаторы науки и сми прибегают к обману, чтобы набрать классов. кликбейтные заголовки - greatest breakthrough of the 21 st century, future has come, вечный двигатель возможен, и тому подобное. даже уважаемые издания, например, quanta magazine, грешат этим - в статье про кротовую нору они утверждают, что ученые создали кротовую нору, хотя в действительно речь идет о моделировании ее с помощью квантового компьютера. результат на самом деле выдающийся, но тем не менее, до межгалактических путешествий, доставки алиэкспресс с альфа-центавры еще далеко. результаты, опровергнутые впоследствии, цитируются в среднем больше. недавняя история с lk99 вызвала бурный ажиотаж в научном сообществе, множество лабораторий попытались воспроизвести результат. в скорости громкий результат был опровергнут. но внимание которое привекли к себе ребята из кореи, среднестатический ученый не привлечет за всю свою научную жизнь. причина данного явления понятна - ученым популяризаторам науки выгодно поднимать хайп, накручивать просмотры. все мы в конечном итоге боремся за место под солнцем. потенциальные дивиденды большие, а в случае неудачи - никто и не вспомнит. как бороться с подобной ситуацией в науке? автор ролика и его собеседники предлагают поднимать эту проблему, говорить о ней, держать сообщество в курсе. окончательно побороть ее вряд ли возможно. на мой взгляд, основный потенциальный ущерб от недобросовестной популяризации науки, в том, что некоторые интересные и полезные отрасли могут не получать должного финансирования со стороны курирующих структур и насыщения кадрами. тем не менее, пассионарии и ученые верящие в перспективы, полезность своей деятельности, или просто получающие кайф от нее, по всей видимости, будут заниматься ей вне зависимости от внешних обстоятельств. то же машинное и глубокое обучение до того, как проникнуть во все отрасли жизни, получить широкое признание, долгое время развивалось в относительном андерграунде."
136,2023-11-11T12:11:02,"нет, это не печать с раскладкой на другом языке, и не вывод llm, квантованной в один бит. это список флагов интеловского процессора."
137,2023-11-21T11:55:52,"deja vu contextual sparsity for efficient llms at inference time статья код гугл выдаст кучу статей с дежавю в названии do ssl models have déjà vu? a case of unintended memorization in self-supervised learning. dejavu conditional regenerative learning to enhance dense prediction deja vu continual model generalization for unseen domainsdeja-vu a glimpse on radioactive soft-error consequences on classical and quantum computations déjà vu a contextualized temporal attention mechanism for sequential recommendation déjà vu an empirical evaluation of the memorization properties of convnets так что авторы немного прогадали с оригинальностью названия перейдем же к сути дела. введение как известно, нейронные сети перепараметризованы, и имеет место значительная избыточность в весах моделей - значительную долю весов можно отбросить без заметной просадки в качестве. на этом свойстве основаны методы прунинга. тем не менее, чтобы решать широкий круг задач foundation модель должна обладать значительным количеством параметров, чтобы хранить в себе большой обьем знаний. потому добиться существенного сжатия и ускорения без просадки в качестве крайне затруднительно. однако, для конкретного запроса, будь то последовательность или иной тип входа, требуется лишь малая доля аккумулированного в модель знания. чтобы взять интеграл от функции не требуется быть египтологом или знать всех представителей рода соколиных."
138,2023-11-21T11:56:51,"метод ускорить время работы сети без просадки в качестве можно не используя часть голов в attention или каналов в feedforward слоях. первая часть работы посвящена анализу внутренних представлений внутри сети. авторы делают два прямых прохода на первом определяют каналы и головы с наибольшими значениями активаций, и на втором проходе используют только долю параметров, соответствующую самым большим активациям. оказывается, что таким образом можно пропустить до 80 голов в attention и до 95 в mlp слоях без существенного изменения выхода модели. кроме того, оказывается что в большинстве голов attention размазан равномерно по токенам, и эти головы не выполняют никакой полезной работы. при определении важных неважных для конкретного входа параметров описанным выше образом приходится делать один проход с использованием всех параметров, потому толку от него как от козла молока. можно ли как-то заранее определить нужные каналы? авторы рассматриваемой статьи делают следующее берут обученную языковую модель и поверх нее обучают слои предсказывать, насколько релевантен вход данному каналу ffn голове трансформера. на инференсе задается доля весов, которую мы хотим использовать, и берутся только измерения с самым высоким предсказанным скором. выбор нулевых весов зависит от входа то есть контекста - потому sparsity носит прилагательное contextual. предсказывать важность канала головы на основе активаций текущего слоя оказывается технически неэффективно, так как определение используемых для данного входа параметров и прогонка через блок должны осуществляться последовательно, что снижает утилизацию железа. потому предлагается использовать активации с прошлого блока для выбора. благодаря наличию residual connections активации слабо меняются от блока к блоку, потому активации прошлого блока служат хорошим приближением активаций в текущем блоке, и предиктор можно прогонять параллельно с attention и mlp блоком."
139,2023-11-21T11:58:00,"эксперименты авторы валидируют свой подход на больших моделях из семейства opt 66b, 175b и bloom через замеры перплексии на wikitext c4, и zero-shot бенчмарках из lm-eval-harness. contextual sparsity в районе 50-60 даже слегка накидывает в качестве, и до 80 не просаживает качестве по сравнению с исходной моделью. deja vu ускоряет генерацию с opt-175b в 1.8-2x по сравнению с fastertransformers и 4.8-6x c реализацией трансформера в huggingface сильно неоптимальной при contextual sparsity 75 для bloom результаты и выводы аналогичные. затем авторы смотрят на contextual sparsity с ростом количества одновременно подаваемых последовательностей, и оказывается, что количество каналов голов с большим значением активаций растет медленнее, чем линейно с размером батча. причем в первых слоях, активируются одни и те же каналы и головы во всех последовательностях, и различие возникает в более поздних блоках. deja vu можно совместить с другими методами сжатия и ускорения, в частности, квантизацией. вывод с одной стороны логичное и в то же время интересное наблюдение. интуитивно понятно, что все заложенное знание в foundation модель не требуется для конкретного запроса, но вопрос о том, как эффективно извлекать это знание, не перебирая при этом половину книг в эдакой импровизированной библиотеке. результаты для opt выглядят весьма впечатляюще, однако отсутствие подобных экспериментов на более современных и эффективных моделях, пресловутой llama к примеру, вызывает смутные подозрения. по всей видимости, столь высокая разреженность активаций как-то связана с недообученностью и compute-неоптимальностью моделей. тем не менее, contextual sparsity - перспективное направление для развития эффективных foundation моделей."
140,2023-11-23T16:11:18,"exponentially faster language modelling статья код только мы на днях разбирали contextual sparsity, как dl сообщество сотрясла статья, реализующая данную концепцию в экстремальном объеме. согласно аннотации, вариация bert, предложенная в статье, которую без лишней скромности нарекли ultrafastbert, выдает умопомрачительные показатели по эффективности прямой проход с использованием всего 0.3 параметров ускорение в 78 раз по сравнению с оптимизированной реализацией прямого прохода ну все, значится. теперь bert можно гонять хоть на старой нокии или калькуляторе. agi в кармане - теперь дело времени. так ли все замечательно? ан нет. первый нюанс заключается в том, что оптимизируются только feedforward блоки, а attention блоки остаются без изменения. авторы мотивируют это тем, что bert обычно обрабатывает последовательности длины порядка 128, где основные вычисления происходят в feedforward. это действительно так, тем не менее, attention занимает все равно нетривиальную долю вычислительного бюджета, потому всю модель в десятки раз ускорить не выйдет."
141,2023-11-23T16:13:15,"теперь про сам метод. fastfeedforward network, которая предлагается в качестве альтернатива обычной ffn, представляет собой следующее две матрицы размера 2 d - 1 x h, где d глубина дерева, а h - embedding dim, соответствующие входной и выходной проекции, и функция активации. вернее даже правильнее будет сказать, что это последовательности матриц размеров 1, 2, 2 d-1 , отвечающие разным уровням дерева. прямой проход выглядит следующим образом для входной последовательности на каждом уровне l дерева считается скалярное произведение со столбцом соответствующим узлу, в который мы попали по результатам предыдущего шага, матрицы весов для данного уровня, и если полученное скалярное произведение положительно, то переходим в правый узел, и в левый узел в противном случае. проходясь от корня к листьям, записываем индексы узлов на всех уровнях и результат произведения. итого имеем после первой проекции матрицу индексов n вершина размера длина последовательности на глубину дерева, и матрицу логитов l - промежуточных активаций. к матрице логитов применяем функцию активации и считаем произведение с матрицей выходной проекции, откуда мы берем столбцы из матрицы индексов n. асимптотическая сложность алгоритма - o log n по промежуточной размерности против o n у обычный ffn из двух линейных слоев и активации. красиво? красиво. несколько таких деревьев можно прогонять параллельно. вариации описанной выше архитектуры называют ultrafastbert-kxd, где k - количество деревьев, а d - глубина дерева. внутренняя размерность, таким образом, равна k x 2 d - 1 . в частном случае k - деревьев глубины 1 имеем привычную архитектуру трансформера."
142,2023-11-23T16:15:14,"эксперименты ultrafastbert - бертоподобная модель с 12 блоками с деревьями глубины 11 т.е 4095 2 12 - 1 нейронами в скрытой размерности ffn . в качестве бейзлайна авторы берут crammedbert из статьи, где обучили бертоподобную модель на mlm до качества, немного уступающему оригинальному bert за день на одной gpu. сравниваются со стандартной версией со скрытой размерностью 3072, и версией, в которой feedforward_dim 4095. из этой же статьи берут гиперпараметры обучения. для валидации замеряют точность на бенчмарках из glue. авторы обучают вариации ultrafastbert разной глубины, от 1 до максимально возможной, при примерно постоянной ширине. по качеству все смотрится довольно неплохо с ростом глубины среднее качество на glue просаживается, что ожидаемо, учитывая, что все меньше и меньше параметров используется на инференсе, но не драматически. сильнее всего страдает качество на cola, оно и тянет усредненный результат вниз. гораздо веселее история с замерами. свою реализацию fff авторы сравнивают со своими же самописными реализациями операций для бейзлайновой архитектуры. сетап сравнения тоже довольно экзотический - батч размера 128 последовательностей длины 128, и в качестве l1 бейзлайна они пробегают циклом по всем строкам матриц и вызывают скалярные произведения. в качестве l2 бейзлайна копируют матрицы весов batch_size раз и запускают batched matvec, что крайне неэффективно по памяти к тому же. отсюда и берутся невероятные цифры по ускорению. абсолютные цифры по времени инференса в статье тактично опущены. вывод идея статьи на самом деле не так уж безумна, и если бы существовало железо, способное претворить предложенную идею в жизнь эффективно, то она бы могла найти широкое применение на практике. однако, у реальных ускорителей есть множество нюансов с доступами к памяти, аппаратной реализацией операций. если бы авторы честно упомянули все limitations и провели честное сравнение с бейзлайнами, то была бы вполне неплохая статья уровня типичного постера neurips iclr, с разумной идеей, но не оставившей большого следа. но авторы решили сорвать хайп. геростратова слава, тоже слава. по итоге имеем lk-99 из мира dl."
143,2023-11-23T16:16:36,"ps ноутбук от вашего покорного слуги сравнения на колабской цпушке их ffn с crammedbert торчовой реализацией. в моих замерах время инференса ultrafastbert и baselinebert на батче размера 1 практически интересном , совпадает в пределах стандартного отклонения. точные цифры будут зависеть от железа и версий библиотек, но вряд ли вы где пронаблюдаете 100 кратное ускорение. ultrafastbert 43.8 - 8.2 ms not ultrafastbert 40.5 - 13.3 ms"
144,2023-11-30T11:55:18,"adversarial diffusion distillation статья блог код заветным желанием производителей и пользователей диффузионных моделей является генерация в один шаг с сохранением высокого качества. в недавнее время появилось множество работ, где были предложены решения той иной степени успешности. ко всей движухе подключились метры из stability-ai и выкатили sdxl turbo при выборе названия явно вдохновлялись openai , генеративную модель достигающую довольно хорошего качества генерации в один или несколько шагов, обученную посредством adversarial diffusion distillation, о котором будет рассказано ниже. метод в настоящий момент, наиболее успешные подходы по одношаговой или малошаговой генерации сводятся к progressive consistency distillation или старому доброму gan. и в данной работе авторы по существу совместили дистилляцию и адверсариальное обучение. за основу берут предобученные sd v2.1 и sdxl. дистилляционный лосс представляет собой взвешенный mse лосс между предсказаниями незашумленной картинки сети-ученика и сети учителя, взвешенный с некоторым коэффициентом, зависящим от шага зашумления например alpha_t - весом исходного сигнала в зашумленном сэмпле . адверсиальный лосс - hinge loss c r1 gradient penalty для дискриминатора. утверждается что r1 регуляризация особенное полезна при обучении в высоком разрешении. дискриминатор инициализируют весами одного из современных feature extractor-ов - dino v1 v2, clip. дискриминатор обуславливается на текстовый эмбеддинг промпта и картиночной эмбеддинг незашумленной картинки."
145,2023-11-30T11:58:20,"эксперименты обучают две модели 1 add-m c 860m параметров из stable diffusion v1.5 для честного сравнения с бейзлайнами v2.1 для ablation 2 add-xl из sdxl. текстовый эмбеддинг для дискриминатора получают из clip-vit-g-14 и картиночный эмбеддинг из dinov2-vit-l. в качестве бейзлайнов выступают прогрессивная дистилляция sd, latent consistency model, instaflow, openmuse и stylegan-t реимплементация stylegan-t, достигающая даже более высоких метрик чем модель из исходной статьи и конкурентный gigagan . сравнивают стандартные генеративные метрики - fid clip score и пользовательские предпочтения по качеству изображений и соответствию запросу. как нетрудно догадаться, add разбивает конкурентные подходы в пух и прах. один шаг уже работает хорошо, а 4 шага для sdxl-turbo бьют даже базовую sdxl с 50 шагами сэмплирования. примечателен ablation. дистилляционный лосс по отдельности работает плохо, лучше всего работает взвешенная комбинация дистилляционного лосса и адверсариального лосса, но что любопытно и адверсариальный лосс по отдельности работает почти так же хорошо. выбор инициализации для дискриминатора существенно влияет на качество, причем лучше всего себя показывает не самый большой vit-small с dinov2 обучением. обуславливание генератора немного улучшает качество. add-m лучше большей add-xl по fid, но хуже по clip score. случайно инициализированный студент не способен обучиться до приемлемого качества. результат итеративного расшумления сетью-учителем картинки вместо предсказания в один шаг в качестве таргета дистилляции не накидывает. вывод результаты генераций в 1, 2, 4 шага, вероятно черрипикнутые, выглядят неплохо. подход сравнительно простой по сравнению с типичными пайплайнами дистилляции, при этом картинки остаются резкими и четкими. однако за красоту и скорость инференса все же приходится платить определенную цену - снижение разнообразия генераций. по существу имеем некоторый trade-off между gan и vanilla диффузионной моделью."
146,2023-12-05T10:56:08,"quik towards end-to-end 4-bit inference on generative large language models статья код вряд ли кто станет оспаривать утверждение, что сжатие и ускорение больших языковых моделей является одной из наиболее приоритетных задач для человечества. большинство современных работ посвящего квантованию только весов сети, так как в случае инференса с батчом размера 1 основную часть времени занимают операции с памятью, а не вычисления. однако, в некоторых сценариях пользователи могут быть заинтересованы в том, чтобы прогонять несколько последовательностей одновременно, как в случае обработки промптов. тогда ускорение математических операций начинает приобретать смысл. метод в этой статье авторы квантуют и веса, и активации в 4 бита. для квантования весов используется симметричное квантование gptq, а для активаций динамическое границы квантования определяются по время инференса по токенам. как известно, существуют отдельные размерности, где малые изменения значений весов и активаций могут существенно исказить выход. потому предлагается их хранить в исходной точности. определять их во время инференса неэффективно, но оказывается, что они находятся на тех же позициях, потому их можно определить заранее. в этом по существу заключается и основная суть метода. кроме того, для моделей семейcтва llama-2 оказывается предпочтительным квантовать down проекцию в mlp в 8 бит вместо 4-х, так как ее квантование сильно просаживает качество . ускорение вычислений достигается за счет того, что матричные операции проводятся в int4. название quik расшифровывается как quantization to int4 with gpu kernel support. при числе токенов от 1 до 16 операции матричное перемножение memory-bound, с большим количеством вычисления начинают доминировать."
147,2023-12-05T10:59:15,"эксперименты метод валидируют на моделях семейства opt, llama-2 и falcon. для оценки качества замеряют перплексию на wikitext2 и точность на zero-shot из lm-eval-harness. во всех экспериментах берут 256 оутлаеров, примерно 3 измерений для opt-66b. quik достигает заметно лучшего качества по сравнению с бейзлайнами smoothquant, omniquant, rptq при квантовании в 4 бита. просадка по качеству значительная, но приемлемая для многих приложений. квантование в 8 бит сохраняет исходное качество для всех рассмотренных моделей. предложенный метод дает примерно двукратное ускорение по сравнению с fp16 при квантовании в 8 бит, и до 3.4x при квантовании в 4 бита при 4-кратном теоретическом . пиковый расход памяти уменьшается от 2 до 3.5 раз в зависимости от размера модели. ablation study показывает, что 256 оутлаеров около оптимально. квантование down proj в 4 бита сильно ухудшает качество, при этом инференс этой группы слоев в 8 битах не слишком сказывается на общем времени работы. квантование можно совместить с 2 4 sparsity, но для сохранения качества приходится прунить только attention проекции. вывод практически полезный технический результат."
148,2023-12-10T23:28:50,"striped hyena-7b блог модели hessian nous народ уже на протяжении долгого времени занимается поиском альтернативы архитектуре трансформера. однако многочисленные попытки и заявления про рождение убийцы трансформеров выглядели недостаточно убедительными, ибо полученные модели сильно не дотягивали до sota моделей. и команда из together.ai совместно с hazyresearch выпустили stripedhyena-7b, которая оказывается достойным конкурентом современным lm уровня llama-2 и mistral-7b. метод архитектура модели составлена в основном из эффективных сверточных блоков, умеющих эффективно обрабатывать длинный контекст - flashfftconv, и некоторого количества стандартных attention блоков. авторы показывают, что смесь attention и conv блоков в соотношении 25 75 достигает наилучшего качества при заданном размере модели и бюджете обучения. одни лишь сверки без attention работают однако хуже, чем просто attention. кроме того, в свертки привносят multi-head из attention и утверждается, что это накидывает grouped convolution ? эксперименты together.ai выпустили две модели - hessian базовая модель , и nous instruction finetuned . sh-7b опережает llama-2-7b и слегка уступает mistral-7b на openllm бенчмарке, nous версия лучше файнтьюна llama-2-13b-openhermes, но слегка слабее похожего файнтьюна для мистрали. на длинных последовательностях 128k токенов предложенная архитектура в 1.5 раза быстрее, чем оптимизированный трансформер с flashattention-2 и grouped query attention. расход памяти тоже уменьшается до 2 раз на длинном контексте. кроме того. модель не проседает при увеличении контекста в 2 раза по сравнению с самым большим, увиденным во время обучения а если в 4 8 раз? . вывод неплохая попытка скинуть трансформеры с пьедестала больших языковых моделей. однако наличие некоторой доли attention блоков в конечной архитектуре все же делает пока multihead attention незаменимой компонентой в дизайне llm."
149,2023-12-11T12:30:28,"mixtral of experts блог прогресс в языковых моделях идет насколько быстро, что ни день, то новая sota-lm. mistral.ai, которые некоторое время назад выкатили lm mistral, которая несмотря на скромные размеры опередила более крупные версии llama-2, теперь выкатили уже смесь экспертов, где поменяли одну букву в названии mistral mixture . в модели 45b параметров, и на прямом проходе активируются 2 из 8 экспертов attention блоки задейсвтуются все . и в итоге для каждого сэмпла задействуются 12b параметров - чуть более одной четверти от общего количества. модель обладает следующими фичами 1 хорошо умеет в , , , , языки. 2 обрабатывает контекст до 32k 3 могет неплохо в код и математику слой распределяющий по экспертам обычная линейная проекция учится вместе с моделью. детали обучения - на каких данных, сколько токенов, неизвестны. mixtral 8x7b на бенчмарках в основном бьет llama-2-70b, gpt-3.5. хоть и в нынешнее время стоит относиться с некоторой осторожностью к подобным заявлениям, ибо неизвестно, какова вероятность, что данные из бенчмарков, или похожие на них, не попадали в pretrain. время инференса у модели, как у llama-2-13b, при этом перформанс на порядок выше на ряде разных доменов. и вроде бы меньше подвержена biasам и галлюнам. похоже, за смесями экспертов и правда будущее, во всяком случае, настолько насколько это обозримо в мире dl, где революции случаются постоянно."
150,2023-12-11T12:36:49,"название стартапа, кстати, вдохновлено маркой риса"
151,2023-12-14T00:32:39,"imagen 2 блог гугол выкатил вторую версию своей знаменитой диффузионки на публику. следую модной традиции хранить таинственную загадочность, создатели предпочли не выдавать военных тайн, ограничившись лишь высокоуровневым описанием возможностей модели. из блога можно понять или предположить следующее 1 использовались более подробные синтетические описания картинок как в dalle-3. 2 черри-пики выглядят вполне фотореалистично 3 была обучена aesthetics модель на предпочтениях пользователей, которая была затем использована для conditioning imagen 2. 4 imagen 2 умеет в генерацию а-ля dreambooth textual inversion по нескольким примерам 5 imagen 2 умеет в inpainting модификацию части картинки и outpainting продолжение картинки извне . 6 imagen 2 идет вместе с synthid, тулзой для обнаружения nsfw контента и ватермарок остальное остается за кадром - латентная ли диффузия или каскадная, информация о архитектуре и обучении."
152,2023-12-19T21:31:24,"controlling text-to-image diffusion by orthogonal finetuning статья cтраница проекта страница документации в peft введение разнообразные адаптеры и методы peft стали общепринятым способом дообучения. хороший адаптер должен быстро и эффективно обучаться выучивать специфику конкретной задачи, при этом сохраняя накопленное с таким трудом в модели знание. пресловутая lora обучает добавки низкого ранга к матрицам весов. в этой статье же предлагается обучать ортогональное преобразование исходной матрицы весов orthogonal finetuning , сохраняя похожесть разных признаков между друг другом. метод авторы мотивируют использование ортогональных преобразований следующим экспериментом обучают сверточный автоэнкодер с уменьшением разрешения и последующим повышением не нормируя веса и активации. если на инференсе модели вход и веса сверточного фильтра на их норму, то результат реконструкции не испортится, в то время, как если сохранить только информацию о величине фильтра и входной активации - то на выходе будет что-то не очень красивое. то есть угол между векторами более информативен, чем норма. конечный адаптер имеет вид w r w0, где r - обучаемое ортогональное преобразование, а w0 - исходная матрица весов. такой адаптер сохраняет сущность с устрашающим и внушающим благоговейный трепет названием - гиперсферическую энергию. однако суть на самом деле проста, и по факту мы хотим сохранить попарные углы между весами матриц. но как добиться условия на ортогональность матрицы r? авторы используют параметризацию кэли - r 1 q 1 - q -1 c кососимметричной q -q t. однако в исходной форме метод имеет слишком много обучаемых параметров - o d 2 , примерно столько же, сколько и исходная сеть. потому предлагается рассматривать блочно-диагональные адаптеры, тем самым снизив сложность до o d 2 r , r - число блоков. можно еще сильнее уменьшить число параметров, продублировав адаптер во всех блоках до o d 2 r 2 . в качестве конкретного примера, авторы сравнивают lora адаптер ранга 8 и oft с 8 блоками. в первом случае - 2048 обучаемых параметров, против 960 во втором. однако, для больших матриц выигрыш будет не пользу oft, ибо oft масштабируется квадратично по размерам исходных весов вообще говоря, ортогональное преобразование, может далеко увести от исходных весов, потому авторы предлагают накладывать условие, чтобы выученная матрица не сильно отличалась от единичной. такая модификация называется coft constrained oft . предполагая малое отклонение от единичной матрицы, выполнение данного условия можно достичь с помощью проектированного градиентного спуска. утверждается, что данная опция обладает большей стабильностью"
153,2023-12-19T21:33:27,"эксперименты метод валидируют на dreambooth и других задачах conditional генерации - на основе keypoints и сегментационных масок. дообучение всей модели методом dreambooth сколь-либо продолжительное время приводит к генерации сильных артефактов. lora выглядит несколько лучше, но все равно начинает генерировать артефакты. oft и coft даже после большого числа шагов выдают хорошие картинки в приведенных примерах. по метрикам качества - похожести dino эмбедов, соотвествия текстовому и картичному промпту oft опережает бейзлайны. в задаче контролируемой генерации лиц по ключевым точкам oft сходится быстрее чем конкуретные lora, text2image адаптер и controlnet, при этом достигая более высокого качества. аналогичный результат достигается и для генерации по маскам сегментации. вывод выглядит как вполне неплохой и разумный адаптер. основной областью применения скорее всего будут диффузионные модели, тушки из компьютерного зрения и небольшие по современным меркам языковые модели. а вообще интересный вопрос - какой самый эффективный адаптер по числу параметров в зависимости от задачи, и какие исходя из каких соображений следует выбирать тот или иной метод peft под конкретное приложение?"
154,2023-12-23T21:42:54,"switchhead accelerating transformers with mixture-of-experts attention статья код смеси экспертов для mlp блоков были неоднократно с успехом применены в больших языковых моделях в частности в недавно вышедшей mixtral, и gpt-4 по слухам тоже ей является . однако, кроме mlp в трансформере можно добиться ускорения за счет использования за счет части голов в multiheadattention. на больших последовательностях attention слои требуют много вычислений и памяти, и использование части голов даст некоторую экономию. даже если бы статья была анонимной, то по цитированиям шмидхубера при каждом упоминании attention, можно было бы угадать одного из авторов метод наивный подход был бы следующий - иметь предиктор, который предсказывает скор для каждой головы и берет только с наибольшим скором. однако, проблема в том, что при авторегрессивной генерации могут активироваться разные эксперты и в итоге придется все равно хранить кэш keys и values на все головы проблема, правда, нынче лечится multiqueryattention . авторы предлагают следующее - иметь экспертов внутри каждой головы и активировать только часть экспертов на прямом проходе. в этом по сути и вся суть метода. эксперименты подход валидируют на transformerxl, который обучают на с4 pes2o 100к шагов. не самый типичный setting для 2023. качество замеряют по перплексии на wikitext103, c4, pes2o. в бейзлайновом трансформере 2 или 10 голов с одинаковым числом параметров , в предложенном switchhead 2 головы с 5 экспертами. во всех случаях смесь экспертов имеет столько же параметров, сколько и исходная модель. в разных конфигурациях q, k, v и o проекции могут быть как разбиты на экспертов, так и нет. перплексия везде примерно - одинаковая и будто бы метод не проигрывает сильно в качестве исходному трансформеру с 10 головами и уверенно опережает двуглавый . вопрос в том, правда, насколько хорош бейзлайн. сравниваются с работой moa, при том же качестве их подход гораздо экономичнее по памяти, с экономией в 4-5 раз, в то время, как у moa выходит 1.5-3 раза. далее, можно сделать mlp тоже экспертами и получить switch-all модель. и switch-all при примерно той же перплексии расходует снова заметно меньше памяти. выводы специализация attention в трансформере - полезное направление, однако эксперименты выглядят не слишком убедительно. нет и замеров ускорения, а лишь сравнение macs между switchhead all и бейзлайном. да и в популярном нынче multiqueryattention можно было бы активировать часть query проекций и иметь профит в плане памяти и вычислений."
155,2023-12-25T12:47:37,"не проплаченной рекламы пост год близится к концу и я захотел поделиться подборкой тг каналов по машинке и глубокому обучению. с многими из них многие из вас уже, полагаю, знакомы, но некоторые среди них могут стать для кого-то открытием. список не имеет четкого порядка и элементы ниже в том порядке, в каком доставались из сознания автора 1 gonzo-обзоры ml статей классный канал с детальными и подробными разборами, во многом вдохновивший меня на создание собственного канала. кроме того там публикуются новости из мира ai, математики, философии и всякая сборная солянка. 2 abstract dl короткие разборы и анонсы разных вкусностей из мира cv, nlp. 3 concise research краткий, но исчерпывающий рисерч в области компьютерного зрения и не только. если хотите в компактной форме, но при этом в достаточном содержании понять суть статьи - вам сюда. 4 сиолошная новости из мира nlp, где иногда разьясняют всякие прикольные штуки и фишки в современном nlp. новости из мира ai и еще про космос. 5 что-то на dl-ском классный блог с аннотациями статей по nlp, cv, находками автора и обзором полезных инструментов и хаков. 6 love, death, transformers классная подборка всякой всячины из мира ai, от образовательных статей до социальных и житейских моментов. культура, мемы, веселье. 7 voronkov_ai_ru_public chatgpt, langchain, llm анонсы и новости из мира глубокого обучения и ai. преимущественно про nlp, но не только. сборник новостей, образовательных и обучающих материалов с разных источников. 8 grokaem себя милый и уютный канал, где время от времени появляются хорошие статьи, проясняющие тот или иной концепт из области nlp. саморазвитие, опросы, разьяснение и разбор распостраненных и не очень ошибок. классные истории из жизни с счастливым концом. 9 machine learning краткие анонсы и ссылки на разные новинки в области машинного обучения. преимущественно прикладного толка и с ориентацией на репозитории, где есть разные красивые демки. 10 awesome dl отличный канал с разборами статей из разных областей dl разной длины, в том числе и с видеоразборами. 11 kali novskaya преимущественно про nlp, но и не только. подборка новостей из разных областей. мемы, культура, искусство. 12 эйай ньюз просто самый лучший канал."
156,2023-12-27T15:21:40,"your student is better than expected adaptive teacher-student collaboration for text-conditional diffusion models статья код введение человечество добилось значительных успехов в ускорении диффузионных моделей путем разработки более совершенных солверов и различных стратегий дистилляции по шагам. однако, методы генерирующие в один или малое количество шагов так или иначе уступают многошаговым моделям либо по качеству генераций, либо по их разнообразию. отсюда возникает мысль а что если не отправлять сразу учителя на пенсию, а призывать по мере необходимости, когда ученик не способен достичь желаемого качества. за счёт этого можно убить двух зайцев иметь в среднем более быстрый инференс и хорошее качество. и на данном соображении построена рассматриваемая статья. метод в качестве учителя берут предобученную stable diffusion sd v1.5 или sdxl модель. ученика инициализируют весами учителя и проводят процедуру consistency distillation. полученная модель может за малое число шагов в большинстве экспериментов используют 5 выдавать генерации неплохого качества. тем не менее, дистиллированная модель все же уступает учителю по оценке разметчиков 50 за учителя против 30 за студента . дальнейший анализ весьма примечателен и интересен. чем меньше модель ученик подражает учителю, тем больше доля голосов ассесоров, сделавших выбор в пользу модели-студента. примечание гистограммы отнормированы на суммарное число побед конкретной модели, а не число побед для данного расстояния. то есть 60 побед на правой гистограмме означает, что среди побед студента 60 из них достигаются, когда расстояние между студентом и учителем велико характерно, что модель ученик отходит от учителя сильнее на более сложных картинках для оценки сложности картинки используется icnet модель и на длинных текстовых промптах. по всей видимости, и то и другое реже встречается в данных, потому у ученика появляется больший простор для фантазии. возвращаясь к исходной задаче как же все таки понять, использовать ли ученика или учителя? и для этого используют imagereward модель, которая оценивает качество генерации. если качество генерации выше некоторого порога, то используется генерация студента, иначе прибегаем к помощи учителя. оптимальный порог оценивается по валидационной выборке как некоторый квантиль imagereward для учителя. корреляция с человеческими предпочтениями не идеальная около 60 , но лучшего результата текущие методы оценки качества изображений пока не могут достичь. предлагается два варианта использования учителя 1 regeneration генерация с нуля 2 refinement неполное зашумление результата ученика с последующим расшумлением"
157,2023-12-27T15:25:24,"результаты в первой серии экспериментов используют sdv1.5 в качестве ученика учителя. consistency дистилляция проводится на 80m подвыборке laion-2b. сравнивают модели на промтах из laion-aesthetics и coco2014-сaptions. тандем ученик учитель с 10 шагами генерации в среднем примерно равен dpm и ddim с 25 шагами и 50 шагами генерации учителя, соответственно. при 15 шагах даже чуть лучше. при сравнении использовался refinement подход. адаптивный подход лучше, чем просто refinement. что касается автоматических метрик, refinement и regeneration имеют примерно одинаковый image reward, regeneration лучший clip alignment с текстом , а refinement лучший fid мера соответствия распределению реальных картинок . тандем ученик учитель поверх sdxl сравнивается с sdxl c 50 шагами солвера и опережает sdxl-turbo. эффективность метода ограничена в первую очередь несовершенством процедуры оценки качества генерации студента, авторы показывают, что при наличии идеального асессора генераций, можно было бы добиться значительного разрыва с учительским бейзлайном. кроме того метод неплохо сохраняет разнообразие генераций учителя и хорошо себя показывает в text-guided image editing и контролируемой генерации. выводы красивая идея и результаты, подкрепленные занимательным анализом. использование нескольких моделей для решения конкретной задачи выглядит действительно сильным подходом, уже неоднократно возникавшим в литературе . основная сложность именно в том, как оптимально выбрать ту или иную модель и сколько моделей и какие мы можем позволить держать в памяти и применять."
158,2023-12-30T23:24:13,"denoising diffusion bridge models статья кода нет, и не обещают введение диффузионые модели показывают на данный момент наилучшее качество во множестве задач и приложений. однако сама постановка диффузионного процесса, где пример из целевого распределения превращается в шум из нормального распределения, а модель учится обращать данный процесс, предполагает на одном конце распределение реальных данных, а на другом - чистый шум, что не очень подходит для задач image-to-image, таких как перенос стиля, контролируемая генерация и super-resolution. существуют подходы, которые с тем или иным качеством справляются с вышеупомянутыми задачами. sdedit зашумляет входной пример и расшумляет обратно, обуславливаясь на целевую задачу, и типичные архитектуры диффузионных sr моделей конкатенируют на входе шум и картинку низкого разрешения. но эти архитектурные ухищрения требуют требуют тонкой настройки и выглядят несколько неестественно? возможно ли дать такую постановку задачи, которая бы напрямую решала задачу отображения из одного распределения в другое? и в данной статье предлагают ответ на данный вопрос. метод сам подход основан на стандартной теории диффузионных процессов. однако берется специфичная форма процесса, такая, что удовлетворяет заданным граничным условиям на обоих концах то что и хочется для задач перехода из одного распределения в другое . в литературе такой процесс известен как doob h-преобразование. при заданной начальной точке он называется диффузионным мостом. далее авторы предлагают формулировку такого процесса для variance preserving с постоянной дисперсией и variance exploding с растущей дисперсией расписания шума. предложенный подход обобщает генерацию из шума в целевое распределение и ot-flow-matching rectified flow. результаты метод проверяют на задачах image2image - edges2handbags и diode-256 256 как я понял, это не про электронику, а генерацию сцены по маске сегментации и unconditional генерации. предложенный подход заметно опережает gan-based pix2pix и подходы, основанные на диффузионных моделях и работу i 2sb, использующую мосты шредингера, по метрикам fid и lpips. unconditional генерация из шума тоже работает весьма достойно на cifar и ffhq, выдавая качество сравнимое с edm при том же количестве шагов генерации. значение guidance в vp постановке значительно влияет на качество, и почти не влияет для ve диффузии. кроме того метод, имеет дополнительный гиперпараметр, определяющий промежуточный шаг в солвере, который тоже следует подбирать аккуратно для наилучшего качетства. вывод интересный и разумный подход, адаптирующий имеющуюся диффузионые техники на задачи перехода из одного распределения в другое. было бы интересно провалидировать подход на большем масштабе и более сложных задачах с широким распределением данных и высоким разрешением изображений ."
159,2023-12-31T15:06:56,"вот и подходит к концу 2023 год, выдавшийся очень насыщенным и непростым в различных сферах, но богатый на научные открытия и яркие результаты. крутые llmки, запитанные разнообразными instruction finetuning техниками и датасетами, улучшенные генеративные модели, прогресс в компьютерном зрении, аудио, видео и мульмодальных моделях и многое другое - вот что дал нам этот год. а какие у вас ожидания от следующего года? прорыва в какой области и какого результата вы ждете большего всего?"
161,2024-01-02T23:49:25,"начнем год с маленькой викторинки. создадим пустые тензоры из одного элемента следующим образом x torch.empty size 1, 2 и заполним их следующим образом 1 for i in range 2 x i torch.full size 1, , fill_value i 2 for i in range 2 x i .data torch.full size 1, , fill_value i"
166,2024-01-03T12:41:00,"ответ на задачку и обьяснение за спойлером 1 tensor 0 , tensor 1 , 2 tensor 1 , tensor 1 почему так? выражение x torch.empty size 1, 2 создает один обьект в памяти, на который продублированы ссылаются x 0 , x 1 . в случае 1 обращение происходит по имени, и x 0 теперь ссылается на обьект tensor 0 и x 1 на обьект tensor 1 . в случае 2 обращение происходит по ссылке, и сначала обьект, на который ссылаются как x 0 и x 1 записывается значение tensor 0 , а затем tensor 1 . как ваш покорный слуга сам напоролся на сий вроде бы очевидный, но заковыристый нюанс? когда работал с torch.distributed.gather заполнял тензоры на разных процессах которые вообще говоря должны были генерировать разные значения , чтобы собрать затем на главном потоке. какого же было мое удивление, когда увидел копии одного и того же тензора в полученном списке"
167,2024-01-07T21:19:36,"the llm surgeon статья кода нет на текущий моментнаиболее успешные методы по прунингу и квантизации моделей так или иначе опираются на некоторое приближение матрицы вторых производных для определения важных весов и отпимальной сжатой конфигурации. чем больше модель, тем дороже становится вычисление любого приближения, и многие подходы не масштабируются на современные llm. sparsegpt gptq и их производные оптимизируют квадратичную ошибку на выходе линейного слоя свертки, но ничего не знают о целевой функции потерь. чтобы учесть целевую функцию, надо так или иначе вычислить градиент или его прокси и агрегировать. и на помощь приходит приближение матрицы фишера для данного слоя линейного или свертки кронекеровским разложением f a otimes g. первый фактор матрица ковариации входных активаций идентичен гессиану квадратичной ошибки для данного слоя. второй фактор - матрица ковариации градиентов по выходу слоя. кронекеровское разложение точно в предположении независимости градиентов по выходу слоя от активаций, которое вообще говоря не выполняется, но попытка - не пытка. и в данной работе, получилось успешно применить кронекеровское разложение в фреймворке optimal brain surgeon. метод суть метода в следующем - берут optimal brain surgeon, где вместо настоящего гессиана используется кронекеровское разложение и выводят формулы для неструктурированного 2 4 и структурированного прунинга. для более высоких уровней прореживания метод применяют итеративно - между итерациями прунинга дообучают lora добавки и пересчитывают матрицу фишера до 40 раз при прореживании до 50 . недешевое удовольствие, но качество подымает почти гарантированно."
168,2024-01-07T21:21:06,"результаты метод валидируют на моделях семейства opt и llama-2-7b 13b в приложении . для замера качества традиционно берут перплексию на wikitext2 lm_eval_harness 0-shot в приложении . llm surgeon уверенно опережает sparsegpt и диагональное кронекеровское приближение как структурированном, так и неструктурированном пруниге. конкурентные подходы ломают модель уже при слабом прореживании в то время как llmsurgeon с серьезной просадкой, но тем не менее сохраняет какое-то качество вплоть до 50 . ожидаемо пересчет гессиана и дообучение низкоранговых добавок дает заметный прирост в качестве. так как метод оценивает важность параметров на уровне конечного слоя прореживать слои можно неоднородно. однако, полученное распределение слоев после фильтрации по глобальному порогу сравнительно однородное для llm surgeon - первый и последний слой прореживаются несколько сильнее, чем слои посередине немного неожиданно . разные виды проекций q, k, v, o proj и в fc1 2 тоже прореживаются достаточно однородно в отличие от global magnitude и диагональных приближений . вывод весьма достойный результат, хоть и недешевой ценой. выглядит на текущий момент, как sota метод для языковых моделей размером порядка нескольких миллиардов параметров, где блочно-диагонально фишеровское приближение становится непомерно дорогим, а предложенный метод требующий дополнительной памяти порядка 2 размеров модели является еще подьемным пусть даже с шардингом, но в пределах одной вычислительной ноды . метод заметно дороже, чем sparsegpt, который отрабатывает за 5 минут на llama-2 на одной gpu, против 2 с лишним дней на 4x h100 для llm surgeon. учитывая запрос сообщества на ускорение llm, цена вполне приемлемая. было бы полезно скомбинировать метод с квантизацией."
169,2024-01-09T23:24:59,"вот думаю, разобрать ли у себя в блоге mamba и powerinfer? статьи уже возникали в блогах уважаемых тг каналов, но как мне кажется ряд интересных мыслей и нетривиальных наблюдений решений все же были не затронуты в обзорах, о которых было бы интересно поведать."
171,2024-01-10T13:28:19,"mamba linear-time sequence modeling with selective state spaces статья код введение поиск архитектуры, способной сбросить трансформеры с пьедестала, по своей важности можно сравнить с поиском философского камня или высокотемпературного сверхпроводника. время от времени появляются работы s4, rwkv, retnet , в которых предъявляют архитектуру конкурентную или превосходящую по эффективности масштабируемости трансформер, но по какой-то причине научное сообщество и пользователи до сих пор сидят на трансформерах, и все самые высокопроизводительные foundation модели так или иначе зиждятся на self-attention. одна из наиболее интересных попыток предъявить альтернативу - была серия работ по state-space моделям s4, h3, hyena . однако показав хорошее качество на ряде задач, связанных с обработкой последовательностей, аудио, они не стали сильны в общей задаче языкового моделирования. в этой работе проанализировали недостатки и ограничения существующих state-space моделей и предложили модификацию, устраняющую или минимизирующую эти недостатки."
172,2024-01-10T13:34:00,"метод state-space модель по существу - это рекуррентная модель вида в дискретной форме h_ t a h_ t-1 b x_ t y_ t c h_ t где x - вход, h - скрытое состояние, y - выход. a, b, c - обучаемые матрицы параметров, параметризованные специальным образом. есть еще шаг дискретизации и преобразование из непрерывной формулировки в дискретную с некоторым шагом дискретизации delta. в отличие от типичной рекуррентной архитектуры а-ля vanilla rnn, lstm между h и y нет нелинейности. благодаря этому формулу можно развернуть и представить в виде свертки, которая эффективно параллезируется на gpu. важно, что проекции a, b, c не зависят от времени, и динамика состояний сети обладает свойством linear time invariance lti . данное свойство накладывает ограничения на возможность моделирование определённых зависимостей и авторы рассматривают пару синтетических задач, которые принципиально не решаются state-space моделями с lti 1 selective copying, где нужно запомнить и вывести несколько элементов последовательности с произвольным расстоянием между запоминаемыми элементами. 2 induction heads, где приняв начало словосочетания на вход, на основе полученной ранее информации - продолжить. если на входе harry, выдать следует potter прочитай другую книгу . данные задачи требуют от модели способности работать с неоднородными временными последовательностями и адаптивно учитывать контекст, которой нет в state-space моделях со статичными сверточными ядрами. авторы вводят два понятия с похожим звучанием, но разным значением efficiency компактное внутреннее состояние и дешевизна вычислений effectiveness способность хранить релевантную информацию трансформер effective - так как может хранить в себе во всяком случае в теории контекст произвольной длины. но не efficient, ибо состояние key и valuе кэш растет пропорционально длине последовательности, что делает инференс трансформера дорогим по памяти и вычислениям. причем большинство хранимой информации избыточно. рекуррентные сети efficient, ибо хранят состояние постоянного размера, не зависящего от длины последовательности. но в это маленькое состояние не всегда удается на практике впихнуть всю релевантную информацию. идеальная модель, должна потому адаптивно отбирать нужный для задачи контекст. чтобы добиться данного свойства, авторы предлагают делать параметры state-space модели зависящими от входа - проекции a, b, c, как и шаг delta. последнее позволяет заодно учитывать и временную неоднородность. развернуть рекуррентную зависимость как для lti state-space модели теперь не получится. вместо этого авторы реализуют эффективный алгоритм parallel scan на кастомных чудо-kernel дающий высокую производительность. чтобы иметь возможность запихнуть много информации в скрытое состояние, используют большой expansion фактор внутри ssm 10-100 . современные gpu зачастую memory-bound, потому чтобы не тратить драгоценные миллисекунды на перекачку данных из hbm в кэш gpu, это расширенное состояние материализуется только на gpu. и в духе flash attention один из авторов - автор flash attention чтобы дополнительно сэкономить на памяти и ее трансфере, промежуточные состояния отправляются в dev null после прямого прохода и пересчитываются снова на обратном проходе. далее авторы, предлагают полностью однородную архитектуру, состоящую только из mamba блоков, в то время как трансформер - перемежающиеся attention блоки и mlp, и прошлые state-space модели состояли собственно из state-space слоев и gated mlp. так как алгоритм впридачу к свойствам s4 новый тип state-space слоев обладает селективностью selective и использует сканирование scan для краткости авторы обозначают новый тип архитектуры - s6."
173,2024-01-10T13:36:55,"результаты авторы валидируют новую архитектуру на следующих задачах 1 selective copy и induction heads 2 language model pretraining 3 моделирование днк 4 моделирование и генерация аудио на selective copying s4, h3 и hyena выдают качество порядка 20-60 в то время как s6 достигает почти 100 качества. на induction heads как трансформеры с разными позиционными энкодингами, так и прошлые state-space модели не обобщаются далеко за пределы контекста увиденного на обучении. а s6 mamba выдает стабильно хорошее качество на последовательностях любой длины. mamba s6 достойно себя показывает на языковом моделировании при обучении на pile, демонстрируя масштабируемость на уровне и даже чуть лучше, чем трансформер архитектуры llama-2, со всеми последними достижениями и наработками в развитии архитектур данного семейства. h3, retnet, hyena, rwkv заметно отстают. обучают модели следуя предписаниям chincilla размером от 130m до 2.8b. mamba заметно опережает по качеству открытые модели сопоставимого размера, а иногда и вдвое большие. на моделировании днк mamba превосходит уже заметно трансформер и state-space модели прошлых поколений. модели, кстати, совсем небольшие, 1.4m и 7m параметров. на генерации аудио mamba сильно опережает по метрикам прошлую sota diffwave sashimi и иные подходы. s4 mlp, стоит отметить, работает тоже весьма достойно. на больших последовательностях scan из mamba работает куда быстрее, чем даже оптимизированный flashattention-2. авторская реализация scan в десятки раз быстрее той что в торче. благодаря эффективности по памяти в одну gpu можно пихать гораздо больший батч, чем в трансформер. например, 6.9b мамбе можно скормить 128 последовательностей длины 2048 и не поперхнуться. в ablation показывают, что полезно иметь все параметры state-space модели a, b, c, delta обучаемыми. но больше всего накидывает delta. комплексная параметризация a не накидывает по сравнению с нормальным распределением. expansion внутреннего состояния улучшает качество - чем больше фактор, тем больше информации можно запихнуть в скрытое состояние. по ходу работы пользуются n 64. вывод выглядит весьма впечатляюще, крутая работа с хорошим анализом и мотивацией, серьезными инженерными достижениями и убедительными результатами. однако, все же интересно, сможет ли mamba родить убийцу трансформеров , или останется одной из многих, пусть и сильных попыток ограниченной успешности. самая большая модель, которую использовали в бенчмарках по качеству имеет размер менее 3b параметров, что немного по современным меркам. как признаются сами авторы, масштабирование архитектуры дальше может породить новые технические сложности и вызовы. поживем-увидим, как говорится."
174,2024-01-13T19:54:58,"powerinfer fast large language model serving with a consumer-grade gpu статья код введение за последние пару лет llmки забавно прибавили в весе и многие модели не влезают в типичную пользовательскую gpu. а инферить умного ассистента локально ой как хочется. как выход, можно держать веса в озу или на диске и подгружать по мере необходимости - то есть заниматься offloading, есть gpu-centic offloading, когда все вычисления происходят на gpu, и hybrid offloading используемый в llama.cpp , где часть вычислений на gpu, а часть на cpu. основная проблема в том, что передача данных с gpu на cpu занимает уйму времени, и большая часть времени уходит на операции с памятью, пока ядра простаивают, и работает все дико медленно. отсюда следует вывод, что ключом к ускорению является минимизация передачи данных между cpu и gpu. метод в deja vu ранее было замечено, что в слоях трансформера активируется малая часть активаций для конкретного входа. здесь же развили идею и заметили, что нейроны активируются неравномерно, и некоторые активируются часто - так называемые горячие , а другие же только время от времени холодные . в частности, для opt-30b 26 для llama-2-70b 43 наиболее часто активируемых нейронов отвечают за 80 суммарной величины активаций. статистики активаций считаются в offline режиме на подвыборке из c4 и wikipedia. отсюда возникает идея - держать горячие коих не очень много нейроны на gpu постоянно, а холодные - предсказывать с помощью специальных слоев, и подгружать по мере необходимости. оказывается даже, что практичнее не выгружать холодные нейроны на gpu для малых батчей , а считать на cpu. следующий нюанс в том, что делать со слоями, предсказывающими активные нейроны среди холодных . держать их памяти - накладно для больших моделей, а наивное использование предикторов с меньшим числом параметров просаживает качество. и оказывается, что в слоях, где больше sparsity, можно использовать более слабый предиктор, и потому размер предиктора подбирается для каждого слоя отдельно. на gpu мы работаем с матрицами фиксированного размера, и потому пользуемся эффективными ядрами для gemm, а для cpu, обладающим куда меньше параллелизмом, отдельные вектор-векторные произведения считаются и так эффективно."
175,2024-01-13T19:56:31,"результаты powerinfer тестируют на моделях семества opt llama 2 и falcon. код основан на llama.cpp. рассматривают 2 конфигурации, pc-high - с intel i9-13900k и rtx 4090 pc-low и i7-12700k c rtx 2080ti. нормальные такие геймерские компы. powerinfer достигает впечатляющего ускорения в 11 раз на длинных последовательностях и моделях opt-30b, falcon-40b. на llama 2 ускорение не столько велико до 3х раз , но тоже весьма достойно. в powerinfer большинство вычислений происходит на gpu, в то время, как в llama.cpp основную вычислительную нагрузку берет себя не столь быстрый cpu. все компоненты метода полезны, но наиболее важной, по всей видимости, является то, что критичные для вычислений нейроны сидят все время на gpu. метод дает ускорение и при квантизации в 4 бита. оверхед от предикторов холодных нейронов мизерный по сравнению с общим методом инференса. качество моделей не просаживается статзначимо при используемой схеме fixed contextual sparsity а если проверить на mmlu? . метод настолько хорош, что даже на относительно слабой rtx 4090 на коротких последовательностях метод уступает всего лишь на 20-30 в скорости vllm оптимизрованному gpu движку на a100. вывод полезный и сильный результат, основанный на наблюдениях из прошлых работ. по существу сочетание fixed и contextual sparsity с эффективной реализацией, учитывающей специфику вычислений на cpu и gpu."
177,2024-01-15T00:03:49,"fast inference of mixture-of-experts language models with offloading статья код введение смеси экспертов moe , использующие лишь часть параметров на инференсе, дают значительную экономию вычислений. однако sota-модели оказываются слишком тяжеловесными для многих gpu и встает вопрос эффективного оффлоадинга. наивная подгрузка экспертов работает довольно медленно из-за передачи большого обьема данных с озу ram на gpu. возникает вопрос - можно ли оптимизировать данную процедуру? и ответ, как можно было заранее догадаться, да . метод чтобы понять, как оптимизировать загрузку экспертов, авторы смотрят на то, как активируются эксперты. и оказывается, что в конкретном слое один и тот же эксперт может активироваться 2-4 раза подряд, потому если их оставлять в памяти, они будут доступны при прогонке следующего токена. отсюда возникает идея поддерживать lru кэш токенов, и сгружать экспертов на диск по необходимости. следующий нюанс в том, как загружать следующие блоки. для обычных трансформеров никакой проблемы нет, но для экспертов заранее неизвестно, какие активируются. но можно воспользоваться стратегией из de javu, где предсказывали активные нейроны по активациям предыдущего блока, и отбирать экспертов, подавая в gating функцию на следующем блоке текущие активации speculative loading . это работает в силу residual структуры трансформера. чтобы уменьшить размер модели, используется квантизация hqq. не самый очевидный выбор, ибо это data-free квантизация, уступающая более продвинутым подходам. но раз работает, то и ладно. qmoe, говорят, не завелось может надо было квантовать в больше, чем 1 бит? . attention квантуют в 4 бита, экспертов в 2 и 3 бита. эксперименты рассматриваются две конфигурации системы - условный colab с бесплатной gpu и хороший геймерский пк. для gpu c 12gb vram можно держать кэш из 2 экспертов, а для 16gb - влезают 4. чем больше размер кэша - тем больше hit-ratio вероятность того, что эксперт лежит в кэше и точность удачного speculative loading. выбранная схема квантования более менее оптимальна, так как квантизация attention в меньшее число бит уже сильно просаживает качество, а 4 бита некритично хуже fp16. конечные конфигурации в 4-5 раз меньше по размеру базовой модели. некоторая просадка по качеству на бенчмарках есть, но приемлемая. все компоненты метода - lru-cache, пре-подгрузка экспертов заметно ускоряют инференс. итоговое ускорение порядка 2-2.5 раз на rtx3060 rtx3080 и до 3.5 раз на t4 по сравнению с наивной реализацией в accelerate. для прикола замеряют еще и на a100, куда влезает квантованная модель и так для сравнения . вывод простой и разумный подход с практически полезным результатом. по всей видимости, умный оффлоадинг - одно из самых, если не самое перспективное направление, ускорения моделях на пользовательских устройствах. однако, букву о гармонично вставить в название канала не получается. кажется, что результаты работы можно улучшить за счет более сильного алгоритма квантизации экспертов, уменьшив просадку качества."
178,2024-01-16T12:08:13,"rosa accurate parameter-efficient fine-tuning via robust adaptation статья код введение низкоранговые адаптеры lora уже давно используются при дообучении больших языковых моделей на downstream задачи. эффективность данного метода основана на наблюдении, что при дообучении на малые задачи, разница между исходными и дообученными весами обладает низкоранговой структурой - то есть основная масса концентрируется в нескольких первых сингулярных векторах. однако, данное утверждение - приближенное, и поэтому lora, по всей природе неспособная учить высокоранговые добавки, не всегда сравнивается по качеству с дообучением всей модели. другая опция малопараметрической добавки к матрице весов - разреженные матрицы. а совместив низкоранговую с разреженной добавкой получаем своего рода robust pca - метод принциапиальных компоненты с шумом, который используется много где рекомендую видео со стивом брантоном . основная техническая сложность использования разреженных матриц - эффективная реализация на gpu. благо при сильном прореживании неплохо работают ядра из специальных библиотек, в частности, sputnik. метод как можно догадаться, суть работы в том что совместили низкоранговый и low-rank adapter и дообучили на downstream задачах. разреженную маску получают на основе самых больших элементов диагонали матрицы фишера. авторы отмечают, что отдельно диагональ для sparse адаптера работает не очень, но хорошо в связке с низкоранговым адаптером. авторы анализируют точность приближения суммой низкорангового и sparse приближения на одном слое, и оказывается, что минимум ошибки достигается при сопоставимом вкладе от low-rank и sparse компоненты. чем больше доля параметров от исходной - тем эффективнее перекладывать в sparse который уже не такой sparse . эксперименты метод валидируют на llama-2-7b и датасетах viggo, gsm-8k математика без подвоха , sql. при фиксированном бюджете некоторая смесь, обычно с примерно равной долей sparse и low-rank компоненты, дает лучшее качество. по отдельности low-rank и sparse не так хороши. больший адаптер работает обычно лучше и самой большой с суммарным числом обучаемых параметров 160m почти сравнивается по качеству с полным файнтьюном. все эксперименты влезают на одну rtx3090 24gb памяти . выбор разреженной маски на основе усредненного градиента и диагонали фишера дает примерно одно и то же качество. в статье не говорится, но знаю лично от авторов, что случайная маска и по абсолютной величине весов работает хуже вывод поиск эффективных и выразительных адаптеров - архиважная задача, ибо пользователи без high-end gpu только так и могут обучать современные llmки на целевую подзадачу. lora и ортогональные адаптеры oft показывают себя неплохо в конкретных приложениях, но ограничены в выразительности по природе своей и оптимальный адаптер, по всей видимости, представляет собой некоторую смесь низкопараметрических матриц. ее правда, надо еще сначала найти"
179,2024-01-18T23:23:18,"marlin статьи нет, обычно наоборот код marlin, mixed auto-regressive linear kernel - новое быстрое ядро fp16xint4 для инференса llm с большими батчами. как известно, вычисления на современных gpu для современных gpu обычно memory bound, и можно получить заметное ускорение даже с fp умножением за счет одной лишь подгрузки весов в кэши. и идеальное ускорение, которое дает int4 - 4x. однако с увеличением размера батча, мы начинаем выходить на compute-bound и выигрыш от имеющихся ядер не использующих tensor cores исчезает, то есть они становятся медленее fp16 native матричных операций. в этом репозитории выложена реализация нового ядра, оптимизирующего работу с кэшами и позволяющего иметь значительное ускорение по сравнению с fp16 даже на больших батчах. основные фичи следующие 1 префетчинг 2 использование активаций в вычислениях несколько раз до загрузки сгрузки 3 асинхронная загрузка весов модели 4 тонкая настройка порядка операций квантования деквантования и матричных умножений с tensor cores для максимальной утилизации железа 5 оптимизация конфигурации варпов групп потоков на gpu в итоге удается достичь почти идеального speedup - 4x по сравнению с fp16 на батчах до 32, и иметь ускорение даже на больших 64-128 , в то время как безйлайны из bitsandbytes, exllamav2, awq становятся в 2x медленее fp16 с батчей 16. ускорение имеет место на нескольких gpu поколения ampere - a10, a6000, rtx 3090 и сохраняется даже на пониженных частотах. на поколении hopper пока нет реализации."
180,2024-01-18T23:30:10,"видала я статьи без кода, но код без статьи! такого я в жизни ещё не встречала."
181,2024-01-23T22:39:21,"tinygsm achieving 80 on gsm8k with small language models статья кода нет, как и моделей и датасета введение обучить здоровенную модель на здоровенном датасете здоровенное число итераций, которая умеет во все и всея - большого ума не надо, поставил обучаться на десятках тысяч видеокарт на несколько месяцев - и готово. а вот получить небольшую модельку, способную в сложные логические конструкции и заключения, решение задач по математике - вот это уже настоящее мастерство и искусство. в разбираемой статье авторы обучили семейство сетей небольшого размера от 125m до 1.3b , которые превосходят по доле правильных решений куда более крупных конкурентов. метод в великом множестве статей ранее было показано, что синтетические данные, сгененированные могучей сетью а-ля gpt-3.5 4, позволяют добиться значительно более высокой эффективности обучения по сравнению с типичным корпусами, собранными из интернета. в частности, можно дотюнить alpaca, vicuna, wizard, platypus и многое и многое другое , или обучить с нуля серия моделей microsoft-phi и tinystories . математические датасеты невелики по размеру. gsm-8k, в частности, имеет всего 7k примеров в обучающей выборке, и наивное обучение приведет к переобучению. потому авторы аугментируют данные с помощью gpt-3.5, перефразируя вопросы и добавляя нерелевантный контекст. чтобы обеспечить качество данных, убирают слишком короткие и не содержащие числа задачи. кроме того, выфильтровали задачи, которые оказались похожи на тестовые по n-грамному сравнению. итого вышло более 12m синтетических задач. 1.8b токенов однако одно лишь это не позволяет преодолеть порог в 70 top-1 accuracy на тесте gsm-8k. следующим краеугольным камнем работы является использование модели-verifier проверятеля , которая оценивает корректность каждого шага решения. мотивация заключается в том, что одно решение может быть неправильным, но если сгенерировать несколько, то хотя бы одно да залетит. и модель-verifier учится определять корректные шаги в решении. обучают ее следующим образом - берут 3 чекпоинта модели, решающей gsm-8k, с разных итераций обучения более ранние больше ошибаются , и учат предсказывать корректность конкретного токена в решении. если модель-решатель решила задачу верно, то все токены в последовательности размечаются, как верные - label 1, иначе - наоборот, вся последовательность, как неправильная. сэмплируют 48 решений на каждый из 7к примеров в обучающей выборке. таким образом рецепт успеха состоит из двух основных идей 1 аугментация датасета 2 обучение модели - проверятеля, оценивающей правильность конкретных шагов совместно с моделью, решающей задачи."
182,2024-01-23T22:40:53,"эксперименты модель-решатель и модель-проверяльщика инициализируют из преобученных phi-1.5. одно лишь увеличение размера модели-решателя дает небольшой прирост качества, потому добавления проверяльщика - критично для достижения хорошего результата. любопытно, что увеличение проверяльщика дает больший прирост качества, чем модели решателя. сэмплируются 48 решений, из которых подается лучшее в качестве ответа. самая лучшая модель достигает качества в 81.5 , что больше, чем у gpt-3.5 77.4 , породившей в определенном смысле данную модель, и открытых моделей, дообученных на решение математических задач поверх llama 2 7b и 13b. gpt-4 для справки добивается 97 точности. метод проверяют и на другом известном бенчмарке asdiv и там тоже получают сильный результат. ожидаемо, случайная инициализация работает хуже, чем обучение с чекпоинта phi-1.5, хоть я бы ожидал большей разницы. вывод прикольное и эффектное решение. тем не менее уместно заметить, что стоимость инференса при использовании модели-проверятеля возрастает кратно числу сгенерированных моделью-решателем ответов и выигрыш от использования меньшей модели надо умножать на число прогонов, после чего сравнить с качеством при одноразовой прогонке через большую модель."
183,2024-01-23T23:04:53,"и из забавного на ночь от коллег . вывод одной модели на задаче из gsm-8k question gerald wants to buy a meat pie that costs 2 pfennigs. gerald has 54 farthings, and there are 6 farthings to a pfennig. how many pfennigs will gerald have left after buying the pie? answer nobody cares n 54"
184,2024-01-26T21:09:18,"parameter-efficient sparsity crafting from dense to mixture-of-experts for instruction tuning on general tasks спасибо milana_shhanukova статья код введение как известно, дообучение на грамотным образом собранных инструкциях позволяет заметно повысить полезность llm-ок. для создания широкопрофильного ассистента требуется обучение на широком и разнообразном наборе инструкций. к сожалению, не все модели обладают достаточной емкостью, чтобы впитывать все подаваемое знание через трубочку и на практике происходит catastrophic forgetting при переходе от одного типа инструкций к другому. естественным решением проблемы являются смеси экспертов moe . каждый эксперт условно соответствует некоторому домену. в работе sparse upcycling наделали реплик ffn слоев, и дообучили, но такой подход, довольно дорогостоящий по памяти для больших моделей. метод суть метода проста как пробка - раз полноценные ffn слои дорогие - даешь смесь bottleneck экспертов а-ля lora c активацией посередине . и чтобы необучаемые параметры было еще проще уместить на gpu - заквантовать необучаемые параметры в 4 бита как в qlora. эксперименты обучают все хозяйство на смеси slimorca, magicoder, metamathqa 520к инструкций в сумме . обучение длится одну эпоху. в качестве основных моделей берут - llama-2-7b, llama-2-13b, yi-34b потому что llama-2-33b не выпустили . если я правильно понял, модель дообученная на наборе инструкций c qlora - называется camel, а эксперты со смесью lora адаптеров - camelidae . как и в mixtral - создают 8 экспертов, из которых активируются только 2. модель валидируют на разнообразных бенмарчках - mmlu знания в разных предметных областях , hellaswag commonsense reasoning , humaneval код , gsm8k элементарная математика и др. camelidae ожидаемо опережает базовую модель по метрикам, и самая большая модель camelidae-8 34b выглядит якобы даже сильнее большего mixtral-8 7b-instruct с плотными экспертами и llama-2-70-b-chat. однако, если присмотреться, кажется что разница между camel и camelidae не очень-то велика, и вероятно основной прирост от удачного выбора смеси инструкций. вероятно, в camelidae больше обучаемых параметров. вывод смеси экспертов в исходной формулировке или как адаптеры выглядят логичным решением для модели, ориентированной на решение широкого круга задач. однако в данном случае не очевидно, насколько эффективен новый адаптер, а не данные и постановка эксперимента благоприятствуют хорошим результатам."
185,2024-02-02T23:22:40,"литература многие из вас читают статьи про диффузионные модели. в них регулярно возникает таинственная и загадочная, внушающая благоговейный трепет, сущность под названием sde stochastic differential equations . и эти самые sde описывают диффузионные процессы в непрерывной формулировке, потому разные продвинутые солверы и методы сэплирования в том или ином виде зиждятся на теории стохастических дифференциальных уравнений. однако, классическая литература по данной теме довольно сложна, и оперирует множеством сложных понятий из теории меры, разными борелевскими сигма-алгебрами, фильтрациями, и тяжела для освоения читателю без сильного математического бэкграунда. двое авторов из aalto university откуда вышло немало ярких работ по генеративным моделям выпустили книжку по sde, доступную широкому кругу читателей достаточно иметь за плечами стандартный вузовский курс по диффурам и теорверу , с упором на приложения. написано очень доступно и понятно. и упражнения в конце глав вполне себе посильные, а не как обычно бывает... рекомендую всем тем, кто получить некоторую математическую базу для понимания диффузии."
187,2024-02-03T19:59:52,"mobilediffusion subsecond text-to-image generation on mobile devices статья кода нет было еще тут. в последнее время был достигнут значительный прогресс в ускорении диффузионных моделей. тем не менее инференс на мобильных устройствах работает все еще недостаточно быстро. скорость генерации диффузионной модели определяется количеством шагов сэмплирования и стоимостью одного шага. на текущий момент, большинство внимания уделяется ускорению моделей за счет первого фактора - изобретением новых солверов, алгоритмов пошаговой дистилляции. оптимизации архитектуры уделяется же значительно меньше внимания. типичные модели хороши для gpu, но тяжеловаты для мобилок, потому имеет смысл уменьшать саму модель, сохраняя качество. в этой работе основное внимание уделено как раз архитектуре, но и про уменьшение количества шагов сэмплирования тоже не забыли. метод за основу берут stablediffusion v1.5. трансформерные блоки первая оптимизация, мотивированная прошлыми работами uvit и snapfusion, - уборка дорогих и не очень полезных attention с самого высокого разрешения. кроме того, на разрешении 32x32 оставляют только cross-attention с текстовыми токенами, потому что self-attention с 1024 все еще дорогой. таким образом, self-attention есть только на разрешении 16x16. кроме того, объединяют проекции key и value, чтобы еще чуть-чуть сэкономить на параметрах, заменяют gelu на swish, который более эффективен на мобилках и приятнее с точки зрения последующего квантования модели, и softmax на relu. в последнем случае, обученную с softmax модель, файнтьюнят 10к шагов с relu, и якобы не теряют в качестве. и уменьшают expansion в feedforward слоях с 8 до 6 без просадки в качестве. сверточные блоки вместо обычных сверток 3x3 используют depthwise-separable с меньшим числом параметров и операций. утверждается, что качество не просаживается. кроме того, уменьшили общее количество блоков с 22 в sd до 12. целевая модель должна иметь не более 400 миллионов параметров и 200 gflops на инференсе. авторы запустили перебор разных конфигураций блоков и ширин каналов детали процесса опущены и отобрали самых удачных кандидатов. пошаговая дистилляция для пошаговой дистилляции были рассмотрены две стратегии 1 прогрессивная дистилляция в 8 шагов сэмплирования 2 ufogen с адверсариальным лоссом автоэнкодер кроме того, вместо исходного автоэнкодера из sd, авторы обучили свой, более компактный, но кодирующий в 8 каналов вместо 4-х. потом его еще запрунили непонятно как . полученный vae и быстрее и лучше по метрикам."
188,2024-02-03T20:01:17,"эксперименты модель обучали на 150m изображениях из pali, 750к шагов на разрешении 256x256 и 250к шагов на 512x512. качество замеряют по fid и clip на 30k изображениях из ms-coco, следуя стандартной практике. кроме базовой модели md mobile diffusion c 400m параметров, обучают еще более компактную версию md-lite c 300m параметрами. на приложенных картинках все модели генерируют примерно одинаково хорошо, на одном уровне с sdxl, что с 50 шагами сэмплера, что с прогрессивной дистилляцией в 8 шагов, что с ufogen в одношаговое сэмплирование. по метрикам md, сэмплирующая в 50 шагов, на уровне sd-v1.5, 8 шагов имеет немного худший fid, и одношаговая уже просаживается заметно. по скорости выходит вполне себе здорово. при сэмплировании в 8 шагов, md почти в 2 раза быстрее разобранной ранее snapfusion, которую тоже дистиллировали в 8 шагов генерации. а в режиме одношаговой генерации удается достичь скорости в 238мс на изображение при замерах на iphone 15 . вывод достойный технический результат с использованием разных техник и приемов из прошлой литературы. однако, для полноты неплохо было бы иметь side-by-side evaluation c sd, и замеры разнообразия генераций."
189,2024-02-10T16:38:07,"альпаки бывают не только llmками, но ещё и домашними питомцами"
190,2024-02-10T23:48:45,"quip even better llm quantization with hadamard incoherence and lattice codebooks статья код только-только мы с коллегами выкатили aqlm , как конкуренты нанесли ответный удар llm-ки становятся все круче, новые модели, датасеты с инструкциями выходят почти на ежедневной основе. однако, самые сильные из опенсорсных моделей llama-2-70b с дохреналлионом файтьюнов, микстраль и загадочное нечто под названием miqu не влезают в колаб или условную rtx 3090 4090. квантизация в 4 бита до недавних пор бывшая парето-оптимальная недостаточна, чтобы позволить уместиться такой здоровенной модели на щупленькую видеокарту, а предыдущие методы квантизации в 2-3 бита ломают заметно модель и проще взять меньшую модель в большей точности. но свежие работы открывают дорогу к инференсу оверсайзд моделей на хорошей геймерской gpu. метод quip решетка, потому что e8 решетка квантования стоит на трех 1 incoherence processing 2 e8 решеточная векторная квантизация 3 дообучение неквантованных параметров 1 как известно, веса больших языковых моделей обладают выбросами outliers , плохо поддающимися квантованию, и в ряде прошлых работ было предложено их тем или иным образом изолировать. здесь же, следуя своей прошлой работе quip авторы домножают веса на случайное ортогональное преобразование, благодаря которому величины и чувствительности параметров растекаются равномерно по матрице incoherence processing . отличие от прошлой работы в том, что вместо произведения кронекеровских матриц для параметризации ортогонального преобразования используется случайное адамарово преобразование, более эффективное вычислительно и не уступающее по качеству. 2 поэлементная квантизация не совсем оптимальна в том смысле, что оптимизирует ошибку в пределах некоторого гиперкуба если рассматривать группы весов . а в действительности группы весов распределены в некотором шаре. а объем шара того же диаметра, что и сторона куба, в большой размерности много меньше описанного вокруг него куба. и при том же количестве кластеров квантованных значений , можно добиться значительно меньшей ошибки. для используемой в работе квантизации группами по 8 весов оптимальная решетка - e8, как было показано сравнительно недавно. 3 чтобы уменьшить ошибку квантизации, можно потюнить модель воспроизводить выходы слоев исходной модели. как и в aqlm авторы сначала дообучают неквантованные модели на уровне отдельных блоков трансформера. а на второй стадии обучают уже все неквантованные параметры на минизацию разницы между сжатой моделью и исходной fp16. для больших битностей 3-4 бита используется residual vector quantization, когда ошибка квантизации еще раз квантуется и конечный вес представляется в виде суммы квантованного веса и квантованной ошибки."
191,2024-02-10T23:49:58,"результаты одним словом, вышло здорово. валидация стандартная - бенчмарки по перплексии wikitext2 c4 и подборка zero-shot из lm_eval_harness arcc, arce, winogrande, piqa . на битностях 2,3,4 опережают известные методы по качеству почти везде. отрыв от aqlm небольшой, но все же есть. quip достигает парето-оптимальности при квантовании в 3 бита отдельные конфигурации aqlm парето-оптимальны в 2.5-2.6 битах . выбор решетки имеет влияние на качество модели, но дообучение неквантованных параметров накидывает больше. кроме того, все это хозяйство можно вполне эффективно инферить, с приемлемой производильностью. при этом авторы отвечают, что среди них нет мастеров по написанию cuda кернелов, и можно еще ускорить генерацию. выводы сильный результат и красивая математика. гонка по сжатию llm становится все захватывающее. интересно, какова же все-таки битность парето кривой при использовании самого лучшего из возможных методов квантования моделей и как скоро мы посадим почти lossless 70b модель на сolab."
192,2024-02-12T10:50:45,"возникла следующая мысль, навеянная работой то minicpm отличный разбор у сиолошной . типичные scaling laws для обучения языковых моделей имеют следующий вид l a n alpha b d beta l0 где l - лосс-функция на отложенной выборке l0 - ошибка идеальной модели n - число параметров в модели d - число токенов на обучении a, b - некоторые константы но в них никак не фигурируют детали обучения - learning rate, расписание обучения. вероятно, они не явно зашиты в константы, но зависимость нетривиальная. при нулевом learning rate ничего учиться не будет, очевидно, при learning rate выше некоторого порога лосс улетит в стратосферу. потому данный закон справедлив в некоторой области параметров. при большем learning rate модель больше подстраивается под значение батча, но в то же время больше информации, чем было в train она постичь не может по-любому. и для точной сходимости к оптимуму, по всей видимости, необходимо иметь малый learning rate в конце обучения чего можно добиться условным косинусом, линейным или 1 sqrt t угасающим learning rate . интересно, насколько, кроме самой архитектуры и данных, качество полученной модели зависит от параметров обучения, и насколько оптимально подобраны рецепты обучения всех этих шиншилл, ллам, мистралей и прочей живности..."
193,2024-02-16T10:56:14,"sora video generation models as world simulators полагаю, все уже успели полюбоваться новым детищем от openai под названием sora. классные разборы уже появились у светочей и рупоров отечетсвенного ai пост сиолошной, пост на любовь, смерть, трансформеры , как и веселые мемчики. со своей стороны лишь накину пару мыслей и соображений после прочтения отчетов и залипания в видосики. в отчете ожидаемо given the competitive landscape and the safety implications бла-бла-бла не прозвучали какие-либо детали о сборе и фильтрации датасета, архитектуре, пайплайне обучения. на основе отчета можно сделать следующиее выводы 1 архитектура модели - некий здоровенный трансформер, работающий на пространственно-временных патчах. 2 модель - латентная. есть энкодер, превращающий видео с большим пространственным и временным разрешением в некое компактное латентное представление, в котором и происходит диффузионный процесс. и декодер, превращающий латенты обратно в видео. 3 модель можно обуславливать на текст, текст изображение, и использовать для продолжения коротких клипов. кроме того, можно редактировать видео с помощью промпта и интерполировать два ролика осмысленным образом. 4 в процессе обучения модели, по всей видимости, на вход подавались и изображения, и картинки. картинка эквивалентна видео из одного кадра. 5 модель обучалась на исходных, необрезанных картинках. благодаря этому она не генерирует обрезанный контект и способна работать в любом разрешении. интересно, как они батчевали кадры разного разрешения. возможно, encoder decoder способен приводить все к одному разрешению, обуславливаясь на разрешение входа. а может, модель настолько большая, что батч больше одного все равно не лезет в сие чудище. 6 модель обладает пространственно-временной консистентностью. закрытие одного обьекта другим occlusion , не приводит к характерным артефактам. 7 генерировать можно ролики и изображения с разрешением до 2k. 8 как и в dalle-3, большую роль играют синтетические описания, сгененированные специально обученной для этого моделью. спекуляции и догадки 1 по всей видимости. модель под капотом и обьем данных вычислений, потраченных на обучения модели, реально колосалльны. скорее всего сама модель значительно больше, чем sdxl, emu, и иные модели, о которых хоть что-то известно. 2 полагаю, что в обучении было задействовано много синтетики зарендеренной unreal engine 5, или чем-то подобным. многие ролики напоминают генерации движка 3d графики. таким образом можно задавать пространственно-временную информацию куда более явно, чем weak supervision с огромного числа видеороликов и клипов."
196,2024-02-21T17:55:36,"aqlm добавили в transformers 4.38.0! https github.com huggingface transformers releases tag v4.38.0 пользуйтесь, наслаждайтесь, делитесь впечатлениями кроме того, мы потюнили end-2-end все обучаемые параметры на логиты fp16 модели и сравнялись по качеству на llama-2-7b моделях с quip . обновленные 7b модели уже лежат в хабе, остальные будем добавлять по мере готовности. https huggingface.co blacksamorez llama-2-7b-aqlm-2bit-1x16-hf wikitext2 ppl было 6.31, стало 5.92 https huggingface.co blacksamorez llama-2-7b-aqlm-2bit-2x8-hf wikitext2 ppl было 7.91, стало 6.69"
197,2024-02-22T17:16:50,"stability.ai анонсировали stable diffusion 3. пока без конкретных деталей, но обещают в будущем папиру. что известно на данный момент 1 будет несколько моделей от 800m до 8b как минимум 2 размера 2 по всей видимости в латентном пространстве работает некая вариация dit как в sora 3 flow matching еще что-то делает модель доступна только через waitlist для early preview. типа из соображений безопасности, чтобы никто не начал использовать ее во вред человечеству и для удовлетворения небогоугодных фантазий. анонс на love. death. transformers."
198,2024-02-24T08:37:54,"sdxl-lightning progressive adversarial diffusion distillation статья кода нет, но есть модели пока все ждут технического отсчета по stable diffusion 3 и стебутся над поделками от google, рассмотрим свежую работу про дистилляцию диффузионок. некоторое время назад stability.ai выкатили sdxl-turbo, дистиллированную с адверсариальными лоссами версию sdxl. ребята из bytedance предложили альтернативный хоть и основанный на схожих принципах метод дистилляции в один и малое число шагов. метод предложенный подход основывается на progressive distillation чуть ли ни самой первой технике уменьшения шагов сэмплирования . однако в исходной работе качество сильно проседало при уменьшении числа шагов до однозначного. авторы утверждают, что диффузионные модели при многошаговой генерации обладают большой константой липшица не путать с автором десятитомника по теоретической физике и могут аппроксимировать сложные траектории. при малошаговой не могут, и выдают лучшее, что позволяет их емкость при оптимизации mse лосса - мыльные генерации. предлагается вместо mse лосса использовать адверсариальный - чтобы дискриминатор не мог отличить результат расшумления учителем от ученика. в sdxl-turbo дискриминатор работает в пиксельном пространстве, что требует декодирования картинки, обработки ее vitом в довольно высоком разрешении и вся процедура выходит довольно дорогой. здесь же авторы в качестве тушки дискриминатора используют копию энкодера sdxl и классификатору подают агрегированные признаки со всех уровней. при обучении нет нужды декодировать картинку и можно обучать оценивать реалистичность промежуточных шагов. в качестве функции потерь берут ванильный non-saturating adversarial лосс. однако, оказывается, что полученная сеть подвержена генерации артефактов в стиле двуликого януса существа и объекты раздваиваются. утверждается я не понял почему , что это тоже проявление недостаточной емкости модели. авторы дообучают модель еще некоторое время на adversarial лосс, но без примеров родительской модели. утверждается, что это лечит янус-артефакты. дистилляция проходит в несколько стадий 128- 32- 8- 4- 2- 1. первая стадия довольно простая, и даже обычный mse лосс работает, а дальше используют предложенную технику. classifier-free guidance применяется только на первом этапе дистилляции."
199,2024-02-24T08:39:49,"эксперименты обучают на отфильтрованном классификатором эстетичности подмножестве из laion и coyo. при обучении вместе с генерациями учителя используют lora, которую затем вшивают в веса модели, и уже всю модель учат дурить классификатор. интересно, вывод о недостаточной емкости модели был сделан до или после решения обучать с lora? кроме того, обучают на расписаниях с разным количеством шагов обучения. говорят, стабилизирует результат. визуально генерации в один и малое число шагов выглядят неплохо. генерации в один шаг часто меняют существенно картинку, не нарушая семантики. для численной оценки качества используют fid на всех картинках, fid на патчах и clip score. для замера fid к целому вагону недостатков на целых изображениях надо приводить картинки к нестандартному разрешению 299x299, и это вносит сдвиг в результат замеров, потому предлагается смотреть и на кропы. по fid и clip sdxl-lightning сопоставим с конкурентными sdxl-turbo и latent consistency models, но несколько лучше по fid на патчах если не можешь явно побить бейзлайны по метрикам, придумай правильную метрику! . все дистилляты несколько проседают по fid, в первую очередь из-за разнообразия, по всей видимости. выводы логичный и интересный подход, но не хватает sbs с асессорами. кроме того, неизвестно насколько sdxl-lightning подвержен проблеме снижения разнообразия генераций, коей страдает sdxl-turbo."
202,2024-02-25T20:17:01,"этическая дилемма - это когда рецензируемая статья не очень, но ее автор активно цитирует тебя."
203,2024-02-27T23:15:00,слайды с моего доклада про optimal brain surgeon на сегодняшней встрече в точке кипения . спасибо всем участникам за вопросы и интересные дискуссии! и спасибо kraidiky за приглашение и увлекательный рассказ про коннектом.
204,2024-02-28T23:55:33,"the era of 1-bit llms all large language models are in 1.58 bits статья довольствуйтесь readme.md либо результат этой статьи невероятно крут, и ваш покорный слуга может завязывать с сжатием и идти продавать пирожки с капустой, либо бы имеем дело с обманом почище ultrafastbert. утверждается ни много не мало следующее - можно обучать llm с 1.58 log2 3 битами на параметр , которые сходятся так же хорошо как и fp16 модели! метод в линейных слоях все веса квантуются тернарно принимают значения -1, 0, 1 . скейл будто бы берется один на весь тензор, даже не канал, что вдвойне удивительно, учитывая склонность llm к оутлаерам и неравномерность распреления весов. активации же приводятся к 8 битам, и чтобы не хранить zero-point, значения активаций симметризуются. ничего не сказано про процедуру обучения, был ли использован straigth-through estimator, или что-то еще более хитрое безградиентное для обучения тернарных весов. ибо наивное обучение всего этого хозяйства должно сходиться из ряда вон плохо. эксперименты за основу берут llama архитектуру и обучают модели разного размера от 700m до 3.9b параметров на redpajama. в сравнениях с бейзлайновой llama fp16 все модели поглощают 100b токенов на этапе обучения. модели сравнивают по перплексии на wikitext2 c4 непонятно, какая из них приведена в таблице 1, или средняя и zero-shot на lm-eval-harness. на меньших моделях bitnet1.58 так называется семейство квантованных моделей лишь слегка уступает бейзлайну, а на больших - будто бы чуть лучше, чем fp16. замена большого числа умножений на сложения и разности дает огромный потенциал для ускорения. при том же размере модель в разы быстрее, экономичнее по памяти, и жрет куда меньше драгоценной энергии. при обучении на 2t токенах, bitnet1.58 на бенчмарках лучше stablelm-3b, обученной на том же числе данных. вывод практически полное отсутствие описания метода и протокола обучения делает результаты данной работы крайне сомнительными. краткость - сестра таланта, только если ты openai. в общем, ждем дальнейшей информации. может, таки выложат модели и нечто большее, чем readme."
205,2024-03-05T15:12:08,"scaling rectified flow transformers for high-resolution image synthesis статья кода нет, но обещают stability.ai таки выкатили обещанный отчет про обучения stable diffusion 3. метод сама по себе процедура обучения представляет комбинацию большого числа техник из прошлой литературы. авторы перебирают различные архитектурные конфигурации, диффузионные процессы, расписания зашумления и иные трюки, чтобы достичь наилучшего качества. диффузионный процесс авторы рассматривают следующие постановки диффузионных процессов 1 rectified flow который как понятно из названия используется в конечной модели . по существу, линейная интерполяция между изображением и шумом. 2 edm с variance exploding. 3 косинусное расписание из adm. модели предсказывают либо шум, либо скорость. кроме того, рассматриваются различные расписания зашумления. в наивном подходе шаги сэмплируются равномерно по времени диффузии. однако предсказание скорости сложнее в середине траектории, вдали от распределения данных и распредения шума6 потому целесообразно сэмплировать чаще в середине. желаемого эффекта можно достичь с помощью logit-normal sampling log t 1 - t и cosmap. авторы ablaтят различные выборы, смотря на метрики генерации clip-score, fid в class-conditional генерации на imagenet и cc12m. оказалось, что rectified flow rf с логит-нормальным распределением зашумления работает лучше всего."
206,2024-03-05T15:12:59,"архитектура прошлые работы показали, что увеличение размерности латентных кодов повышает качество. следуя данному наблюдению, авторы попробовали автоэнкодер с 4 как и в прошлых sd , 8, 16 каналами. увеличение ширины стабильно накидывает по метрикам. самое примечательное - архитектура трансформера. в типичной диффузионной unet архитектуре условие подается в виде модуляции сверток и через cross attention с патчами изображения латента. здесь же токены изображения и текстовые конкатенируются и обрабатываются совместно единым мультимодальным трансформером. в данной работе авторы используют 3 текстовых энкодера разной размерности - clip-l 14, clip-g 14 и t5xxl. эмбеддинги первых двух энкодеров конкатятся и паддятся до размерности t5xxl которая больше, чем у l 14 и g 14 вместе взятых. то есть имеем 3 потока токенов - текстовые токены от clip-l 14 clip-g 14, текстовые токены от t5xxl, токены изображения. причем feedforward слои, нормализации и модуляционные слои имеют свои веса для каждого из трех перечисленных выше потоков токенов. attention проекции общие для всех токенов. иногда пре-attention выдает слишком большие значения, и для повышения стабильности обучения авторы нормализуют ключи и значения перед attention c помощью rmsnorm. предложенная архитектура работает лучше, чем uvit unet с token merging и splitting , crossattention и dit с общими mlp, модуляциями для всех типов токенов. иметь отдельные слои для разных текстовых энкодеров полезно, хоть выигрыш сравнительно небольшой. модели сравниваются на основе динамики валидационного лосса метрик на сс12m. следуя прошлым работам, авторы добавляют синтетические описания, сгенерированные через cogvlm в пропорции 50 50 . их добавление в обучение заметно улучшает согласованность текста и изображения по мнению разметчиков."
207,2024-03-05T15:13:38,"результаты нащупав хороший сетап, авторы запускают полномасштабное обучение. самая большая модель имеет 8b параметров с учетом t5xxl или без? . данные отфильтровываются по наличию nsfw контента , эстетичности, и дубликатам. на начальной стадии обучаются на 256x256 изображениях, а затем переходят генерации на разрешениях до 1024x1024 с разными aspect ratio. при дообучении на высоком разрешении оказывается важным изменить расписание шагов зашумления, так как изображение более высокого разрешения имеют больше сигнала. сдвинутое расписание улучшает качество, согласно оценке аннотаторов. после обучения на высоком разрешении, модель дообучают с помощью dpo на улучшение эстетичности и пользовательских предпочтений. примечательно, что обучают не все параметры, а lora адаптеры. scaling модели стабильно улучшает качество. разные модели отличаются и шириной и глубиной. валидационный лосс score matching loss хорошо коррелирует с пользовательскими предпочтениями на geneval и t2i-compbench. stable diffusion 3 сравнивают на parti-prompts с прошлыми версиями stable diffusion, pixart-alpha и проприетарными моделями - dalle-3, midjouney-v5, ideogram-v1.0. stable diffusion 3 заметно опережает прошлые sd, pixart и слегка проприентарные модели. основной выигрыш за счет typography, разница по визуальной эстетике не столько велика. большой текстовый энкодер t5xxl полезен при сложных промптах, но особо не влияет на эстетическое качество. выводы сильная модель, вобравшая в себя достижения современной науки и значительный инженерный труд. с точки зрения науки никаких прорывных идей, киллер-фич не предложено. ждем код и возможность поиграться с моделькой. rectified flow постановка по идее должна благоприятствовать хорошим генерациям в малое число шагов."
208,2024-03-07T10:42:19,"gemma оказалась камнем с дефектами. https x.com danielhanchen status 1765446273661075609?s 20 проблемы gemmы 1 нету bos токена. я устал, bos 2 очепятка в end_of_turn 3 sqrt d_embed sqrt 3072 55.4256, но bfloat16 55.5 4 layernorm w 1 не во float32 5 баг в bfloat16 rope 6 rope чувствительна к выбору между y 1 x или y x 7 пофиксили rope должен быть в float16. rope может содержать числа выходящие за машинную точность, потому обыкновенно применяется в float32, а затем выход кастуется к fp16 bf16 при инференсе обучении в половинной точности. а я-то думаю, чего это aqlm квантизация разваливает 7b модель..."
210,2024-03-07T10:46:22,"gemma-2b квантуется, кстати, вполне сносно. вчера выложили 2 модели 1 https huggingface.co blacksamorez gemma-2b-aqlm-2bit-1x16-hf 2 https huggingface.co blacksamorez gemma-2b-aqlm-2bit-2x8-hf правда, эмбеддинги и голова занимают большую часть памяти теперь. планируем сжать их в будущих релизах, чтобы уместить модельку в 1гб."
211,2024-03-08T14:05:23,"поздравляю дорогих подписчиц с профессиональным праздником! желаю вам всех благ, красоты, счастья, любви и здоровья. вы - настоящее украшение наших датасетов для генерации изображений и в топ классификаторов эстетичности"
212,2024-03-10T22:56:48,"сказ о потерянном skip connection. забабахал архитектуру. обучаю трансформер. чет лосс почти не падает. кручу-верчу лернинг рейт, один хрен. смотрю на выход с zero инициализацией на последнем слое , и он еле-еле колеблется в районе 0. чешу репу, смотрю код, и опа! в трансформерном блоке было нечто такое для краткости опущены нормализации и rope x x self_attn x x cross_attn x x x feedforward x добавил потерянный skip и все заработало . мораль сей басни такова - не теряйте skip connections!"
213,2024-03-12T23:53:12,"bitnet scaling 1-bit transformers for large language models статья официального кода нет пойдем в обратном хронологическом порядке и откатимся на несколько месяцев и 0.58 битов назад по отношению к статье. собственно, здесь по существу и расписана вся экспериметнальная постановка, на которой основывается статья с тернарной квантизацией. метод линейные слои заменяются на bitlinear, где элементы принимают только два значения -1 и 1. если вес больше среднего элемента по матрице - то он 1, иначе -1. и общий масштаб равен среднему модулю веса. максимально дешево и сердито . такая операция недифференцируема, потому используется ste. на этапе обучения авторы хранят полный тензор весов во float. активации квантуются в 8 бит с помощью равномерной сетки. чтобы веса лучше квантовались, перед линейным слоем стоит layernorm. вычисление среднего значения для определения среднего значения по тензору требует синхронизации при model parallelism. и во избежание лишней коммуникации между процессами при распределенном обучении среднее считается по части матрицы, лежащей на данном процессе. странно, что они не рассматирвают поканальное квантование, или даже группами по 256 выразительность больше, а размер модели все еще не сильно отличается от 1 бита. далее авторы приводят табличку, показывающую насколько энергоэффективнее int8 по сравнению с fp16 на разных техпроцессах. на случай, если грета будет ревьюером. эксперименты обучают на смеси pile, commoncrawl, cc-stories, realnews модели размером от 125m до 6.7b. по перплексии полученные модели ожидаемо хуже бейзлайна в fp16, но зато, если нормализовать на энергопотребление, то bitnet смотрится весьма представлительно. обучение bitnet с ste требует большего learning rate для адекватной сходимости 8e-4 , на котором fp16 расходится. по 0-шот бенчам якобы не слишком сильно уступают fp16 трансформеру, однако вопрос в том, насколько хорошо был затюнен сам бейзлайн, и сами бенчи были выбраны по-хитрому. bitnet 6.7b выдает 55.9 на бенчах, но при этом качество случайной модели 43.8 и относительный отрыв от рандом выглядит не таким большим. конкурентые ptq подходы, стартующие с fp16 трансформера конкренто ломаются в низкой битности тогда еще не вышли quip и aqlm . вывод с одной стороны, неплохо, что это уже как-то работает, но бенчи и бейзлайны были подобраны не слишком честно bitnet1.58 выдает уже значительно более высокое качество, благодаря возможности принимать хотя бы 0-ое значение."
214,2024-03-13T00:11:59,"забавный каламбур выйдет, если relaxml выпустят quip версию свеженькой https huggingface.co cohereforai c4ai-command-r-v01. модель от cohere подвергнется incoherence processing."
215,2024-03-17T22:50:59,"маск долго тянул интригу, но таки к исходу недели выпустил grok на волю. https github.com xai-org grok 1 здоровенная херобора на 314b. 2 код модели и инференса на jax haiku. 3 8 экспертов на инференсе активируются 2 . 4 словарь - 128к токенов. 5 модель идет с 8-битной квантизацией что все равно потребует 4 a100, чтобы только уместить 6 длина контекста - 8к 7 архитектура навскидку та же, что у mixtral llama-подобный трансформер"
216,2024-03-18T11:58:26,"galore memory-efficient llm training by gradient low-rank projection статья код введение в связи со стремительным ростом размера моделей, все более острой становится проблема memory-efficient обучения llm. по существу, рядовому пользователю доступны лишь parameter-efficient файнтьюны предобученных моделей. в разобранной ранее статье relora было предложено обучать несколько низкоранговых добавок и вливать их в веса. однако, для достижения качества, не сильно уступающему обучению всей модели целиком, этап обучения всей модели целиком был необходим, то есть multi-gpu обучения все равно не избежать. в этой статье был предложен подход, который позволил достичь с llama-7b-подобной моделью качества близкого к полному обучению, доступный владельцам 1 gpu с 24гб vram. метод суть метода довольно проста и не нова, по существу . низкоранговые lora добавки неплохо работают на стадии дообучения, но эффективный pretrain требует заметать в процессе обучения пространство большой размерности. однако, сами изменения весов могут и авторы даже дают некоторое обоснование данному явлению лежать в пространстве низкой размерности. отсюда мысль - проектировать градиенты и состояния оптимизатора в пространство низкой размерности. а именно делают следующее 1 считают градиент по весу на n-шаге 2 считают его svd разложение, откуда достают первые r векторов, отвечающих главным сингулярным значениям. 3 проектируют на полученные подпространства состояния оптимизатора в основе работы метода - предположение о том, что градиенты слабо меняются между соседними итерациями. прикрутить его можно более-менее к любому стандартному градиентному алгоритму оптимизации - sgd, adam хоть исходный, хоть adam8bit . galore требует даже меньше памяти на линейный слой c n входными и m выходными нейронами, чем lora с adam . lora mn mr nr параметров, 2mr 2nr состояний оптимизатора galore mn параметров, mr 2nr состояний оптимизатора"
217,2024-03-18T12:00:13,"эксперименты рассматривают две постановки 1 предобучение на с4 2 дообучение на glue обучают на c4 на бюджетах порядка 1-10b токенов, ибо авторы не слишком богатые . на предобучении galore лишь немного уступает стандартному обучению всей модели целиком и заметно опережает relora. просто низкоранговые веса работают ожидаемо плохо. galore хорошо совмещается с adam8bit, давая значительную экономию в памяти и не просаживая качество. экономия по памяти 63.3 по сравнению с bf16 adam и 52.3 c 8bit adam, соответственно. основные два параметра у алгоритма - 1 размерность низкорангового пространства и 2 частота обновления. чем больше размерность - тем ближе алгоритм к adam, но и прожорливее по памяти. слишком часто обновлять матрицы проекции - вычислительно накладно и не очень хорошо по качеству, слишком редко - дешево, но плохо по качеству. лучше всего работает обновление раз в 100 шагов обучения полагаю оптимум зависит от размера батча и не увеличивает среднее время на шаг обучения. дообучение на glue дает примерно то же качество, что и lora при том же ранге. вывод по ощущениям, хороший и полезный результат, с серьезными перспективами на практике. интересно, насколько хорошо предложенный метод сработает на large-scale обучении порядка триллиона токенов. правда, те, кто могут себе позволить себе обучать на триллионах токенов, имеют в распоряжении не один хост с high-end gpu"
219,2024-03-20T21:52:06,"fp8 и с чем его едят статья док в transformer.engine введение невероятный прирост производительности каждого нового поколения nvidia на графиках в презентациях gtc кроме объективного улучшения архитектуры обусловлен еще и переходом к типам все более низкой точности что было остроумно подмечено love. death. transformers. . как известно, инферить нейросетки с весами низкой точности можно и нужно. но, вопрос в том, насколько сложно обучать в числах с плавающей точкой пониженной точности. наивное обучение в fp16, без automated mixed precision, как известно, нередко приводит к неприятным сюрпризам типа nan в лоссах и градиентах. градиенты могут быть слишком большими, чтобы представляться в fp16, либо слишком маленькими, и адаптивно подбираемый loss scale сдвигает гистограмму градиентов в нужный диапазон значений. есть еще bf16 поддерживаемый, начиная с архитектуры ampere , который имеет более широкий диапазон, но большую погрешность представлений. и в семействе hopper добавили поддержку вычислений с fp8. метод fp8 - на самое деле, не 1 , а 2 типа. e4m3 c 4 битами на экспоненту и 3 на мантиссу используется для весов и активаций, которые обычно лежат в сравнительно узком диапазоне и важнее точность представления, чем возможность принимать экстремальные значения. бесконечность не представлена. e5m2 с 5 битами на экспоненту и 3 на мантиссу используется для градиентов и состояний оптимизатора, где допустима большая погрешность в представлении и больше разброс принимаемых значений. обычный loss scaling как в half precision не работает, и приходится иметь для каждого тензора адаптивный масштаб как в квантизации , чтобы загнать его в удобный диапазон. определять его на лету накладно. и потому предлагается хранить некоторое скользящее среднее максимумов и на него масштабировать. эксперименты авторы тестируют эффективность обучения в fp8 на imagenet-1k, lstm на wmt и претрейне gpt-3 подобного трансформера на pile. fp8 почти везде смог показать конечное качество как half precision бейзлайн за исключением mobilenet-v2, где точность просела на 0.5 . на языковых задачах fp8 модели достигают примерно того же качества, что и half precision. в ablation показывают, что адаптивный масштаб для каждого тензора при конвертации bfloat16 модели в fp8 важен, иначе заметно проседает качество даже при оптимальном выборе сдвига экспоненты. предобученный half-precision bert без проблем представляется в fp8. вывод fp8 тип выглядит вполне полезным и перспективным. однако, обучение llmок с ним, по всей видимости, требует дополнительной возни и несколько вагонов с h100 и маленькую тележку . потому на текущий момент, известные открытые модели обучались в half precision. вероятно, openai и anthropic что-то пробовали шаманить в fp8, но кто об этом расскажет"
220,2024-03-25T16:19:38,"появилась неофициальная версия мистрали-7b, которая является с настоящий момент более-менее sota среди публичных моделей в легкой весовой категории. однако, судя по замерам пользователей, новая версия не то, чтобы улучшилась по сравнению со своей предшественницей. пост на abstract.dl"
222,2024-03-25T16:24:38,"по собственным замерам, наблюдаю небольшое улучшение перплексии на 8k контексте, и нестатзначимую просадку на 0-shot. возможно, при конвертации пользователь с ником alpindale немного повредил модельку."
223,2024-03-27T21:34:19,"denseformer enhancing information flow in transformers via depth weighted averaging статья код skip connections позволяют обучать по-настоящему глубокие сети. впервые они появились в статьях про resnet и unet, и с тех пор было предложено множество их вариаций - densenet connections, csp, и зоопарк вариаций. тот же трансформер унаследовал структуру skip connections от resnet. в этой работе по существу воплотили densenet в трансформере. метод densenet-like skip connections реализованы следующим образом ключи и значения в данном блоке получаются как взвешенная сумма ключей и значений с этого и всех или не только лишь всех предыдущих блоков. дополнительного оверхеда по памяти нет , так как kv-кэши активации с прошлых блоков все равно придется сохранять для backpropagation или авторегрессивной генерации. чтобы воспроизводить поведение стандартного трансформера - веса с прошлых блоков инициализируются нулями. однако, все равно приходится делать массу сложений, что замедляет работу сети. потому авторы предлагают две модификации 1 dilated denseformer. считается взвешенная сумма только через k-слоев. 2 periodic denseformer. добавление kv проводится только для каждого k-го токена. обе модификации снижают объем вычислений, не снижая качество. эксперименты модели обучают openwebtext и смотрят на перплексию на валидационной выборке. бейзлайновые сети довольно глубокие 48 - 90 блоков и узкие 8 голов по 64 канала. при том же числе параметров denseformer немного лучше трансформера по перплексии, но и медленнее. потому авторы сравниваются еще и в сетапе с заданным временем инференса, и denseformer оказывается все еще эффективнее. dilated и periodic опции заметно ускоряют модель, при этом не просаживая по качеству с полносвязным denseformer. далее смотрят на величину выученных коэффициентов. ожидаемо, наибольший вес имеют коэффициенты в данном блоке, но еще сильны skip connections с первым блоком. многие веса малы по величине, тем не менее важны, и прунинг даже небольшой доли самых малых весов сажает качество. вывод дешевая и вроде бы эффективная настройка над трансформером. однако постановка экспериментов изначально неблагоприятна для базовой модели, так как глубокие узкие модели имеют свойство тяжело обучаться. вероятно, более широкий трансформер с тем же числом параметров отработал бы лучше. а для неглубоких сетей, скорее всего, выигрыш от конструкции незначителен. да и много что, дающее выигрыш на малых экспериментах не воспроизводится на масштабе..."
224,2024-03-29T06:47:25,"towards 1-bit machine learning models блогпост спасибо за наводку https t.me toshoseti сжатие и ускорение llm нынче пользуется большим спросом, и потому существуют кожаные мешки, пытающиеся сорвать хайп на этой теме.. и вот недавно вышел воистину чудный образец. метод 1 квантуют модель методом hqq 2 дообучают lora адаптер поверх квантованной модели все вместе называется hqq . эксперименты все эксперименты проводятся с llama-2-7b. для дообучения базовой модели берут 2.8k примеров из wikitext2, для instruction finetuning - cмесь из guanaco, orca-math, metamathqa, ultrafeedback_binarized. на базовой модели hqq ломает полностью модель при 1-битном квантовании, но lora адаптер якобы выравнивает модель по качеству с 2-битным quip . а 2-битная по перплексии даже лучше исходной! переходим на 1 и 2-битные модели? но есть нюансы 1 сравнение проводится со старой версией quip . да и то, почему-то перплексия хуже, чем заявлено в официальном блоге. а новые версии aqlm и quip достигают перплексии 6.2 на wikitext2. 2 нет замеров на каких-либо других бенчмарках хоть c4 и 0-shotах из lm-eval-harness . потому наверняка просто имеем дело с оверфитом под датасет, а сами модели не рабочие где-либо еще. this is a significant finding, as it suggests that quantization with hqq not only reduces the memory footprint and computational requirements but can also potentially improve the model s language modeling performance. при aqlm квантовании в 2 бита у нас тоже улучшилась перплексия по сравнению с fp16, но качество на 0-шотах было хуже, чем у базовой модели. так что нихрена это не улучшение. при instruction finetuning 1-bit hqq снова ломает модель до уровня рандома качество около 1 число ответов . но адаптер позволяет оторваться на 10 от уровня рандома. а 2-битное квантование уже близко по качеству к llama-2-7b-chat. однако, снова нет сравнения с sota quantization methods, и бенчмарки вызывают вопросы. llama-2-7b-chat на truthfulqa имеет качество 57.04 , а здесь репортят 45.32 . выводы по большей части лютый скам, но все же некоторая мораль есть. адаптер поверх сжатой модели дает серьезную компенсацию даже для поломанной модели. идея применить адаптер поверх квантованной модели не нова и ведет начало как минимум от qlora."
225,2024-03-30T19:36:27,"когда в торчовом модуле линейные слои реализованы через матричное умножение на параметер, а не через nn.linear."
226,2024-03-30T19:37:21,"пояснение к приколу выше. многие методы data-aware квантизации прунинга зиждятся на forward хуках поверх nn.linear, nn.conv d."
227,2024-03-30T19:39:28,что вам мешало cделать nn.modulelist из nn.linear?
228,2024-03-31T18:38:26,"finite scalar quantization vq-vae made simple статья код скорее есть, чем нет очередная статья про квантизацию, но в другом контексте . вариационные автокодировщики с дискретными латентными кодами показали себя в качесте неплохих генеративных моделей vq-vae, vq-vae-2 . кроме того, их способность эффективно представлять высокоразмерные данные лежит в основе латентных диффузионных моделей. однако, обучение vq-vae - довольно нетривиальная задача, требующая тонкой настройки. в отсутствие специальных манипуляций активации отображаются в малое число кодовых векторов, в то время как основная масса кодов сидит и грустит . приходится накладывать регуляризацию, чтобы равномерно заметать кодовую книгу. коды обновляются через ema. поиск ближайшего соседа также требует определенных вычислительных затрат. в этой работе предложили использовать фиксированную гиперкубическую решетку для квантования представлений. метод входные активации отображаются в некоторый гиперкуб с заданным числом узлов l вдоль каждой размерности. чтобы ограничить диапазон принимаемых значений, ко входу применяют tanh f z l 2 tanh z - округление к ближайшему целому кодовые векторы - всевозможные узлы в этой решетке, т.е размер кодовой книги l d, d - размерность латентного пространства. причем размерность пространства небольшая - не более 10 во всех экспериментах. достоинством подхода является быстрое квантование - нужно просто найти нужный узел на решетке, зная координаты, и меньшее число обучаемых параметров хотя коды и так мало весят по сравнению с энкодером и декодером . как и стандартном vae квантование недифференцируемо и потому градиент пробрасывается через ste. эксперименты метод валидируют на архитектурах maskgit генерация изображений и uvim оценка глубины . на малом числе кодовых векторов стандартная формулировка работает немного лучше, с увеличением числа кодов vq-vae общего вида начинает страдать в качестве, в то время как у fsq метрики монотонно уменьшаются. по fid оптимальное качество fsq на одном уровне с vq. fsq задействует все кодовые векторы, в то время как у базового vq есть неиспользуемые коды. на depth estimation метрики также на одном уровне с vq-vae общего вида. вывод занятно, что в области vq-vae пошли в противоположную сторону от текущих трендов квантизации llm - от более общей векторной квантизации к скалярной. интересно, насколько полученные коды хороши для обучения латентных диффузионных моделей?"
229,2024-04-01T10:56:28,"уральский умелец собрал рассказы и премудрости своего деда, перевел их на английский, дообучил на этом mixtral-8x7b и вырвался на первую строчку openllmleaderboard c большим отрывом!"
230,2024-04-02T11:09:35,"маленький лайфхак. хотите вы протестировать фунциональность вашего кода на какой-то новомодной llm-ке, но десятки, а то и сотни гигов качать впадлу. да и если раздебаг на колабе, банально и не влезет. кроме того, при прогонке кода обычно приходится ждать , пока чекпоинт подгрузится, да и прогонка активаций через большую модель занимает время. решение простое 1 cоздаете модель через autoconfig 2 урезаете число слоев и ширину модели 3 вуаля. можете хоть на cpu гонять. допер только на днях. до этого гонял llama-2-7b в большинстве тестов. колаб с примером"
233,2024-04-05T07:15:19,"продолжение истории не у одного меня бомбануло с этой особенности dbrx https t.me quant_prune_distill 227 и ребята подняли issue в репозитории модели. пользователи жалуются на отсутствие возможности конвертации в bnb формат и обучения lora поверх экспертов. потому один умелец приготовил модель, где эксперты используют nn.linear. конвертированная модель"
234,2024-04-05T10:37:42,"bigger is not always better scaling properties of latent diffusion models статья кода нет, а и tpuv5 у вас введение есть такие scaling laws, согласно которым, чем больше модель и чем больше количество данных, на которых она училась - тем лучше ее качество или некий эрзац качества. для llm уже вышло множество работ по данной теме и выработаны compute-optimal рецепты обучения, но для диффузионных моделей систематического исследования до сих по не проводилось. и ребята с google вооруженные tpu решили заполнить зияющую пустоту . эксперименты в качестве базовой модели берут unet от stable-diffusion v1.5 866m и меняют его ширину глубина постоянная . и таким образом получают семейство моделей от 39м до 5b параметров. модели обучают на проприетарном датасете из 600m пар изображений и описаний к ним, отфильтрованном по эстетике. базовые настройки сэмплера - ddim солвер с 50 шагами и cfg_scale 7.5. исследуют следующие задачи 1 text-2-image генерация 2 superresolution через дообучение модели из 1 следуя постановке из stablesr. 3 dreambooth поверх модели из 1 1 для оценки качества используют по традиции fid и clip. в области некоторого среднего бюджета обучения модели разных размеров выдают примерно одни и те же метрики. но при наличии значительного объёма вычислительных ресурсов - размер начинает играть значение, и самая большая модель побеждает своих меньших собратьев. для 5b модели пришлось прибегнуть к memory-efficient training, так как при стандартном обучении с adam amp модель с состояниями оптимизатора не влезает в tpuv5. совсем маленькие модели генерируют дефектные картинки , но начиная с нескольких сот миллионов параметров качество генераций моделей примерно на одном уровне по ощущениям. 2 на downstream задачах размер моделей играет уже более существенную роль. в 4x image super-resolution большие модели даже при заданном бюджете обучения заметно выигрывают по fid. однако по lpips модели разного размера с фиксированным training compute сравниваются. 3 большие модели ожидаемо выдают более качественные dreambooth генерации."
235,2024-04-05T10:41:59,"в поисках эффективного сетапа сэмплирования далее авторы исследуют зависимость качества генераций от cfg_scale для разных размеров моделей и количества шагов сэмплирования. выводы следующие 1 чем больше шагов сэмплирования - тем ниже оптимальный cfg scale 2 чем больше модель - тем больше оптимальный cfg scale при генерации с фиксированным ограничением на flops меньшие модели на малых бюджетах показывают себя лучше больших. то есть лучше сделать больше шагов меньшей моделью, чем меньше - большей. выводы справедливы как для ddim семплера, так и для более навороченного dpm . возникает следующая мысль - а если дистиллировать модель по числу шагов генерации, не станут ли больше модели предпочтительнее при ограничительном бюджете генерации? оказывается, что малая недистиллированная модель с большим числом шагов сэмплирования может выдавать одно и то же качество, что и большая дистиллированная в 4 шага. в качестве алгоритма дистилляции используют consistency distillation. на downstream superresolution задаче при малых бюджетах малые модели предпочтительнее, но с ослаблением ограничений ожидаемо выгоднее становится брать более крупные, ибо малые упираются в потолок по качеству. выводы довольно интересное и содержательное исследование. на мой взгляд, самый практически интересный результат работы в том, что при жестких ограничениях на бюджет генерации предпочтительнее уменьшать модель, чем число шагов генерации. однако исследование ограничительно тем, что использует метрики, которые могут плохо кореллировать с human perception и полноценная валидация предполагает sbsы. тем более, что у гугла деньги на это есть ."
236,2024-04-06T10:39:08,"half-quadratic quantization of large machine learning models блог код введение некоторое время назад здесь мы разбирали работу hqq с довольно спорными заявлениями. и сейчас откатимся немного назад в прошлое и рассмотрим исходный метод квантизации - hqq. 1 есть data-agnostic методы квантизации - round-to-nearest, bnb квантование, не требующие входных данных. 2 есть data-aware методы - gptq, awq, spqr, squeezellm, quip , aqlm и д.р, которые ищут конфигурацию квантованных весов, оптимизирующую ошибку на некотором входном распределении данных. вторая группа методов обычно точнее по качеству, но более вычислительно затратна. кроме того, имеет место переобучение под калибровочные данные. метод в данной работе предлагают метод, якобы гарантирующий одновременно хорошее качество и не требующих никаких данных. исходная функция ошибки l_p норма c 0 p 1 , которая устойчива к выбросам, характерным для больших языковых моделей. скейлы квантизации фиксируют, и оптимизируют zero_point. однако, в исходном виде, задача плохо решается из-за невыпуклости, потому авторы вводят вспомогательную переменную, которая в оптимуме равна разнице между исходными и квантованными весами, и решают полученную задачу альтернированной оптимизацией, поочередно оптимизируя скейлы и фиктивную переменную. согласно заявлениям авторов, метод сходится за пару десятков итераций. эксперименты метод валидируют на моделях семейства llama-2 и openclip vitах. в первом случае замеряют перплексию на wikitext2, а во втором качество 0-shot и linear probe. 8-битная квантизация работает без просадки по качеству. 4-битная с небольшой, 2- и 3- битные квантизации уже заметно просаживаются по сравнению с fp16, но все еще выдают стабильный результат. но тут есть нюанс - 2-х и 3-х битные квантизации используют малые группы 64 для 3-битной и 16, 32 для 2-битной . в одной постановке scale хранится в 16 битах, в другой квантуется в 8 бит. потому hqq_g16 - это на самом деле 3.5 бита на параметр, а hqq_g16 - 3 бита. работает метод быстро 1 минута для llama-2-7b 13b и 4 минуты для llama-2-70b на 1-ой a100. vit-b заметно просаживается по качеству при квантовании, но его большие собратья - vit-l, vit-h сохраняют неплохие метрики даже при 2-битной квантизации. размер группы не уточняется, потому непонятно, настоящие это 2 бита, или 3 с лишним вывод на текущий момент данный метод, наверное, лучший среди data-agnostic методов квантизации. тем не менее без информации о входном распределении данных, хорошее качество при высокой степени сжатия модели недостижимо."
237,2024-04-09T00:37:30,"visual autoregressive modeling scalable image generation via next-scale prediction статья код пост на abstract dl введение исследователи со всего мира уже многие годы ищут оптимальную по скорости и качеству парадигму генерации. диффузионные модели нынче самая ходовая тема со своими достоинствами и недостатками, но на ней клином свет не сошелся, и целесобразно рассматривать и альтернативу. некоторые время назад на свет появились авторегрессивные модели dall-e, parti для генерации, мотивированные успехом больших языковых моделей. однако, они не приобрели столь большой популярности, как диффузионки, из-за худшего качества и медленной генерации. генерить токен за токеном долгую последовательность долго и утомительно. прим. вычислительная сложность o n 6 у тех, кто не знает, что можно кэшировать keys и values . и чуваки решили пересмотреть авторегрессионные модели, так чтобы они работали быстро и эффективно . метод суть метода в следующем - изображение кодируют иерархическим автокодировщиком, где каждый уровень представлен в виде некоторой последовательности токенов. в некотором смысле, подобная схема напоминает лапласовскую пирамиду. сначала пытаются приблизить одним токеном насколько возможно исходную картинку, затем увеличивают feature map и уже приближают residual, и так вплоть до максимального разрешения. токены лежат в некотором обучаемом кодбуке и на этапе квантования, токен, полученный некоторой проекцией приближается к ближайшему вектору из кодовой книги. словарь общий для всех уровней иерархии. автокодировщик обучается на взвешенную сумму l2 лосса, адверсариального лосса и lpips. генерация выглядит так - на каждом уровне за раз генерируются h_k x w_k токенов, соответствующих данному уровню, обуславливаясь на все токены с прошлых уровней. таким образом, нужно делать порядка log n проходов вместо n 2, что является безусловным прогрессом. в качестве модели берут обычный декодер-like трансформер без модных финтифлюшек типа swiglu, rope и прочих приблуд, а старый дедовский gpt-2-like c разницей лишь в том, что вместо обычных layernormов используются обусловленные на класс ."
238,2024-04-09T00:39:12,"эксперименты подход валидируют на class-conditional генерации на imagenet-1k в разрешении 256x256 и 512x512. если смотреть на графики fid на figure 1, складывается впечатления, что вообще красота - победа c разгромным счетом на dit-бейзлайнами. однако, взгляд придирчивого читателя обнаружит некие нюансы . в таблице их лучшая модель генерирует изображение 256x256 за 1 секунду, а на графике будто бы за 0.3 сек. если бахнуть stylegan-2 stylegan-xl на этот график, то те будут смотреться парето-оптимальнее их модельки. хотя о чем вообще базар fid - мера оверфита под inceptionv3. по precision recall неплохо, но не лучше всех. на 512x512 тоже все солидно, опережают dit, maskgit. далее показывают, что модель демонстрирует ярко выраженные scaling-laws, хорошую масштабируемость. встает вопрос - а у чего нет scaling laws? визуально качество с размером модели тоже улучшается - малые модели генерируют дефектные изображения, а большие - уже вполне добротные. еще модель может в inpainting , outpainting и class-conditional editing . в ablation показывают следующее 1 var парадигма рулит по сравнению с ar 2 adaln чутка накидывает по сравнению с unconditional ln 3 top-k sampling тоже накидывает 4 cfg-накидывает. хоть тут и не диффузия, но тоже можно определить его подобным образом. 5 увеличение размера тоже накидывает. выводы прикольная и интересная идея, но непонятно, насколько масштабируется на более сложную задачу text-2-image диффузии. есть опасение, что одношаговый алгоритм генерации будет страдать разнообразием по сравнению с современными диффузионными моделями. а может регулирование температуры сэмплирования даст разнообразие в той степени, какой оно нужно для целевого приложения."
239,2024-04-09T06:51:25,папира про yaart на арxиве! контент следующий 1 описание модели 2 описание пайплайна подготовки данных 3 описание сетапа обучения 4 scaling laws 5 исследование влияния качества данных fid не замеряем .
240,2024-04-09T06:56:58,картиночки
241,2024-04-10T12:22:42,"эксперименты и результаты в статьях по современным генеративным моделям, будь то новая парадигма генерации, постановка диффузии или архитектурное новвоведение, натокнули на следующую мысль - а насколько вообще осмысленно понятие sota по class-conditional генерации на imagenet-1k? самый низкий fid - хреновый показатель, по упомянутым неоднократно причинам. интуитивно хочется следующего - чтобы при подаче конкретного класса, будь то рыба , собака или велосипед , генерировалась эта рыба, собака или велосипед без дефектов ну и на каком-то разумном фоне . вопрос эстетичности особо не стоит - ибо imagenet-1k не проходил фильтрации по эстетичности и иным аспектам. а требовать генерировать от модели лучше, чем то, что она видела в данных мы обьективно не можем. что посеешь, то и пожнешь, как говорится. в идеале, хороший классификатор, должен распознавать генерации, как целевой класс, но добиться этого со 100 точностью нельзя из-за ошибок в разметке imagenet и спорности самой разметки. на мой взгляд, нельзя утверждать, что конкретная модель является лучшей по class-conditional генерации на imagenet-1k. можно лишь утверждать, что она решает задачу хорошо или плохо."
242,2024-04-11T00:22:31,"конечно, сказать наверняка нельзя, но сильные результаты моделей command-r command-r пост на love. death. transformers , большая из которых вошла в десятку на lmsys и даже обошла некоторые версии gpt-4, наводят на мысль, что и сама gpt-4 может быть не такой уж огромной и даже одного порядка по размеру с gpt-3 175b параметров . а помните, были слухи, что gpt-4 - это якобы смесь экспертов с 1.8т параметров, инферить которую придется на нескольких хостах? полагаю, что все же основной прирост был за счет большего количества данных, лучшей их фильтрации и предобработки. маленьких секретиков по instruction-tuning, rlhf с достаточным количеством разнообразных инструкций, и обширным подбором ассессоров из разнообразных групп."
243,2024-04-11T11:21:24,"к дискуссии о размере модели и стоимости инференса. command r и command r отличаются в 3 раза по размеру примерно столько же по flops , но по стоимости генерации входных токенов в 6 раз, а выходных в 10. по всей видимости, скейлинг стоимости модели и инференса нелинейный. полагаю, уместно предположить, что openai, как более известный бренд, берет большую наценку, чем cohere4ai. таблица стоимости токенов ниже model m input tokens m output tokens --------------- -------------------- --------------------- gpt-3.5 turbo 0.50 1.50 gpt-4 30.00 60.00 gpt-4 turbo 10.00 30.00 command r 0.50 1.50 command r 3.00 15.00 1 gpt-3.5 turbo - gpt-3.5-turbo-0125 2 gpt-4- gpt-4 не 32к кпд. думайте . подписаться ."
244,2024-04-13T19:23:10,"выложили 1x16 квантованную версию меньшей из command-r. не без просадки в качестве, но зато замерили на чуть более пацанских бенчмарках. почему чекпоинт весит целых 12.7gb, хотя больший по числу параметров микстраль 8x7b 45b параметров занимает 12.6gb? все дело в жирной матрице эмбеддингов, она же lm голова. https huggingface.co ista-daslab c4ai-command-r-v01-aqlm-2bit-1x16"
246,2024-04-14T23:32:34,"exllama by turpoderp exllama exllamav2 локальный инференс больших языковых моделей на пользовательском железе пользуется большим спросом и за последние два года на свет появилось множество движков для локального инференса llmок. и так как llmки нынче большие, приходится пользоваться их квантованной версией. один из самых известных и популярных движков - exllama от turpoderp. метод библиотечка суть standalone реализация на python c cuda llama и некоторых ее производных . 1 exllama-v1 использует vanilla 4-bit gptq для квантования моделей. 2 exllama-v2 в отличие от первой версии позволяет квантовать слои в 2, 3, 4, 5, 6 и 8-бит по отдельности, и иметь разную битность даже в пределах одного слоя. потому можно произвести модель любой битности от 2 до 8. метод создает несколько сжатых версий данного слоя и в итоге выбирается конфигурация, минимизирующая ошибку квантизации на выходе при заданном среднем количестве бит на параметр. пример конфига квантования. целевое железо - rtx серий 30- и 40-. на более старых моделях движок не так эффективен, как утверждает сам творец. на хабе лежит немалое количество моделей в данном формате. поддержка формата добавлена в optimum."
247,2024-04-17T11:13:50,"leave no context behind efficient infinite context transformers with infini-attention статья очередная статья от гугла без кода в попытках найти эффективную альтернативу стандартному механизму внимания в трансфромере человечество перевело не одни джунгли в амазонии, выбросило тучу co2. за последние годы было предложено множество интересных механизмов эффективеого attention, state-space модели, и переосмыслены рекуррентные модели, но все подходы так или иначе уступали в выразительности первородному трансформеру. и группа из google предложила очередную модификацию внимания, способную работать с длинным контекстом с асимптотической линейной сложностью по вычислениям и компактной памятью, не зависящей от длины. метод за основу берут еще старый добрый transformer-xl, который считает внимание в пределах некоторого сегмента фиксированного размера. однако, ограниченность такого подхода в том, что k, v кэши считаются только для последнего сегмента, потому нет возможности учитывать контекст с прошлых окон, и суммарный контекст ограничен размером сегмента на глубину сети. в данной работе предложили привнести рекуррентность в transformer-xl и поддерживают два состояния памяти 1 m - размера ir d_key d_value в числителе 2 z - размера ir dkey в знаменателе и некая комбинация, составленная из этих сущностей и query в текущем сегменте выступает в качестве одного из членов в итоговой формуле attention, которая будет определена чуть ниже. состояние m обновляется после каждого сегмента через некоторую формулу с внешним произведением key, valuе в текущем сегменте. а z - через сумму от ключей, к которым применили функцию активации elu 1 , в данном сегменте т.е z - является по существу скользящей суммой . итоговый контекст получается как взвешенная сумма локального attention в данном окне и полученного выше контекста. относительный вес каждого слагаемого получается из обучаемого скаляра отдельного для каждой головы внимания . эксперименты метод валидируют на бенчмарках по языковому моделированию с длинным контекстом pg-19, arxiv-math . passkey retrieval и суммаризации книг 500k контекста . в первом эксперименте берут трансформер с 12 слоями и hidden_dim 1024. по перплексии метод заметно опережает конкурентные подходы transformer-xl, memorizing transformers , при этом имея значительно меньшее потребление памяти. infini-transformer c контекстом вплоть до 1m токенов. на booksum infini-transformer так же бьет bart и primera, и обе с опцией unlimiformer , будто бы существенно, но не радикально. выводы идея объединить локальное контекстное окно с рекуррентной памятью не выглядит принципиально новой . экспериментальная валидация в статье недостаточна, не хватает очевидного сравнения с теми же state-space моделями. как мне кажется, предложенный подход вряд ли сможет успешно решать с хорошей точность задачи, требующие селективности и способности запоминать несколько фактов одновременно из далекого прошлого, разнесенных по времени с произвольными интервалами между ними."
248,2024-04-18T17:59:17,после сегодняшнего релиза меты
249,2024-04-18T19:23:01,"релиз 3 таки не оказался 18-апрельской шуткой. выпустили ровно спустя 8 месяцев после 2-ой версии. что известно на данный момент. обучение 1 15 t токенов на обучении в 7 раз больше, чем у llama-2 2 8к контекстное окно 3 95 обучающей выборки на английском, и остальные 5 - на других 30 языцех 4 instruction-finetuning включает sft, dpo, ppo модель 1 архитектура не поменялась не moe 2 8b - тоже gqa 3 размер токенизатора увеличили до 128к метрики 1 8b модель бьет модели аналогичного размера mistral, gemma на бенчах 2 70b модель бьет gemini-pro-1, 1.5, mixtral 8x22b и claude 3 sonnet в ходе разработки собрали свой датасет из 1800 разнообразны инструкций на котором замерялись. что еще обещают 1 400b модель, которая еще учится. предьявили метрики на чекпоинте от 15 апреля. 2 будет техрепорт. 3 накатят еще более длинный контекст. блог коллекция на хабе"
251,2024-04-21T08:37:47,"command-r aqlm квантизация 1x16 добежала спустя 2 недели и 50 перезапусков, сбоев инфраструктуры, и прочих шалостей полтергейста. ценность релиза еще дня 3-4 назад была бы на порядок больше, но надо же было довести до ума давно обещанное. история, к сожалению, не про локальный инференс на consumer-grade gpu, ибо даже сжатая модель весит 31.9 gb. целевая аудитория - обладатели gpu уровня от a40, a6000. https huggingface.co ista-daslab c4ai-command-r-plus-aqlm-2bit-1x16"
252,2024-04-24T05:51:13,"how good are low-bit quantized llama3 models? an empirical study статья код спасибо https t.me senior_augur за наводку недели не прошло как llama-3 появилась на свет , как группа исследователей из китая с присущей только азиатам скоростью опубликовала исследования про то, как разные методы квантования и дообучения квантованных моделей просаживают качество. эксперименты авторы рассматривают 8 более-менее современных методов ptq post-training квантования rtn, gptq, awq, quip, pb-llm, db-llm, billm, smoothquant и два метода дообучения квантованных моделей - qlora, ir-qlora. качество замеряют на стандартных бенчах по перплексии и 0-shot на lm-eval-harness 5-shot mmlu. в первом случае, рассматривают weight only квантование в 1 , 2, 3, 4, 8 бит для всех методов окромя smoothquant и w4a4, w6a6, w8a8 квантование и весов, и активаций для smoothquant. для калибровки используют 128 последовательностей из wikitext2-train длины 2к мало! . заметная просадка наблюдается уже в 4 битах, а при приближении к 2 битам большинство методов полностью ломают модель или поднимают значение перплексии до 2-значных значений против 1-значных. замеряют на 2к контекстном окне, хотя было бы логичнее использовать 8к - длине контекста на обучении. для дообучения с qloraми используют инструкции из alpaca. lora адаптеры применяют поверх квантования из bitsandbytes rtn в normalfloat-4 . что любопытно, все файтьюны портят качество по сравнению с просто квантованной моделью. варианта здесь два - либо инструкции из alpaca слишком плохи, либо авторы криво завели дообучение. учитывая, что можно было запустить дообучение с lr 0, как минимум реально не просадить качество выводы авторы явно предпочли скорость публикации ее качеству и полноте. методы векторной квантизации quip , aqlm никак не отражены в работе. справедливости ради стоит заметить, что они требуют значительно больших вычислительных затрат и времени по сравнению с рассмотренными выше. тем не менее основное заключение статьи про то, что качество llama-3 сложнее сохранить при сжатии модели по сравнению с ее предшественниками и иными семействами открытых моделей, похоже, действительно имеет место. полагаю, что это логично, ибо веса модели, обученной на 15т токенов в каком-то смысле должны быть более насыщенными информацией"
253,2024-04-25T11:54:41,"-вы llm-ками пользуетесь? -нет, только квантуем. -красивое."
254,2024-04-30T18:35:47,-почему интеллектуальные люди часто близорукие? -они повышают frames per second за счет прорисовки текстур.
255,2024-05-02T06:07:13,aqlm приняли на icml!
256,2024-05-02T08:41:09,"kan kolmogorov-arnold networks статья код введение в основе всех ну почти всех современных архитектур лежит многослойный перцептрон mlp с обучаемыми матрицами, сдвигами и фиксированными активациями и некоторым механизмом агрегации для пространственных входов свертки, attention, state-spaces, мамба, хуямба . теория гласит, что при некоторых предположениях на целевую функцию и функции активации в сети достаточно большой сетью можно приблизить эту самую целевую функцию. возникает вопрос - оптимален ли такой подход по вычислениям точности и нельзя ли изобрести нечто лучшее? метод в данной статье авторы переосмысляют и в некотором смысле обобщают парадигму построения многослойной сети. в основе идеи лежит знаменитая теорема колмогорова-арнольда, что непрерывную многомерную функцию на ограниченной области можно всегда представить в виде композиции функций от одной переменной. однако, при этом теорема не дает явного вида этих функций, которые могут оказаться сколько угодно плохими, потому не реализуема на практике. в данной статье предлагают выучивать сами функции активации, параметризуя их некоторым образом. каждое ребро между входным и выходным нейроном задается некоторой параметрической функцией довольно общего вида. традиционный mlp является одним из частных случаев предлагаемой парадигмы. в оригинальной теореме перцептрон всего с одним скрытым слоем, но ничто не мешает технически настакать их побольше. на практике kan-слой реализуется как b-сплайн с residual connections, домноженный на константу phi x w b x spline x , где b x silu x x 1 e -x оптимизация такого сплайна довольно нетрививальна, и для улучшения сходимости сплайн инициализирует так, чтобы быть близким к нулю в начальный момент времени, и сетка с узлами сплайна обновляется на лету. при той же глубине и ширине в kan-сети больше параметров, чем в классической mlp в g g - размер сетки раз, но мотивация работы в том, что kan требуется меньшая ширина для достижения сопоставимого качества. далее авторы обосновывают, что kan обладает значительно лучшей масштабируемостью в сравнении c mlp и обходит проклятие размерности за счет того, что представляет многомерную функцию в виде композиции одномерных, тем самым переводя задачу эффективно в низкоразмерное пространство и выводят степенной закон убывания функции потерь. для kan в однослойной сети, аппроксимирующие функции могут быть очень плохими, но с ростом глубины, существуют все более гладкие комбинации, способные решать целевую задачу. по ходу дела, для повышения выразительности сети можно добавлять дополнительные узлы в сплайн."
257,2024-05-02T08:42:08,"эксперименты предлагаемую парадигму валидируют на аппроксимации явно заданных математических функций и еще в ряде физических приложений - решении уравнений в частных производных, исследовании андерсоновской локализации в разных решеточных системах. при том же числе параметров, kan сходятся значительно быстрее и достигают лучшего качества по сравнению с традиционными архитектурами. проблема переобучения , тем не менее, все равно может иметь место. кроме того, для повышения интерпретируемости, авторы предлагают накладывать l1 энтропийную регуляризацию для удаления побочных связей в сети, и полученная спарсифицированная сеть на игрушечных примерах действительно вполне интуитивна. вывод работа весьма занятная и интересная. однако, преждевременно утверждать, что предложенный подход тут же возьмет и вытеснит mlp. во-первых, требуется валидация на реальных данных. удивительно, что авторы не показывают эксперименты на том же mnist или табличных данных, которые должны быть по зубам даже исследователям с ограниченными ресурсами. возможно, kan просто имеют хороший inductive bias для решения определенного круга задач. кроме того, текущая парадигма построения сетей хороша тем, что удачно ложится на возможности современных gpu, способных параллелизовать эффективно операции с большими тензорами. kan же не настолько hardware-friendly в этом отношении. но могут найти свою нишу при инференсе на cpu и fpga. для приложений математики и физики, kan, безусловно, представляют интерес, но можно ли ими заменить mlp в трансформере? поживем, увидим ."
258,2024-05-03T23:31:18,абитуриентов в принципе можно евалить как llmки. вместо егэ давать mmlu-ru. а на вступительных давать проверяющему которого можно заменить gpt4 или зелибобой ответы двух абитуриентов без указания их имен и затем отсекать по elo score.
259,2024-05-04T08:34:37,"1x16 aqlm квантизации -3 на хабе! 1 meta-llama-3-8b-aqlm-2bit-1x16 2 meta-llama-3-8b-instruct-aqlm-2bit-1x16 3 meta-llama-3-70b-aqlm-2bit-1x16 4 meta-llama-3-70b-instruct-aqlm-2bit-1x16 дело заняло несколько дольше времени , чем предполагалось. новую линейку llmок от меты оказалось сложнее квантовать по сравнению с предшественниками с приемлемой просадкой в качестве, а выкладывать шлак, под красивой этикеткой не позволял кодекс чести самурая. пришлось улучшить процедуру файнтьюна - больше токенов, больше компьюта. но в итоге добили до приемлемого качества. пользуйтесь, делитесь впечатлениями 8b версия великовата 4gb из-за больших эмбедов и lm_head так как словарь большой . в будущем планируем попробовать посжимать и эмбеды языковую голову."
260,2024-05-13T23:40:59,"полагаю, многие уже успели восхититься выдающимися возможностями нового детища от openai - gpt4o. детальный разбор и сводка имеющейся информации приведена в постах сиолошной, так что дабы сэкономить на генерации собственных токенов я не буду повторяться. на мой взгляд, самое удивительное во всем этом, что новая модель, не только стала лучше, обрела новые возможности и кучу мелких и крупных плюшек, но и стала дешевле в использовании. безумно любопытно, в чем же секрет столь эффективного и быстрого инференса при сохранении высочайшего качества? подозреваю, что там весьма нетривиальное сочетание алгоритмов сжатия и ускорения, разных оптимизаций движка. вероятно, и архитектурные навороты - смесь несмесь экспертов может в ширину, а может и в глубину . а может просто большой трансформер без причуд? и возможнось бесплатного пользования моделью хоть и в ограниченном количестве - очень неожиданный ход от openai, которые хоть немножечко, но приоткрылись но думаю, что все продумано, не разорятся на этом жесте доброй воли ..."
261,2024-05-13T23:50:54,"судя по количеству создателей, имеет место scaling law не только по размеру модели и обьему данных на обучении, но и числу рисерчеров при обучении llmок."
264,2024-05-22T22:45:30,с приближением дедлайна neurips частота сообщений про новый revision стремительно нарастает...
265,2024-05-24T11:25:28,"квантование kv-кэшей добавили в ! блогпост чтобы не пересчитывать каждый раз все заново при авторегрессивной генерации, в трансформерах используются kv-кэши, ключи и значения, посчитанные для прошлых токенов, сохраняются в памяти и используются при подсчете attention для каждого нового токена. однако есть проблема - если последовательность длинная, кэш слишком тяжелый становится. для 1 или 2 7b для 10к токенов потребуется 5gb памяти 2 2 32 32 128 10000 5gb . а для более крупных моделей и , если контекст переваливает за миллион, страшно представить сколько. существуют разные подходы к сжатию kv-кэшей, и один из наиболее очевидных - квантизация кэша, преставление ключей и значений в более низкой битности. метод в работе kivi предложили 2-битное ассиметричное с zero point квантование кэшей. ключи квантуются поканально, значения - по токенно. поканальное квантование помогает бороться с outliers. в hf использует по-токенное квантование для ключей и значений из соображений скорости, несколько жертвуя качеством. кроме того, чтобы немного улучшить качество, самые свежие 128 токенов держат в исходной точности. они обычно самые важные для текущего токена, и не добавляют много памяти для длинного контекста. hf поддерживает два метода квантизации - quanto простой rount-to-nearest - hqq оба метода data-free. валидация на замерах перплексии на llama-2-7b на pg-19, 4-битный hqq не просаживает качество, а quanto_int4 умеренно просаживает. 2-битные квантизации сильно роняют качество. на longbench в 4 битах почти нет просадки по качеству. на длинных последовательностях удается достичь до 2.5x экономии потребляемой памяти. скорость, генерации, однако замедляется так как на текущий момент нет оптимизированных ядер. просадка по скорости 30-50 судя по графикам крайняя правая точка на графике number_of_tokens sec явно забагованная . кроме того, данная стратегия помогает именно на стадии генерации, а заполнение промпта pre-fill приходится делать стандартным способом. метод совместим с flash attention. использование достаточно установить quanto, и прописать аргументы в методе .generate."
266,2024-06-01T00:18:31,"terdit ternary diffusion models with transformers статья код инференса llmки не квантовал за последние год-два только ленивый, потому назрело время осваивать и другие ниши. квантование диффузионных моделей на текущий момент пока не столь исследовано, как llmки, в связи с тем, что сами модели не доросли до своих собратьев из nlp, потому и не было столь острой необходимости. тем не менее, прогресс не стоит на месте, и стоит быть готовым к дальнейшему масштабированию диффузионных тушек. в рассматриваемой статье авторы перенесли метод тернарной квантизации quantization-aware-training qat из bitnet1.58 на ditы для class-conditional генерации на imagenet. квантуют только веса активации остаются в исходной точности . метод по существу ничего нового по сравнению с bitnet1.58, веса обучаются через straight-through estimator ste с большим learning rate. единственное нововведение - нормализация на выходе adalayernorm. авторы обнаружили, что тернарные веса выдают большие активации, и scale shift gate модуляции слишком велики, чтобы сетка могла нормально сходиться. навешивание rmsnorm на конец mlp для получения модуляций решает проблему. эксперименты метод валидируют на ditах двух размеров - с 600m параметров примерно как dit-xl из оригинальной статьи и 4.2b параметров - на class-conditional генерацию на imagenet 256x256. по метрикам, тернарная 4.2b модель примерно равна dit-xl, 600m несколько хуже. то есть для большой модели близкое качество к floating point модели при чуть меньшем общем размере модели параметров больше в 7 раз, бит на параметр 10 меньше, чем в fp16 . справедливости ради, стоит заметить что terdit обучался меньшее число итераций по сравнению с моделью из статьи фейсбука. с инференсом немного грустненько получилось. для работы с тернарными весами берут кернелы из hqq и деквантизуют на ходу. квантованные модели медленнее fp32 на 20-25 , а при опущенном сравнении с fp16 замедление было бы порядка 3 раз. зато неплохая экономия по памяти. 4.2b моделька есть 3gb видеопамяти на пике при инференсе. в приложении еще зачем-то показывают что существующие 4-битных квантизации ломают полностью dit. берут, правда smoothquant, который в отсутствие квантования активаций, вырождается в round-to-nearest rtn , т.е самый наивный и грубый метод, при существовании куда более сильных ptq методов для диффузии q-diffusion, ptq4dm . вывод с одной стороны, очередное подтверждение того, что тернарный qat как-то да работает. однако результат куда скромнее того, что получили для llm майкрософты, и с таким замедлением инференса вряд ли интересен практикам. неизвестно, масштабируется ли он на случай более сложной задачи text-2-image генерации. тем не менее деятельности представляет определенный интерес, и развитием эффективных алгоритмов qat, вероятно, тернарные модели вполне могут быть около парето-оптимальными. во всяком случае, в некоторых приложениях."
267,2024-06-03T07:02:12,"не проплаченной рекламы пост. хочу порекомендовать канал то шо нейросети. на данном канале вы можете найти сборную солянку интересных и полезных материалов во всякой всячине, связанной с нейронками. и более чем годные туториалы от самого автора. в частности, внимания заслуживает недавняя ветка постов про liquid neural networks . и реализация мамбы с нуля."
268,2024-06-08T00:38:11,"bitsfusion 1.99 bits weight quantization of diffusion model статья ридми yet another свежая статья про экстремальную квантизацию диффузионок от snap research. утверждают, что 2-битная модель даже лучше исходной. если у вас завалялся старый игровой пк, года так 2007, можно его не выбрасывать, а генерить картинки стейбл диффузией . метод по существу предложенный метод представляет собой qat квантизейшн эвер трэйнинг с mixed-precision квантизацией и дистилляцией. каждый слой квантуется в 1 , 2 , 3 бита, или не квантуется вовсе. в прошлых работах было показано, что диффузионные модели очень чувствительны к сжатию timestep проекций, обуславливающих на текущий шаг расшумления. поэтому следуя стандартной практике их не сжимают. авторы анализируют чувствительность разных слоев к сжатию, замеряя mse с картинкой сгенерированной оригинальной моделью и clip score, при квантовании слоев по отдельности и замечают, что данные две метрики не всегда скоррелированны. в частности, сжатие слоев в cross attention слоях не сильно так сильно влияет на mse, но при этом временами ломает семантику изображения. shortcut свертки очень важны. каждому слою сопоставляется важность в зависимости от mse и числа параметров в нем, и подбирается порог такой, что достигается целевая степень сжатия. min-max стратегия по определению масштабов квантования не учитывает наличие выбросов в распределении весов, поэтому авторы применяют lloyd-max итеративный алгоритм для минимизации ошибки. кроме того, важно учитывать симметрию весов относительно нуля и явно накладывать ее. далее авторы дообучают квантованную модель. на первой стадии сжатая модель пытается воспроизвести выход и промежуточные активации учителя. авторы отмечают, что важно учитывать classifier-free guidance, используемый при инференсе. распределение шагов сэмплирования смещено в область где ошибка квантизации больше поздние шаги диффузии . на второй стадии модель учится на noise prediction, как самая обычная диффузионка. эксперименты берут sd v1.5 модель и учат 20к шагов на первой стадии, и 50к на второй на некотором проприетарном датасете. замеряют clip score и fid на ms-coco, tifa, geneval и пользовательские предпочтения на partiprompts. сжатая модель после второй стадии по метрикам примерно равна несжатой. на sbs side-by-side сравнении на parti prompts bitsfusion модель побеждает sdv1.5 с win rate 54.4 против 45.6 . странно, что sbs, который самый показательный, учитывая несовершенство текущих генеративных метрик, скромно запрятан в приложение. в ablation показывают, что более-менее все компоненты метода важны - смешанная точность, дистилляция признаков, сэмплирование шагов и двустадийное обучение. вывод довольно хороший инженерный результат, использующий сочетание разных идей. разве, что без специализиованного железа вряд ли удастя выжать ускорение. однако, вызывает , почему была выбрана sd v1.5, хотя статья свежая, и уже почти как год существует sdxl. можно ли их так же легко сжать? полагаю, что хорошее качество еще во многом обусловлено тем фактом, что загадочный проприетарный датасет неплохо отфильтрованных и дообучение несжатой модели могло бы ее тоже улучшить, ибо sd v1.5 училась на довольно шумных данных из laion."
269,2024-06-12T15:58:50,оценивать интеллект по iq-тестам это как оценивать качество изображений по fid.
270,2024-06-12T19:37:07,"про релиз sd3 medium. добрые люди кровопролитиев от него ждали, а он чижика съел! обсуждение на reddit по всей видимости, выложили pretrain чекпоинт до всяких rl-файнтьюнов."
272,2024-06-12T22:03:14,"в процессе случайного блуждания по интернетам наткнулся на занятный блог от некоего итона эпперли, аспиранта в калтехе по специальности прикладная и вычислительная математика. в частности, у него есть весьма годные посты по разным вопросам из линейной алгебры разреженным и низкоранговым матрицам , теории вероятностей марковские цепи , случайные алгоритмы в линейной алгебре . есть концептуальные посты - low-rank - fft и более специфичные - don t use gaussians in stochastic trace estimation итон - первый автор статьи xtrace, sota алгоритма стохастической оценки следов больших матрицы сходящихся быстрее к истинному значению по количеству сэмплов . в общем, любителям прикладной математики рекомендую!"
273,2024-06-14T23:26:43,"блог nvidia пост на addmeto пока все находятся в томном ожидании релиза -3 400b nvidia сделали ход конем и выкатили семейство здоровенных херовин. знакомьтесь - nemotron-4! идет в трех комплектациях 1 base - претрейн 2 instruct - дообучение на инструкциях 3 reward - reward model для rl alignment, обученная поверх base предобучалось оно на 9 триллионах токенах из 50 человеческих языков и 40 языков программирования. для alignment использовали набор из 20к инструкций, который выложили в открытый доступ. контекст коротковат по современным меркам - всего 4к токенов, небось больше не лезло во время обучения . скоры на бенчах весьма достойны. на свежей arenahard от lmsys уступают лишь свежим версиям чат-гопоты и клод опус. на lmsys arena было бы еще интересно глянуть elo score. ну и самая интересная деталь - размер 340b параметров, т.е 640gb на одни лишь веса в fp16. как вы ее будете инферить - это ваша проблема задача экстремальной квантизации уже не вместить llmку именно на consumer-grade gpu, а хоть на какую-то но зеленым, конечно, спасибо за такой вклад в опенсорс"
274,2024-06-17T11:51:33,"бог создал слабые и сильные методы сжатия, но файтьюн уравнял их."
275,2024-06-17T23:56:50,"прунинг llm в глубину данный пост был в замыслах уже пару месяцев, но все никак не доходили руки. ниже приведен разбор нескольких статей, посвященных одной и той же теме - depth pruning. так как задача и мотивация у всех статей общая, то и введение дедуплицированное, общее для всех работ. введение при структурированном прунинге веса отбрасываются не поодиночке, а группами - поканально, поголовно трансформерно поголовно , иногда даже весь слой целиком отправляется в царство аида . структурированный прунинг хорош тем, что явно уменьшает размерности матриц и тензоров в операциях, потому дает неплохое ускорение на разнообразном железе и движках. увы, серьезного сжатия без серьезной просадки в качестве обычно сложно достичь без серьезных вложений в дообучение. трансформеры содержат skip-connection как в attention, так и в mlp. residual type архитектуры, как было замечено еще давно , являются в некотором смысле ансамблями, где конечный выход можно воспринимать как агрегацию знания от нескольких неглубоких экспертов. одна голова хорошо, две лучше, еще больше - еще лучше, но и иногда и пары специалистов достаточно. потому есть основания полагать, что прунинг в глубину - прореживание целых блоков - имеет шанс завестись и сжатая модель будет выдавать адекватную точность."
276,2024-06-17T23:59:24,"shortened llama a simple depth pruning for large language models статья код первая статья в хронологическом порядке и с открытым исходным кодом. метод хотим мы, значится, выбрасывать блоки целиком - но какие брать? авторы рассматривают 3 критерия 1 среднюю величину весов magntude pruning 2 taylor не свифт, а норму градиента на вес, вдв кароч 3 изменение перплексии при выкидывании каждого блока по отдельности в первых двух случаях временами выпиливаются первые и последние блоки, которые оказываются позарез важными, поэтому предлагается убрать их рассмотрения. чтобы восстановить качество модели после сжатия, навешивают lora на то, что выжило и дообучают на небольшом количестве данных. эксперименты берут llama-1 и vicuna-7b, 13b. для калибровки оценки важности блоков берут 10 последовательностей длины 128 из bookcorpus, и потом дообучают на alpaca. предложенный метод не хуже, а то и лучше llm-pruner и структурированный wanda где критерием важности параметра выступает норма веса на норму активации . просадка при 20 прореживания заметная, но все же довольно умеренная. на малых степенях сжатия лучше всего работает критерий по градиенту умножить на вес, при запрете на отбрасывание первых четырех и последних двух блоков, при больших - оценка важности по перплексии. файнтьюн с lora неплохо восстанавливает метрики после сжатия, особенно с увеличением степени сжатия. тем не менее разрыв между исходной моделью и сжатой все еще существенен просадки больше, чем при sota 2-битной квантизации . данных требуется совсем немного для калибровки, качество существенно не меняется если взять больше, чем 10 примеров. запрещать сжимать для taylor и magnitude первые и последние блоки важно - иначе перплексия взлетает до нескольких тысяч. комментарий метод прост и логичен. однако просадка, на самом деле несколько больше, чем декларируется. на собственных замерах с lm_eval 0.4.0 поверх дообученных чекпоинтов 26 блоков против исходных 32 замерил просадку в 3 по сравнению с исходной моделью."
277,2024-06-18T00:02:39,"shortgpt layers in large language models are more redundant than you expect статья нет кода метод трансформерные блоки, как известно обладают аддитивными skip connections следующего вида для простоты изложения забьем на нормализацию x x attn x x x mlp x выход mlp, attn нередко инициализируется нулем и остается небольшим по норме в процессе обучения. следовательно, каждый блок меняет понемножку внутренне представление. в данной работе предлагают выбрасывать те блоки, которые изменяют меньше всего внутреннее представление на некоторой выборке примерах из pg19 . в качестве расстояния используют среднее косинусное расстояние между токенами перед и после данного блока. в итоге убираются блоки с наименьшим косинусным расстоянием. результаты так как работа от китайских коллег, то замеряют качество как на llama-2, так и на baichuan. для оценки качества используют стандарнтые 0-шоты, mmlu и cmmlu. предложенная метрика выбирает блоки ближе к концу сети но не самые последние . результаты бенчмарков вызывают вопросы, утверждается, что качество на mmlu почти не просаживается вплоть до 28 sparsity, при этом перплексия на wikitext2 возрастает довольно заметно. при таких значениях обычно сеть с трудом два слова связать может, не то что решать задачи на логику . lm-eval-harness нигде не цитируется. по всей видимости используют какой-то иной или самописный фреймворк. безлайны - методы структурированного прунинга в ширину, естественно, бьют. с shortenedllama, вышедшей чуть раньше, не сравниваются."
278,2024-06-18T00:04:18,"the unreasonable ineffectiveness of the deeper layers статья нет кода метод идея по существу та же, что и выше, но выбрасывают блоки не по отдельности, а пачкой - т.е находим группу последовательных трансформерных блоков, которые минимально меняют представление. критерий тот же самый - косинусная близость. как и в shortenedllama для восстановления качества сжатой модели дообучают с lora. и дообучение называется лечением healing . результаты замеряют качество языкового моделирование вместо перплексии смотрят на изменение кросс-энтропии и mmlu boolq. рассматривают llama-2, mistral, phi-2. кросс энтропия просаживается, но неплохо лечится после файнтьюна. результаты на mmlu вызывают большие сомнения - спад качества имеет скачкообразный характер. по личным наблюдениям спад mmlu довольно монотонный, постепенный и коррелировал с ростом перплексии. утверждается, что некоторые модели можно сжать до 40 без просадки на этом бенчмарке. далее смотрят на корреляцию признаков в разных блоках. наибольшая корреляция наименьшее расстояние в блоках ближе к концу, согласуясь с наблюдениями из двух прошлых работ. отсюда предлагают простое правило - пруньте блоки с конца, исключая последний, и типа хорошо работает."
279,2024-06-18T00:07:46,"your transformer is secretly linear статья пост в телеге пост на хабре код интересное релевантное исследование от коллег из airi сколтеха. кратко, ибо все хорошо и доступно изложено самим автором в постах в телеге и на хабре. метод оказывается, что активации трансформера в соседних блоках связаны почти что линейным преобразованием. в качестве меры линейности используется linearity score, который по существу является r2 коэффициентом детерминации . сам по себе факт не столь удивителен, ибо норма skip-connection обыкновенно значительно больше преобразования в attn mlp. но даже если вычесть skip connection выделяется явная линейная и нелинейная компонента. проанализировав ряд промежуточных чекпоинтов открытых моделей, авторы замечают что норма линейной компоненты уменьшается в процессе предобучения, но возрастает в ходе файнтьюна . и предложенная регуляризация на нелинейность улучшает качество. исходя из наблюдений, предлагается прунить самые линейные слои, а для восстановления качества заместо запруненного блока учится линейный слой. и такой адаптер неплохо восстанавливает качество. эксперименты рассматривают большое число открытых моделей - от gpt-2 когда-то openai был действительно открытым , bloom до свежих phi-3. рост линейности имеет место для всех моделей и задач. прирост качества от регуляризации хоть и невелик, но все же заметен. интересно, насколько он будет иметь место на большом масштабе? несколько нетипичный сетап замера перплексии - короткие последовательности вместо обычно используемого 2к 4к контекста как на обучении , но общая суть от сего не меняется."
280,2024-06-18T00:09:12,"вывод трансформеры, вероятно, являясь в каком-то смысле ансамблем, устойчивы, как печень, к ампутации крупных кусков. однако просадки там где результатам можно доверять - первая и последняя работа , все же значительны при степени сжатия более 20 . как ни крути, scaling laws существуют не просто так, и представления, выученные разными блоками, хоть имеют и разную полезность, но все же содержат в себе релевантное знание, хоть для какого-то входа. отсюда и успех контекстуальных методов сжатия, вроде deja vu и powerinfer."
281,2024-06-23T22:11:24,"qtip quantization with trellises and incoherence processing статья кода нет, но обещают, что будетт было очевидно, что после выхода обновленного aqlm и pv-tuning, что ответочка от конкурентов из cornell university - это лишь вопрос времени. тем более, что в issue в peft первый автор quip дал ясно понять, что новый, более совершенный, метод квантизации скоро выйдет на свет. и тихо, незаметно, так что сам я обнаружил лишь по чистой случайности, на архиве появилась статья и классическая репа с ридмишкой. метод векторная квантизация при той же битности точнее если реализована не криворуко , чем скалярная. раньше я думал, что данное явление обусловлено некоторыми корреляциями между измерениями, но на самом деле векторная квантизация работает точнее даже для последовательностей i.i.d хорошее обьяснение здесь . чем больше группа - тем более высокой точности при заданном числе бит можно добиться потенциально. однако, большие кодбуки становятся довольно обьемными и вносят нетривиальный вклад в общую битность в aqlm 1x16 кодбуки накидывают 0.3 дополнительных бит на параметр и не влезают в кэш gpu. в идеале бы хотелось иметь компактный кодбук и большие группы. и авторы qtip названного в честь рэпера? прибегают к красивой идее из теории кодирования - trellis coding. обычно при кодировке последовательностей в l бит, каждый элемент может принимать любое из 2 l значений, и последовательность длины кодируется tl битами. если представить последовательность в виде блуждания по графу, то граф полносвязный - из любой вершины можно прийти в любую другую. в trellis coding названном в честь архитектурной решетки , каждый узел графа соответствующий одному из 2 l значений соединен только с 2 k где k l ребрами с другими вершинами. последовательность кодируется как индекс начального узла и индексы ребер при блуждании по графу. итого расход памяти l t-1 k, что может быть заметно меньше tl. однако, наивная реализация будет чрезмерно тяжеловесной для произвольного trellis при достаточно большом l и k, так как где-то надо хранить граф связей. кроме того, декодирование для произвольного trellis - много последовательных операций, что непозволительно медленно на практике, и qtip предлагает несколько хитрых идей, чтобы это работало эффективно. как и в quip quip квантуется не исходный вес, а обработанный случайным ортогональным преобразованием адамаровыми матрицами , так что элементы матриц близки к i.i.d нормальному распределению. далее, чтобы не хранить полный произвольный граф, авторы рассматривают специальный анзатц - bit-shift trellis, где связаны только вершины с индексами, отличающимися на битовый сдвиг. однако, такой выбор слишком ограничителен, и авторы предлагают перемешивать связи некоторой хитрой случайной перестановкой и предлагают три метода 1 1mad 2 3inst 3 hyb в первых двух случаях в качестве индексов в случайной перестановке выступает некоторое фиксированное преобразование использующее линейный конгруэнтный генератор случайных чисел и побитовые операции. а в третьем случае - некоторое обучаемое преобразование. для определения оптимальной конфигурации минимизируется ошибка на выходе слоя с помощью алгоритма витерби динамическое программирование . полученный метод выдает mse ошибку близкую к минимально теоретически возможной."
282,2024-06-23T22:14:09,"эксперименты метод валидируют на -2, 3, и в качестве бейзлайнов выступает quip и aqlm первой версии. с pv-tuning не сравниваются, шельмецы , хоть и лежит он на архиве 3 недели с их публикации квантуют группами 16x16 число весов по входной и выходной размерности с l 16, и q 9 гиперпараметр в обучаемом гибридном коде . разные битности отличаются числом ребер k из каждой вершины. метод демонстрирует выдающееся качество даже без какого-то дообучения, в 4 бита почти без просадки, а в 2 бита почти на уровне aqlm и quip c end-to-end finetuning жулики в ft репортят цифры aqlm только с блочным файтьюном . с полноценным файнтьюном дообучение знаков в адамаровых матрицах и параметров hyb преобразования метод вырывается далеко вперед при низких битностях от quip aqlm. неупомянутый pv-tuning работает несколько лучше на меньших моделях - 7 8b , 13b, и сравнивается на 70b используя, правда, больше данных и вычислений . на -3 просадки больше, данное семейство моделей, согласно наблюдениям извне и личному опыту, тяжелее квантизуются. скорость инференса замеряют на rtx4090 на уровне quip . сравнение с aqlm полная шляпа , чуваки, небось коряво поставили либу для инференса, либо что еще наворотили, ибо tokens s в нашей статье на более медленной rtx3090 куда больше. выводы несмотря на корявое сравнение с aqlm, в общем и целом, конкуренты проделали отличную работу. здорово, когда красивые идеи из математики находят применение на практике. кажется, что стоит реально засесть за ботанье матана и чтение статей из 80-х"
283,2024-06-27T00:32:56,"accelerating neural network training with semi-structured 2 4 sparsity блог торч недавно выкатил блог, где показывает, что 2 4 sparsity паттерн может быть полезен не только для инференса, но и на стадии обучения модели. напомню, что 2 4 sparsity - это когда из 4 подряд идущих весов 2-нулевые. начиная с ampere нвидиевские gpu имеют поддержку такого формата. веса хранятся в виде сжатого вдвое тензора и еще есть тензор индексов элементов. ранее было показано, что такой паттерн выдает ускорение 10-30 по сравнению с fp16 на инференсе, но чтобы добиться эффективного обучения необходимо учесть еще ряд нюансов. обучение с фиксированной маской работает не очень, и нужно ее обновлять по ходу дела чтобы не просесть по качеству. наивный kernel слишком медленный и по порядку величины близок к времени forward пасса. и заслуга авторов в разработке эффективной процедуры прореживания. 1 на обратном проходе при подсчете градиента участвует не w, w.t, которая вообще говоря не 2 4. авторы ограничивают паттерн прореживания так, что 2 4 паттерн имеет место как для w, так и для w.t прореживая блоки 4x4 . еще есть нюанс в том, что на уровне железа реализовано умножение sparse матриц на dense, но не наоборот. потому предлагается считать транспонированное умножение, а затем его транспонировать. в cusparselt есть опция выбирать индексирование в памяти как по колонкам, так и по столбцам, что позволяет иметь непрерывное расположение элементов в обоих случаях. 2 потоки cuda считывают за раз по 128 байт, и чтобы отпимизировать операции чтения записи каждый поток берет на себя по 4 блока 4x4 4x4x4x2 fp16 128 байт . 3 if else операции, когда разные потоки в группе warp попадают в разные условные ветви - работают сильно неэффективно на gpu. используется sorting network для определения важных неважных элементов без условных операторов. 4 сжатые матрицы и метаданные индексы ненулевых элементов хранятся в column-major для оптимизации операций записи. эксперименты обучают dinov2 vit-l c разными пропорциями sparse dense обучения. сначала идет sparse обучение, а затем dense. для оценки качества смотрят на точность лог-регрессии на imagenet-1k поверх признаков обученной модели. обучение в sparse режиме на протяжении 40-70 обучения не просаживается по качеству по сути по сравнению с dense обучением. всегда sparse - теряет полпроцента качества. ускорение времени обучения при 70 sparse обучения порядка 6 на а100 . вывод 2 4 паттерн не дает какого-то впечатляющего ускорения, но может слегка повысить эффективность. интересно , насколько полученные результаты масштабируются на llmки с миллиардами параметров. будет ли выигрыш от 2 4 компенсироваться более медленной сходимостью?"
284,2024-06-27T20:06:19,"real-time video generation with pyramid attention broadcast cтатьи нет код есть в ряде отечественных тг каналов пост на эйай ньюз, пост на machine learning упомянули проект real-time video generation with pyramid attention broadcast. самой папиры еще нет и потому какие-то нюансы могут быть неизвестны. от себя добавлю несколько деталей. суть подхода заключается в следующем. в видео диффузии есть 3 вида attention операций 1 пространственное 2 временное 3 перекрестное внимание на condition ранее в cache me if you can было замечено, что карты attention между соседними шагами диффузии мало меняются между соседними шагами на большей части процесса за исключением начала и конца при картиночной генерации. в данном проекте авторы замечают, что для скорости изменения attention карт справедливо следующее неравенство v_cross v_time v_spatial и соотвественно, чем медленее меняется attention, тем чаще он переиспользуется. за счет переиспользования карт attention можно параллелить эффективно между разными gpu разные шаги генерации по времени уменьшении оверхеда на 50 без переиспользования карт . переиспользование карт дает ускорение 30 . а распаралелливание на 8 gpu дает почти линейное ускорение 8.4x-10.6x по сравнению с генерацией на одной gpu наивным способом. то есть ускорение достигается в первую очередь за счет эффективного параллелизма. тем не менее, достойный инженерный результат."
285,2024-06-29T11:31:16,"sparse-marlin код некоторое время назад я затрагивал marlin - быстрый кернел для батчового инференса int4xfp16. пару месяцев назад коллеги из ist выпустили sparsemarlin - где дополнительно к квантизации весов добавляется 2 4 sparsity, за счет чего достигается еще большее ускорение по сравнению с fp16. как я понял, основные инженерные наработки следующие 1 эффективная обработка 2 4 метаданных - позиций нулевых и ненулевых весов 2 использование sparse tensor cores sptcs 3 умное расположение квантованных весов, метаданных для sparsity, и статистик квантования итоговое ускорение до 5.3x на rtx3090 у marlin 4x при перемножении больших матриц. интересно , какое end-2-end ускорение может быть достигнуто на эффективном движке для инференса типа vllm? на hopper и более новые архитектуры пока не завезли таким образом, 2 4 может давать определенный выигрыш по скорости. основная проблема в том, что на текущий момент 2 4 прунинг post-training сильно просаживает качество llm ."
286,2024-07-04T21:04:35,"sparse maximal update parameterization a holistic approach to sparse training dynamics папира кода нет введение подбор шага обучения - занятие довольно потное и утомительное . learning rate, хороший для меньших моделей, зачастую приводит к расходимости для больших. потому большие модели обычно обучают с меньшим learning rate. но подбор по сетке или с помощью байесовской оптимизации может оказаться слишком накладным на практике, а иметь хорошие значения гиперпараметров все-таки хочется. в tensor programs v предложили параметризацию весов, градиентов и learning rate μp , такую, что оптимальные гиперпараметры обучения переносятся с маленьких моделей на большие. динамика обучения в разреженных сетях существенно отличается от плотных. потому естественно предположить, что обобщаемая параметризация на разные степени прореживания потребует дополнительных телодвижений. и в данной статье выводят такую параметризацию. метод параметризация, при которой learning rate и прочие гиперпараметры обучения будут переноситься на разные размеры моделей и степени прореживания, должна удовлетворять следующим свойствам 1 нормы весов не зависят от ширины и sparsity 2 нормы градиентов по весам не зависят от ширины и sparsity 3 изменения весов для произвольного алгоритма оптимизации не зависят от ширины и sparsity из нехитрой математики следует, что variance весов при иниациализации и learning rate следует масштабировать как 1 ширину_слоя 1 - sparsity . чем шире сеть - тем меньше разброс параметров и learning rate, чем более разреженная сеть - тем, наоборот, больше разброс параметров и learning rate. при sparsity 0, предложенная sμpar вырождается в μp. эксперименты метод валидируют на lmках c swiglu и alibi, обучаемых на токенах из slim pajama. в качестве маленькой прокси модели для тюнинга гиперпараметрво берут lmку с 40m параметрами, а основную серию экспериментов проводят на 610m модели. замеряют loss на обучении датасет большой, потому переобучения нет . при стандартной параметризации и μp оптимальный learning rate приходится подбирать для каждой степени сжатия свой, в то время, как для sμpar оптимальные learning rate зафиксирован. перенесенные с меньшей модели параметры на большую для разных степеней прореживания позволяют достичь лучшего качества по сравнению с попытками затюнить стандартную параметризацию и μp. авторы - ребята из cerebras, потому умеют вполне эффективно эксплуатировать разреженные операции . вывод полезная идея при обучении разреженных сетей. ранее в sparsity scaling laws было показано, что при очень большом числе данных sparse модели более compute оптимальны, чем плотные. если в будущем будут активно учить llmки с использованием специализированного железа sμpar будет весьма кстати для подбора гиперпараметров."
287,2024-07-09T23:07:45,"fast as chita neural network pruning with combinatorial optimization статья без кода chita - это гепард, а не административный центр забайкальского края. статья 2023 года от google research про прунинг. введение optimal surgeon, использующий приближение 2 порядка для определения оптимальной сжатой конфигурации, лежит в основе многих методов прунинга и квантизации. алгоритм iterative hard thresholding iht , в котором на каждом шаге делается шаг оптимизации и прунинг самых маленьких весов, лежит в основе некоторых методов sparse training - в частности, ac dc. в данной статье решили поженить obs и iht и назвали полученную сущность chita combinatorial hessian-free iterative thresholding algorithm . метод основная проблема с гессианом нейронной сети, что его хрен посчитаешь честно. существуют различные приближения в данной статье опираются на фишеровское оценка гессиана сумой внешних произведений градиентов . обычно градиентов n много меньше, чем параметров d , потому полученная матрица низкоранговая. можно ее даже не материализовывать, так как в итоговом алгоритме потребуются только матрично-векторные произведения, а хранить лишь n градиентов в матрице a in r n x d . фишеровская матрица выражается как a t a. chita на каждом шаге делает шаг градиентного спуска для квадратичного разложения в окрестности оптимума т.е l w l_0 g t 1 2 w t h w с последующим прореживанием как в iht. метод требует времени и памяти линейной по числу параметров и сохраненных градиентов. оптимальный шаг оптимизации learning rate в iht находят с помощью умного алгоритма поиска на прямой. эксперименты метод валидируют на небольших cnnках на cifar-10 и imagenet. метод быстрее m-fac и при этом достигает лучшего качества. однако честность сравнения в one-shot pruning, учитывая, что метод итеративный, в отличие от m-fac, вызывает вопросы. метод можно применять итеративно, постепенно повышая степень сжатия и дообучая сеть с фиксированной маской. лучше повышать линейно уровень прореживания линейно, чем экспоненциально или за раз пытаться сжать все. при итеративном сжатии удается добиться 1 просадки на mobilenet-v1 при 75 cжатии и 5 просадки при 89 . в этом режиме почему-то забывают в сравнениях про m-fac , который достигает близких результатов по качеству. метод довольно неплох и на resnet50 - с умеренными просадками при 90-95 sparsity. выводы метод, неплох и не слишком сложен в реализации. основная проблема - необходимость хранить много градиентов, что не позволяет масштабировать метод на llmки и интересные в 2024 году модели. у вашего покорного слуги есть одна идейка, как это обойти. но это пока секрет"
288,2024-07-11T22:27:21,"flash attention 3 вышел! статья блог не прошло и года с выхода flash attention 2 , как вышло продолжение, доведя серию до трилогии. метод основная идея первого flash attention в уменьшении передачи памяти между hbm и кэшом gpu, а второго - в отпимизации не matmul операций. flash attention 2 позволяет довольно эффективно использовать вычислительные возможности a100, но для более современной h100 все еще имеет место сильная недоутилизация - всего 35 . в flash attention 3 отпимизируют attention с учетом новых архитектурных особенностей hopper - новых тензорных ядер с большим throughput, тензорным ускорителем памяти tensor memory accelerator и поддержкой fp8. основных источника ускорения два 1 паралеллизация вычислений между gemm general matrix multiply и иными операциями softmax, rmsnorm . хоть по flops softmax и нормализации кажутся мизерными, они выполняются в 256 раз медленее на h100, потому могут занимать до половины времени матричного умножения. за их выполнение отвечают другие компоненты gpu, чем тензорные ядра для перемножения матриц, потому их можно обрабатывать параллельно. 2 использование fp8. fp8 позволяет почти удвоить скорость. однако есть нюанс - выбросы, приводящие к большим ошибкам квантования. чтобы избавиться от них используют incoherence processing из quip. благодаря этому ошибка квантизации уменьшается до 2.6 раз. результаты с fp16 удается достичь ускорения до 1.6-1.8 раз по сравнению с flash attention 2. и под 1.2 pflops c fp8 на больших последовательностях и с большими головами трансформера. обещают в будущем накатить в торч. вывод сильно! ждем через год flash attention 4 !"
289,2024-07-19T07:20:39,"vision language models are blind статья код наряду с чисто языковыми моделями в последнее время интенсивно развиваются и мультимодальные модели, умеющие работать с текстовой и визуальной информацией одновременно. флагманские проприетарные модели gpt-4 v o , claude, gemini 1.5-pro неплохо умеют решать нетривиальные задачи, связанные с обработкой визуальных данных - понимание содержимого сцены, нахождение нужного объекта, извлечение информации из графиков и табличек, и самое главное - понимание мемов . однако данные задачи еще более менее имеют формулировку с точки зрения естественного языка. кроме того, имеет место утечка в процессе обучения. и авторы задаются вопросом - как хорошо современные модели справляются с задачи, требующие умение опираться сугубо на визуальную информацию, причем те, что под силу даже детсадовцу ? и оказывается, что несмотря на простоту рассматриваемых задач, нынешняя sota не то чтобы блестяще справляется с ними. и gpt-4o не всегда лучшая метод предложенный бенчмарк состоит из 7 задач 1 подсчет числа пересечений двух графиков из 3 точек от 0 до 3 2 определение того, пересекаются ли 2 круга или нет 3 определение буквы, на которую наложили круг 4 подсчет числа пересекающихся геометрических примитивов 5 подсчет количества вложенных квадратов 6 определение числа строк и столбцов в таблице 7 умение следовать за разноцветными ломаными сколько путей следует из одной вершины в другую . путей от 0 до 3. рассматривают 4 прориетарные vlm vision language model модели 1 gpt-4o 2 gemini-1.5 pro 3 claude sonnet 3 4 claude sonnet 3,5 эксперименты несмотря на простоту перечисленных выше задач, модели справляются с ними далеко не идеально. в частности, в задаче определения числа пересечений gpt-4o достигает качества 48 , а лучшая из моделей - claude-3.5 sonnet 77 , что далеко от 100 . чем ближе графики и чем толще линии - тем больше ошибка. модели хорошо понимают, что такое круг, и умеют читать текст без ошибок, но, тем не менее определение того, какую же все-таки букву закрыл кружочек, оказывается не такой уж простой задачей. и здесь лидирует gemini-1.5 pro со средней точностью около 93 . vlm-ки умеют хорошо считать разнесенные в пространстве объекты. но как только они пересекаются, или оказываются вложенными - качество заметно проседает. и здесь снова побеждает в claude-3.5 sonnet. такая элементарная задача, как подсчет числа столбцов и строк, тоже дается нелегко, при том, что перечисленные сетки умеют решать куда, казалось бы, более сложные задачи про обработку табличных данных. умница claude-3.5 sonnet выдает точность в среднем 74.26 , в то время как остальные модели между 35 и 40 . если в ячейках есть текст, точность немного выше. определение числа путей по картинке, тоже дается нелегко. claude-3.5 sonnet не подкупили antropic авторов? снова побеждает с точностью в среднем 50 . если путь один, то claude-3.5 с большим отрывом точнее всех, на большом числе путей - 3, gpt-4o лидирует. вывод любопытное исследование, хоть результаты и во многом ожидаемы. текущие модели основаны преимущественно на достижениях в nlp, способности llm улавливать сложные закономерности в последовательностях. но задачи, опирающиеся сугубо на зрительную информацию, по всей видимости, требуют от модели обучения с упором на геометрию, выявления пространственных закономерностей. вопрос в том, насколько это важно в прикладных задачах. тем не менее разработка такой процедуры обучения может стать следующим шагом в развитии vlm."
290,2024-07-20T18:14:18,"hydra bidirectional state space models through generalized matrix mixers статья код современные нейронные сети, обрабатывающие пространственно-временные данные различной природы будь то текст , изображения , аудио и видео так или иначе обладают механизмом перемешивания каналов channel mixing , обрабатывающим независимо признаки для каждого элемента последовательности, и механизмом обработки последовательности sequence mixing , использования взаимосвязей между элементами. в сегодняшнем рассказе речь пойдет про sequence mixing. существуют разнообразные опции sequence mixing. операция смешивания может не зависеть от входа, как например свертка или обучаемая матрица l x l l - длина последовательности в mlp-mixer, s4 и h3 state-space модели, или зависеть - attention механизм в трансформерах или mamba selective state spaces . кроме того, разные механизмы обладают разной сложностью от длины последовательности. sequence mixing в attention или mlp-mixer требует квадратичного по длине последовательности числа элементарных операций с плавающей точкой flops , так как используют матричную операцию довольно общего вида. sequence mixers, обладающие некоторой структурой низкоранговые, toeplitz матрицы, dft, бабочки позволяют добиваться субквадратичной сложности обычно с некоторой просадкой в качестве . и sequence mixing может быть как причинным causal attention, большинство ssm, в частности, модная нынче mamba , где текущий элемент последовательности может смотреть только в прошлое, и двунаправленным как в masked language modelling, и большинстве задач с vitами , где элементы последовательности могут изменять свое состояние, как глядя как на прошлые, так и на будущие токены. и задача, которую, перед собой ставят авторы в данной работе - получение эффективного механизма двунаправленного sequence mixing, такого, чтобы он был с одной стороны субквадратичным в идеале линейным по длине последовательности и в то же время выразительным."
291,2024-07-20T18:22:34,"метод общий фреймворк выглядит следующим образом функция преобразования данных f_x x - входные данные функция конструкции смешивающей матрицы f_m l x l, которая может быть постоянной или зависеть от входов результат sequence mixing имеет вид f_m f_x x далее авторы вводят термин sequence aligned matrices sam, еще один означающий, что матрица смешивания зависит от входных данных. такие sequece миксеры хороши с одной стороны тем, что более адаптивно подстраиваются под входы, и, кроме того, работают с последовательностями разной длины. авторы рассматривают разные механизмы из литературы mlp-mixer, s4, h3, monarch, vandermonde и cauchy миксеры - не sam attention, linear attention, s6, ssd - sam потому хороший двунаправленный sequence mixer должен быть sam, и представлять собой некоторую структурированную матрицу. в частности, предлагаются способы сделать vandermonde и cauchy миксеры зависящими от входов, но основной упор делается на прокачку ssd не твердотельного жесткого диска, а механизма в mamba-2! под двунаправленность. напомним, что ssd во второй мамбе является полуразложимыми semi-separable матрицей - каждый блок является низкоранговой матрицей. для двунаправленности можно было бы чередовать слои ssd один бегущий слева направо, другой справа налево , но здесь предлагают использование одной матрицы смешивания, такой что любой ее блок в верхне-треугольной и нижне-треугольной части является низкоранговой матрицей. иначе говоря, получается нечто типа суммы исходной ssd из mamba-2 нижнетреугольной матрицы и транспонированной по длине последовательности верхнетреугольной матрицы и диагональной части. такие матрицы называют квазиразложимыми qs . данная модификация требует всего пары дополнительных строчек в реализации по сравнению с исходным ssd слоем shift - сдвиг на один элемент, flip - разворот последовательности задом наперед, dx - диагональная добавка . qs x shift ss x flip shift ss flip x dx называют гидрой, потому что много голов, как в ssd, и звучит красиво . эксперименты метод валидируют на задаче masked language modelling, где в качестве бейзлайнов берется bert, обученный по рецепту от mosaicml, и иные варианты sequence mixerов из литературы. для оценки качества моделей смотрят на валидационную кросс-энтропию на c4 на train set которого обучают и точность на бенчах из glue. все модели имеют размер порядка 70m параметров несколько меньше, чем bert-base , так что хрен вам sota на lmsys. hydra модели глубже трансформеров примерно в 2 раза при примерно том же числе параметров. sam модели стабильно опережают свои не sam версии toeplitz, cauchy, vandermonde с параметрами, зависящими от входа, заметно точнее версии с обучаемыми, не зависящими . hydra естесна , лучше всех, и на втором месте любимый нами трансформер. однако, памятуя о недавнем результате mobilellm, где более глубокие и тонкие трансформеры, оказываются лучше по качеству более коротких и жирных при том же числе параметров, задаешься вопросом - можно ли устранить разрыв в качестве за счет изменения конфигурации трансформера ? исходная mamba-2 не очень сильна в mlm, так как умеет обрабатывать информацию только в одном направлении, и предложенный способ hydra лучше вариантов с суммой, конкатенацией, и перемножением результатов двух мамб-2. далее метод проверяют на задаче классификации imagenet-1k, где обучают модели размера порядка vit-base 87-91m параметров и hydra опережает vit-b, hyena и s4. однако, vit бейзлайн вызывает вопросы, ибо согласно их результатам vit-b имеет top-1 точность 78.8 , а его ema 80.6 , в то время, как c рецептом обучения из swin, на который они ссылаются унаследованный в свою очередь из deit выдает 81.8 их лучший результат 81.6"
292,2024-07-20T18:22:34,"выводы данная статья полезна с формальной точки зрения, ибо дает некоторый обобщенный взгляд на современные варианты нейросетевых архитектур обрабатывающие последовательности, и немножечко вкусненькой математики. результаты экспериментов вызывают некоторые вопросы, да и валидация на небольшом, ограниченном сетапе. как мне кажется, пока преждевременно говорить об убийце трансформеров . тем не менее, интересно и занимательно."
293,2024-07-21T16:06:53,"вот мой скромный канал преодолел рубеж в 1 0 0 0 подписчиков когда я создавал его, я и не мечтал о таком количестве. думал, что это будет глубоко андерграундная гаражная телеграм-канава на пару десятков заинтересованных, в силу специфики тематики и моей лени в отношении заморочек с pr. но несколько упоминаний на знаменитых и влиятельных отечественных телегах про dl и связанные темы, за что огромное спасибо che_shr_cat, alexwortega, nadlsk, milana_shhanukova, открыли мои безделицы широкой аудитории. всем спасибо за то, что вы здесь мне очень приятно видеть ваши реакции, комментарии к постам. желание опубликовать разбор очередной статьи, наверное, лучший стимул поглубже в ней разобраться. будем дальше стараться. быть добру!"
294,2024-07-23T18:30:37,"-3.1 стала доступна широкой публике! пост на meta model card на github коллекция на пост на gonzo-ml пост на эйай ньюз пост на love.death.transformers к версиям 8 и 70b добавилась жырная 405b. из основных фичей 1 мультиязычность english, german, french, italian, portuguese, hindi, spanish, and thai 2 128k окно контекста 3 обрезка знаний по декабрь 2023. 4 умеет с тулами работать обучалось на 15t неизвестно насколько токенов из публичных как-то собранных данных. базовые меньшие модели иногда, кста , просаживаются по сравнению с v3 по метрикам, но instruct стабильно лучше. большая модель вполне себе бодается с gpt-4-omni и claude-3.5-sonnet. посмотрим, что выдаст lmsys арена на хабе есть и fp8 версии"
295,2024-07-23T18:33:25,боль...
296,2024-07-24T11:07:50,"кванты llama-3.1 уже появились на хабе. на текущий момент выложены 1 awq-int4, gptq-int4 квантизации llama-3.1-405b-instruct 2 bnb-nf4 квантизация llama-3.1-405b 2 awq-int4 квантизация llama-3.1-70b 4 awq-int4, bnb-nf4 квантизации llama-3.1-8b-instruct судя по скорости выкладывания, для калибровки моделей использовалось довольно мало данных, поэтому я бы ожидал значительной просадки в качестве по сравнению с исходной моделью. int4 требует 203 gb vram для 405b модели, а bnb-nf4 220 gb."
297,2024-07-30T11:18:58,"vllm совместно с neuralmagic регулярно проводят так называемые office hours, где рассказывают о новых фичах в релизах vllm и проводят туториалы по пользованию своими либами, сжатию и ускорению моделек. в частности, заслуживают внимания 1 туториал по квантизации с vllm выступает мой соавтор eldar kurtic . 2 fp8 и с чем его едят. в общем, рекомендую"
298,2024-08-01T17:08:11,"на свет появился еще один стартап про генеративное ии - black forest labs помните черный лес из облы ? . чуваки привлекли 31m инвестиций, и в дело включились такие серьезные люди как timo aila и vladlen koltun. команда выпустила семейство моделей flux.1 из трех моделей 1 проприетарной pro , доступной через их api и на replicate.ai 2 открытой dev с 12b параметрами 3 открытой schell тоже с 12b параметрами, но нацеленную на генерацию в малое число шагов 1-4 , по всей видимости дистиллированную из dev c помощью adversarial diffusion distillation. утверждают, что по elo score бьют open-source и closed-source модели типа mjv6, sd3-ultra, ideogram. сами модели по себе представляют ditы, обученные на flow matching. модель умеет генерировать изображения разного разрешения с разными aspect ratio. техрепорт обещают выложить в будущем. веса моделей на 1 flux.1-dev 2 flux.1-schnell гитхаб проекта"
299,2024-08-05T00:53:02,"после долгой борьбы с инфраструктурой, богами хаоса и разрушения, удалось-таки квантизовать aqlm-ом и дообучить pv-алгоритмом меньшие версии -3.1. 1 ista-daslab llama-3.1-8b-aqlm-pv-1bit-1x16-hf 2 ista-daslab llama-3.1-8b-aqlm-pv-2bit-1x16-hf 3 ista-daslab llama-3.1-8b-instruct-aqlm-pv-1bit-1x16-hf 4 ista-daslab llama-3.1-8b-instruct-aqlm-pv-2bit-1x16-hf 70b в процессе. самая интересная из , безусловно , 405b, но пока развлекаемся с тем, как поместить сие жирное чудище на машину"
300,2024-08-05T00:55:32,"мы настолько тормозили, что instruct 2-битную модель люди из community выложили раньше нас. любопытно. откуда такое разночтение в метриках для исходной модели. если конкретно по нашим замерам неквантизованная llama-3.1-instruct выдает 68.17 точности в среднем на mmlu. а квантизованная aqlm в 2 бита и зафайнтьюенная нами pv модель - 59.17 . то есть просадка у нашей модели меньше, но и бейзлайн выше"
301,2024-08-12T11:18:50,"ишшуя на гитхабе в webui накатили квантизованные версии недавно нашумевшей flux.1 dev . 1 bnb nf4 4 c небольшим бит на параметро 2 fp8 если считать fp8 квантизацией в зависимости от железа рассматривается случай gpu от ampere и новее и версий pytorch cuda bnb nf4 быстрее от 1.3 до 4 раз. так как модель довольно здоровая, 12b параметров, то трансфер памяти, по всей видимости играет существенную роль. кроме того, после недавних обновлений bitsnandbytes 0.42-0.43 скорость матричных операций bnb.matmul_4bit сильно выросла. кроме того, утверждается, что ошибка квантизации nf4 даже меньше благодаря более адаптивной квантильной квантизации чем у fp8 e4m3fn e5m2 . далее пост содержит обсуждение хаков, как эффективно запускать flux на винде и примеры генераций."
302,2024-08-12T11:29:16,"кроме того, в свежем pr отличная работа galqiwi ! в bitsandbytes добавили классы embedding4bit, embedding8bit. в современных llmках идет тренд на рост размера токенизатора, и недавняя работа пост на эйай ньюз , показывает, что это даже положительно влияет на качество вдобавок к уменьшию длин последовательностей. для экстремально сжатых моделей в 1 и 2 бита эмбеды и lm_head занимают значительную долю в суммарном обьеме памяти на хранение модели. в недавней issue показано, что эмбеды можно сжимать в 4 бита простой bnb квантизацией без просадки, а lm_head в 8 бит в 4 с небольшой просадкой . данное обновление весьма полезно для дальнейшего движения в сторону экстремального сжатия и посадки llmок на мобилки."
306,2024-08-15T00:09:07,"how fireworks evaluates quantization precisely and interpretably блог недавно ребята из fireworks.ai, провайдера инференса llmок, выпустили занятный блог про то, как следует оценивать качество сжатых моделей. главные выводы и предписания следующие 1 trade-off между степенью сжатия и качеством определяется конечным приложением. не существует универсального предписания для всех задач, что, мол, во столько раз можно сжать с умеренной просадкой в качестве, а дальше нельзя. 2 использование kl-дивергенции для оценки степени расхождения квантизованной модели от исходной. 3 не полагаться на точность на стандартных бенчмарках, а диверсифицировать протокол замеров по первому пункту, вывод довольно логичен. некоторые задачи function calling менее чувствительны к ошибкам, чем другие code generation , где один косячок, и весь дальнейший вывод сыпется как домино. перплексия тоже плоха, так как ниже у моделей с низкой энтропией, но не всегда более точных. ссылаясь на свежую работу от microsoft accuracy is not is all you need, авторы заявляют, что точность на бенчах - довольно шумная метрика оценки качества. может быть так, что исходная модель где-то ошибалась, а квантизованная случайно стала предсказывать нужный токен. и прирост качества скорее всего мизерный обусловлен случайностью, а не тем, что модель стала лучше . kl-дивергенция между сжатой и несжатой моделью предлагается в качестве альтернативы точности на наборе задач, как устойчивая к шумам. если конкретно, предлагается генерировать несжатой моделью forced generation , и смотреть, насколько различаются вероятности следующего токена предсказанные fp моделью и квантизованной. кроме того, еще можно смотреть на token rejection rate - насколько отличается выбор top n токенов n 16 в дальнейшем между двумя моделями. ниже под квантизацией подразумевается переход от fp16 к fp8 братве явно завезли баржу с h100 . рассматривают 4 уровня квантизации по возрастанию степени сжатия 1 квантизуем только mlp без первого и последнего блока трансформера 2 квантизуем все веса 3 квантизуем вдобавок еще kv-cache 4 квантизуем attention k, v? на prefill точность на mmlu просаживается немонотонно, уровень 2 даже оказывается лучше fp16 на тютельку. kl-дивергенция же монотонно растет со сжатием. далее авторы смотрят еще на ряд задачек из helm-lite - gsm8k, naturalquestions, wmt-2014 и другие, сравниваясь с другим известным провайдером - together.ai . что fireworks, что together выдают качество близкое к fp16 при инференсе в fp8 а то и лучше некоторых задачах , но fireworks якобы чуть лучше. однако, радоваться рано, говорят авторы из fireworks.ai. нужен замер на чем-то как можно более похожем на людские предпочтения на том-же lmsys. alpaca-eval слишком маленький и не использует современные методы промптинга, потому предлагают смотреть на arena-hard-auto от создателей lmsys. fp8 модели от fireworks и together будто бы чуть хуже fp16 на -3.1-405b, но не статзначимо. выводы вопрос оценки качества llmок довольно нетривиален, ввиду того, что современные модели умеют решать широкий круг задач, и сложно объять всю полноту возможных задач и областей знания. академические бенчмарки из нескольких задач и замеров перплексии на паре датасетов хороши для статей здесь и ваш покорный слуга согрешил , но не всегда удовлетворительны для реальных приложений и запросов пользователя. потому следует полагаться не на чужие обещания, а самому всю тщательно прощупывать ."
307,2024-08-15T11:22:10,"как сделать text2image модель, которая по sbs бьет midjourney? 1 учишь модель, которая хоть иногда лучше midjourney 2 собираешь большой набор промптов 3 прогоняешь side-by-side comparison 4 отбираешь промпты, где ваша модель бьет midjourney 5 утверждаешь, что на внутреннем бенче добился победы над mj 6 stonks!"
308,2024-08-16T23:36:09,"on the impact of calibration data in post-training quantization and pruning статья лаконичный ридми введение многие современные методы сжатия моделей что квантизация, что прунинг оптимизируют некоторую меру ошибки на репрезентативной выборке данных калибровочном датасете . интуитивно понятно, что эта выборка есть приближение целевого распределения данных, поэтому желательно, чтобы этой самый калибровочный датасет как можно точнее и полнее приближал его. на текущий момент, в сообществе не сложилось четких правил и предписаний по сбору калибровочных данных. преимущественно, исследователи и практики полагаются на то, что было предложено ранее в работах, или тому что предлагает gguf. и в этой работе, авторы исследуют вопрос влияния калибровочных данных на качество data-aware методов сжатия llmок. метод берутся 2 метода квантизации 1 gptq 2 spqr я польщен и 2 метода прунинга 1 sparsegpt 2 wanda модели квантизуют в 4 бита spqr в 4.5 по факту , и прунят в 2 4 sparsity. в качестве калибровочных данных рассматривают следующие источники 1 c4 бессмертная классика 2 cnn-dm новости, длинный текст хорошего качества, именно текст, не сами новости 3 redpajama 4 refinedweb 5 wikipedia английская отовсюду берут 128 сэмплов длины 2048, как в статье gptq. качество замеряют на десяти 0-shot бенчах. берут 9 моделей 1 llama-1 7b, 13b, 33b 2 vicuna, полученные из выше 3 opt 6.7b, 13b, 33b"
309,2024-08-16T23:37:25,"результаты и наблюдения 1 точность может сильно различаться между разными подвыборками из одного датасета. 2 есть более и менее шумные задачи. boolq и rte показывают наибольшую дисперсию. 3 одни датасеты лучше других. в среднем refinedweb показывает лучшее качество, а wikipedia худшее, но разброс значителен - между моделями и методами и нет четкого ранжирования. 4 прунинг сажает качество моделей сильнее, потому и влияние данных более заметно, по всей видимости. разброс для sparsegpt больше ожидаемо, так как sparsegpt не только прунит, но и обновляет оставшиеся веса . 5 optы более чувствительны к выбору данных, чем парнокопытные модели 6 качество быстро насыщается с количеством калибровочных примеров для всех методов, кроме sparsegpt. по нашим наблюдениям, чтобы выжать максимум из качества пара тысяч последовательностей все же нужна . 7 sparsegpt лучше wanda хотя исходная статья утверждала обратное . логично, ибо wanda есть де-факто кастрированный sparsegpt. выводы мораль сей басни такова - подбирайте калибровочные данные осмысленно. полезное исследование, но чувствуется, что для полноты картины нехватает более сложных задач и других областей математика, код , как и более свежих моделей."
310,2024-08-16T23:44:50,"в более свежей работе, кстати, утверждают, что данные вообще не важны, и случайные последовательности работают якобы так же хорошо за исключением opt . но кажется, что авторы набагали где-то..."
311,2024-08-17T21:11:25,"compact language models via pruning and knowledge distillation статья код есть, хоть это поначалу и не очевидно введение с этими всеми квантизациями мы стали забывать про прунинг. в отличие от довольно бюджетных техник квантизаций, дающих значительное сжатие за дешево, прунинг, тем более структурированный, требует серьезного дообучения. но если вы не гаражный стартап, а компания с триллионной капитализацией - вам он под силу. подданые кожаной куртки выпустили запруненные версии nemotron-15b под названием minitron-4b и minitron-8b, которые по качеству якобы конкуретноспособны текущей sota в легком весе. метод у трансформеров можно прунить целые блоки про что был обзор некоторое времчя назад размерность признаков был у вас d_model - 768, стал 512 головы в attention внутреннюю размерность mlp первый метод про прунинг в глубину, а остальные про ширину. для оценки важности используются критерии, основанные на активациях средний attention по голове при прунинге головы средняя активация по каналу для mlp средняя активация layernorm для оценки важности признаков для прунинга в глубину смотрят на среднее косинусное расстояние между входом и выходом блока и изменение перплексии при выкидывании блока. рассматривают оценку важности one-shot, когда выкидываем все за раз, или итеративно, по чуть-чуть. далее выбирают некоторое около 20 число конфигураций сжатых моделей с целевой степенью сжатия, сжимают модели и дообучают на 1.8b токенов. берется самый перспективный кандидат и дообучается уже на 100b токенов. немало, но все же на 2 порядка меньше, чем у современных llm. на дообучении рассматривают сумму следующих лоссов 1 кросс-энтропию исходный лосс 2 kl-дивергенцию на логитах с несжатой моделью 3 l2 лосс между промежуточными признаками так как сжатая модель может быть уже, есть матрица переводящая в размерность учителя"
312,2024-08-17T21:19:20,"результаты качество замеряют на широком круге бенчей всякие mmlu, arc, winogrande, humaneval, и mt-bench, ife-eval. основные выводы следующие 1 сжимать большую модель, дешевле чем обучать новую. на том же числе токенов хрен вы получите тоже же качество с нуля. после дообучения нет разницы между one-shot и итеративным сжатием. 2 в ширину сжимать эффективнее, чем в глубину. блоки прунятся хуже, чем каналы 3 базовый лосс не нужен, используйтк только дистилляционный. 4 итеративная оценка важности не накидывает после дообучения кандидатов. 5 если глубина уменьшена, дистилляция промежуточных признаков важна, иначе хватает только логитов. 6 прунить лучше модель, как можно ближе к целевой по размеру логично 7 короткое дообучение 1.8b токенов дает более надежных кандидатов моделей для дальнейшего дообучения. 8 если нужна instruct модель, лучше запрунить instruct модель, чем base и дообучить на инструкциях. minitronы выдают качество не хуже других open-sourcr моделей - того же размера 8b как llama-3-8b и qwen-2 4b как phi-2 и gemma-2-it только безлайны поменьше будут конкурентные методы сжатия, поверх других моделей sheared-llama, llm-pruner сильно уступают . вывод структурированный прунинг - перспективная стратегия сжатия llm, если есть ресурсы. сильно не хватает экспериментов с квантизацией - комплиментарны ли оба подхода, или прунинг усложняет квантизацию?"
313,2024-08-18T12:33:39,типичный area chair и рецензенты https ok.ru video 37602200235
314,2024-08-19T00:20:08,"spinquant llm quantization with learned rotations статья код если в названии статьи встречаются вместе spin и quant - это наверняка что-то из области физики. но нет, это снова про квантизацию llm ранее в ряде работ quip, quip , quarot было показано, что поворот на случайную ортогональную матрицу сильно облегчает квантизацию. и в этой работе ребята из meta предложили развитие идеи, дающее лучшее качество на weight activation cache квантизации. метод наличие выбросов в отдельных каналах активаций сильно затрудняет квантизацию так как per-channel квантизация обычно не поддерживается в железе . ортогональные матрицы размазывают выбросы в весах и активациях между каналами, так что все активации имеют примерно один и тот же порядок. но конкретный выбор ортогональной матрицы имеет значение. авторы замечают, что качество сильно варьируется от конкретного выбора матриц, если сэмплировать их произвольным образом и редко бывает достаточно хороших . случайные адамаровы матрицы из quip гораздо лучше, но все разброс существенен и нет гарантий на оптимальность. потому предлагается искать хорошие матрицы поворота с помощью оптимизации на stiefel многообразиях, параметризующих ортогональные матрицы. оптимизация представляет собой градиентный спуск на многообразии. а именно кэли sgd. есть 4 места куда можно вкрутить поворот r1 - в residual перед attention и ffn r2 - между v и o матрицами в attention r3 - в query key r4 - между активацией в ffn первые два поворота можно влить в параметры модели без изменения выходов модели, последние же придется считать на каждом прогоне. потому предложенная процедура оптимизации применяется для поиска оптимальных r1, r2 матриц. а в случаях r3, r4 используют дешевые адамаровы матрицы, кои практически не замедляют работу llmки и почти ничего не весят. для оптимизации используют небольшой набор из wikitext-2. результаты процедура оптимизации матриц занимает час с небольшим на меньших представителях семейства на 8 a100 и полдня для 70b моделей. замеряют по классике качество на перплексии и 0-шотах. рассматривают 3 сценария 1 weight only quantization 2 weight activation quantization 3 weight activation kv cache quantization по качеству операжают бейзлайны ближайший quarot примерно на 1 в среднем . в weight-only quantization где-то бага при замерах aqlm и quip , ибо эти методы сжимают модели почти без просадки если верить бенчам, а не здравому смыслу почти без просадки. метод работает как с rtn round-to-nearest , так и c gptq. важны все матрицы, но большой прирост дает будто бы добавление r4 - вращение внутри ffn. вывод квантизация весов активаций и кэша в 4 бита на текущий момент, наверное, то что дает самое большое ускорение инференса на современных gpu, потому и представляет значительный практический интерес. и улучшение качества - это хорошо. при квантизации активаций и кэша просадки все еще значительные и есть куда улучшаться."
315,2024-08-20T10:37:21,"fun with sparsity in pytorch via hadamard product parametrization блог занятный бложек про то, как можно сделать sparsity-inducing regularization. увидев слово hadamard я триггернулся на адамаровы матрицы, но нет, это адамарово произведение. идея в следующем l1-регуляризацию можно представить как l2, c добавлением вспомогательных параметров u, так что исходный вес w параметризуется как w u v для пар тензоров u и v. коэффициент регуляризации определяет степень разреженности получившейся матрицы. подход валидируют на табличной задаче с 500 признаками и двумя классами. сначала рассматривают логистическую регрессию реализованную в cvxpy, затем в торче. потом рассматривают групповую регуляризацию применительно к нейронным сетям. групповая регуляризация зануляет целые каналы и имеет меньше дополнительных параметров для неструктурированной маски число параметров удваивается, что может быть накладно . в эксперименте берется небольшая mlp обучается на california_housing известном всем табличникам с некоторым коэффициентом регуляризации. исходно было 4500 весов, в полученной сетке остается около 600 ненулевых коэффициентов. большинство каналов можно взять и вырубить. вся кухня имеет довольно прикольную реализацию через модуль torch.nn.utils.parametrize см туториал . после задания такой параметризации, сети можно обучать не задумываясь вашим любимым оптимизатором с weight decay. разве что в данном случае логичнее было брать adamw, а не adam. в общем, прикольно. но интересует масштабируемость."
316,2024-08-24T13:09:44,"если гитхаб и гитлаб заблочат, телегу с функционалом чатов можно использовать в качестве альтернативы сообщения с файлами - коммиты. разные чаты - разные ветки. pr - пересылка файлов из одного чата в другой. прямо как в старые добрые времена на заре систем контроля версий."
317,2024-08-25T20:13:07,"из double blind review вы пока полайкайте, а я потом шутку придумаю..."
318,2024-08-30T13:05:07,"bibtex-tidy наткнулся в интернетах на офигенную тулзу для наведения красоты в bibtex. если вы ссылаетесь на дохренища работ, в частности, пишете диссер или обзор, данный инструмент позволяет убирать дубликаты, чтобы overleaf не ругался при компиляции. кроме того, можно красиво отформатировать все как black , отсортировать в алфавитном, так что любо-дорого будет глядеть."
319,2024-09-01T17:29:02,когда собираешь свои статьи в один диссер
320,2024-09-03T12:07:20,"gift-sw gaussian noise injected fine-tuning of salient weights for llms статья код введение peft и квантизация - два понятия, которые за последние пару лет стали неразрывно связаны с большими языковыми моделями, особенно в контексте обучения и инференса в условии ограниченных ресурсов. и в сегодняшнем эфире будет сказ про немного необычное сочетание этих идей в работе от ребят из cколтеха, airi и вышки. метод вкратце напомню, что суть peft в до-обучении небольшого по сравнению с размером исходной модели числа параметров. эти параметры могут быть как адаптерами к весами как lora, dora, oft , кои можно или нельзя влить в модель, а могут быть некоторым подмножеством весов исходной модели. второй вариант еще называется в литературе selective finetuning и про него сегодня пойдет речь. метод основан на двух основных идеях 1 определении чувствительных весов, которые, с одной стороны не квантуются, а с другой являются тем самым подножеством обучаемых параметров. 2 добавлении шума в процесс обучения. наличие выбросов в весах, затрудняющая квантизацию, известная особенность llm временами оспариваемая , и потому для улучшения качества их обычно держат в исходной точности. в данном случае, следуя llm.int8 и quik выбросы определяются как уровне колонок матриц весов входных размерностей . в качестве метрики берутся комбинации вида s_i d_i x_i _ rho gamma где rho - норма весов в колонке, а gamma - степень - -½, 1, ½ . d_i - это пертурбация весов, и в качестве пертурбации рассматривается операция квантования скалярные однородная симметричная . выбросы, для которых значение данной метрики самое большое - не квантуются. градиентный спуск с шумом, примененным к весам до или после шага градиентного спуска, как известно из литературы, помогает избегать седел и плохих оптимумов, повышает устойчивость. в данной работе шум применяется до шага sgd. итоговый метод выглядит следующим образом квантизуем с помощью gptq все веса, за исключением выбросов . затем до-обучаем через обычный бэкпроп эти колонки, а оставшимся весам добавляем нормальный i.i.d шум с std равным шагу квантизации. рассматриваются 3 стратегии 1 pre-gift-sw - сначала учим с шумом и выбросам, типа готовим модель к квантизации, а затем квантизуем 2 salient ft - квантизуем и до-обучаем только выбросы без шума 3 post-gift-sw - квантизуем и до-обучаем выбросы с шумом"
321,2024-09-03T12:08:52,"эксперименты эксперименты проводят на llama-2,3 7b, 8b, 13b . до-обучают на инструкциях tulu2 и openorca. обучение идет 500 шагов т.е довольно короткое . выбросы определяются на основе 500 сэмплов из pile. для оценки качества берут среднюю точность на 5 бенчмарках из lm-eval. есть 2 сценария 1 instruction-finetuning fp16 моделей. в качестве бейзлайнов рассматривается файтьюн всей модели, и lora dora адаптеры с примерно тем же количеством обучаемых параметров. непонятно, однако, какой шаг квантизации для шума в данном сценарии если есть . 2 квантизация в 2 3 4 бита instruction-finetuning. в качестве бейзлайнов берут quik lora, и ste, где выбросы обучаются обычным бэкпропом, а квантизованные веса через ste. я только не понял, quik без или с квантованием активаций? больно уж грустно смотрится. предложенный подход достигает лучшего качества, чем адаптеры, и , как утверждается, ведет себя гораздо стабильнее от количества примеров см figure. 1 из статьи . данный график вызывает вопросы, ибо обыкновенно адаптеры демонстрируют довольно стабильную динамику обучения, будучи малыми возмущениями весов, если только не вкрутить чрезмерно большой шаг обучения. при квантизации предложенный метод опережает quik и ste. правда, есть более свежие и сильные бейзлайны - quarot и spinquant. из ablation study следует, что добавление шума во время до-обучения накидывает в качестве при квантизации в 2 бита. большой разницы нет при разных опциях метрики определения выбросов. l работает чуть лучше в среднем. pre-gift-sw немного лучше при 4 битном сжатии, но при более аггресивном сжатии уступает альтернативам. при квантизации в 2 бита post-gift-sw дает лучшее качество. вывод интересно, и неплохо по результатам. сравнение с адаптерами, я бы рекомендовал перепроверить - не шумят они так на до-обучении. еще любопытно, помогает ли добавление шума при до-обучении статистик квантования в методах типа peqa и p-tuning без v ."
322,2024-09-05T10:53:11,"вчера наша статейка accurate compression of text-to-image diffusion models via vector quantization попала в подборку статей daily papers на про диффузию, что большая честь для нашего скромного авторского коллектива из yandex research. так-то не то, чтобы science - по существу перенесли aqlm на задачу text-2-image генерации с помощью диффузионнок с учетом специфики и нюансов диффузионных архитектур плохой из меня бизнесмен . проверяли подход на sdxl и sdxl-turbo. в целом, вышло сносно, получше скалярной квантизации по качеству c q-diffusion, ptq4dm в качестве бейзлайнов . в 4 бита удается даже достичь паритета по side-by-side с оригинальной fp16 моделью. здесь уместно заметить, что sbs куда более репрезентативная и содержательная характеристика, чем все эти ваши fidы и clip-скоры. 3-битные модели по метрикам почти не отличаются от fp16 по метрикам, но просадка качества налицо не у всех есть толока за пазухой, справедливости ради . с практической точки зрения пока есть над чем работать имеет место замедление инференса на 50 , ибо в отличие от огромных llm, sdxl sdxl-turbo малипусики c 2.5b параметрами, которые кроме того обрабатывают большие тензоры активаций за раз, а не токен за токеном, потому вычисления compute-bound, а не memory-bound. процедура деквантизации начинает сказываться на времени прогона через сеть. потому модельки пока не выкладываем. нынче модный flux-1 выглядит более перспективным кандидатом для прогонки метода, там и трансформер с большими матрицами, и 12b параметров. как руки дойдут, попробуем и его посжимать. кроме того, запилили еще симпатичный сайтик а-ля nerfies credits to vahe527887 ."
323,2024-09-09T16:55:23,обман года. 2023 - lk-99 2024 - reflection
324,2024-09-12T10:36:56,"во вчерашнем посте на kali novskaya была разобрана статья can llms generate novel research ideas? - про то, насколько хорошо нынешние llmки умеют в генерацию наукоподобных статей. и результат такой вкратце - что в некоторых аспектах вездесущей аморфной плохоопределенной научной новизны по мнению ассесоров статзначимо лучше человеков. в то же время технические аспекты и экспериментальная постановка в иишных статьях слишком размыта и расплывчата. да и по факту сгененированные статьи являются некоей сборной солянкой из известных фактов. отсюда возникает идея - давать вступительный экзамен рецензентам core a конференций, где им будут даны несколько публикаций от кожаных мешков и сгененированных условной гопотой или sonnet. если рецензент не способен адекватно отличить зерна от плевел - отфутболиваем . проблема правда одна кто ж рецензировать то будет... хотя было бы интересно посмотреть на рецензию gpt-4 с системным промптом reviewer n2 ."
325,2024-09-13T21:03:06,"есть просто банк, есть банк столбов и матриц, а есть банк оптимальных решеток для квантования данных из нормального распределения... если конректно, чуваки собрали mse-оптимальные решетки для числа точек от 1 до 5000 для одномерного нормального распределения, и для всех размерностей от 2 до 10 и числа точек от 1 до 1450 для многомерной гауссианы с нулевым средней и единичной матрицей ковариации . решетки были найдены через lloyd-clvq algorithm. зачем оно надо? если ваши данные нормально распределены, или вы можете каким-то образом привести их к этому виду, то такая квантизация при заданной битности будет оптимальной с точки зрения квадратичной ошибки. сайт проекта"
326,2024-09-16T00:20:12,"eden communication-efficient and robust distributed mean estimation for federated learning статья код квантизовать можно веса, можно активации. а что еще можно? правильно, градиенты! и это очень даже нужно и полезно в контексте распределенной оптимизации. если обучение на многих хостах, а скорость передачи информации между серверами не очень высока, то на синхронизацию между процессами может уходить основная доля времени, а не на сами вычисления. дабы уменьшить трафик можно сжимать градиенты - прунить, квантовать - что душе угодно. при этом, само собой, хочется их не слишком испортить, иначе все обучение пойдет прахом. метод идея квантовать градиенты для распределенного не нова - датируется еще 2016 годом qsgd . но данная работа предлагает ряд интересных идей некоторые из которых мы узнаем в литературе вышедшей позднее, а сама статья 2021 года . 1 случайные повороты 2 оптимальные решетки квантования 3 случайный прунинг 4 энтропийное кодирование в тензоре значения могут быть распределены, вообще говоря, самым произвольным и поганым с точки зрения удобства квантования образом. но, известно, что если повернуть тензор некоей случайной матрицей, то с большой вероятностью полученные значения будут распределены как n 0, sigma 2 . в качестве случайных поворотов используют кто бы мог подумать! - случайные адамаровы матрицы. каждый процесс хранит seed генерации, а главный сервер, зная этот seed на деквантизации, может применить обратное преобразование. решетку квантизации а тут дело идет о квантовании в 1 и 2 бита надо выбирать с умом. потому берут не абы какие, а mse-оптимальные для 1 бита находится аналитически, для 2-ух численно . дабы увеличить сжатие - можно еще случайным образом запрунить часть весов. маску передавать при этом не надо - она как и вращение задается состоянием случайного генератора. решетки квантизации можно еще дотюнить при заданном ограничении на битность посредством entropy-constrained vector quantization. таким образом, каждый процесс квантует градиенты, посылает главному серверу, тот их усредняет и получает средний градиент по всем батчам. и вся эта кухня идет с теоретическими гарантиями на ошибку аппроксимации. эксперименты метод валидируют на синтетических распределениях и распределенном обучении. в качестве бейзлайнов рассматривают qsgd, hadamard sq stohastic quantization , kashin sq. в качестве бенчей для распределенного обучения рассматривают emnist с маленькой cnnкой и shakespeare с lstm . не самые свежие и впечатляющие бенчи, но что поделаешь . eden наиболее точно приближает исходные данные в сравнении с бейзлайнами , при этом по скорости не такой быстрый как qsgd, но быстрее kashin sq ближайший с точки зрения качества подход . в 4 бита разницы вообще нет между eden и float обучением, в 2 и 1 бит появляется некий зазор в качестве, но не критичный, и eden стабильно лучше конкурентных подходов. вывод интересная с точки зрения математики и методологии статья. кажется, что если бы публикация чуть дотерпела до тех времен и у авторов был на то ресурс , когда квантовать llmки стало мейнстримом предложенные идеи были бы обязательно применены в контексте сжатия llm."
327,2024-09-23T11:23:24,"fine-tuning llms to 1.58bit extreme quantization made easy блог model card pr с добавлением bitnet в transformers некоторое время назад здесь мы разбирали статью от мелкософта про bitnet-1.58, где авторам удалось завести qat c тернарной квантизацией и добиться вполне сносного качества при обучении с нуля. лицехваты недавно опубликовали весьма занятный и содержательный длиннопост про дообучение llm в 1.58 log2 3 бит. метод и эксперименты напомню, что при тернарной квантизации каждый элемент тензора принимает одно из трех значений - -1, 0, 1 и еще есть floating-point масштаб вместо absmax берут среднюю абсолютную величину элемента тензора . дабы квантовалось лучше, перед каждым линейным слоем ставится layernorm, и это накидывает на предобучении. при попытке наивно затюнить llama-3-8b в bitnet сходимость по лоссу не сильно лучше, чем у случайно инициализированной модели, и авторы начинают копать глубже. тернарная квантизация это довольно болезненное надругательство над весами после которого сетке тяжело оправиться, потому авторы предлагают переходить к квантованной модели не сразу а интерполировать из fp16 состояния в квантизованное постепенно. пробуют разные функции - степенную, экспоненциальную интерполяцию. одна опция степенной в итоге завелась лучше всего. шаг обучения тоже пришлось основательно подбирать, чтобы выжать приемлемое качество. после подбора удачных гиперпараметров на коротких запусках, лучшую конфигурацию учат уже 100b токенов 0.6 бюджета обучения llama-3-8b . тем не менее, итоговая просадка по сравнению с исходной моделью довольно значительная. для маленьких lm smollm 125m разогрев постепенная квантизация не дает улучшений. по бенчам просадки значительные по сравнению с fp16, но лучше чем у прошлых квантизаций в похожую битность. сам протокол сравнения спорный , не у всех вариантов есть результаты, сравниваются перплексии разных версий llama с разными словарями. да и llama-3 выбивает 65, а не 49 на mmlu 5-shot. авторы реализовали kernel для инференса на тритоне, который дает некоторое ускорение. а с библиотекой bitblas ускорение становится еще более заметным. однако работа с ней требует небыстрой компиляции. вывод неплохой результат и исследование. но есть вопросы к протоколу замеров качества, и пока bitnet теряет слишком много в качестве для того чтобы стать общепринятой практикой. чтобы пользоваться модельками надо поставить версию transformers из pr."
328,2024-09-25T20:58:54,"смотрите-ка, что появилось!"
329,2024-09-25T21:41:25,зальем чем-нить на iclr и засядем за квантование. тока сначала надо раздобыть веса.
330,2024-10-02T19:14:10,"vptq extreme low-bit vector post-training quantization for large language models статья код пристегните ремни, а лучше прячьтесь в бомбоубежище . будет горячо . на текущий момент, векторная квантизация является наиболее эффективным с точки зрения качества методом сжатия llm aqlm, aqlm pv, quip , gptvq . и ребяты из китайского отделения мелкософта выкатили yet another vector quantization for llm. метод по существу метод представляет собой gptq с векторной single- и multi- codebook квантизацией с рядом нюансов 1 эффективной инициализацией центроид векторов в кодбуках . используют hessian-weighted k-means. для полного гессиана считать сложно и дорого, потому в этом месте прибегают в диагональному приближению. подобное я в свое время заводил, когда работали с коллегами над spqr и думали про неоднородную 1-мерную квантизацию. 2 точность квантизации за счет повышения битности можно повысить за счет нескольких кодбуков. используют residual quantization как в quip , где новые кодбуки приближают ошибку квантования с прошлых шагов. 3 outlierы. находят outlierные колонки входные размерности , которые выдают большую ошибку на выходе слоя и квантуют их отдельно. часть без outlierов обрабатывается стандартным алгоритмом. далее прогоняется поблочный файнтьюн, как в aqlm quip и end-to-end дистилляция. результаты метод валидируют на моделях семейств -2, -3 и mistral. в качестве бейзлайнов берут gptvq, aqlm, quip . и далее начинается самое интересное они применяют поблочный и end-2-end finetuning, как в quip и обновленной версии aqlm, но при этом сравниваются со старой версией aqlm а надо тогда с таблицей 4, где метрики на 7b и 13b примерно такие же, и немного даже лучше на 70b . при сравнении скорости инференса разных подходов криво завели quip , при этом мотивируя низкую скорость тем, что перемножение на адамаровы матрицы требует o n 2 операций, хотя любой детсадовец знает, что o n log n . статья вышла в конце сентября, но про pv-tuning и qtip , вышедшие в конце мая и июня, соотвественно, ни слова, как будто весть еще не успела дойти по великому шелковому пути в поднебесную. и вишенка на торте - отсутствие aqlm quip среди бейзлайнов мотивируют тем, что модели новые, никто не выложил, а самим впадлу и напряжно квантовать. на что можно тактично заметить, что aqlm модели лежат на хабе, и не просто лежат, а там еще есть и метрики, с которыми можно сравниться. только одна беда - они несколько получше будут. что делать, притворимся, что мы ничего не видели моделей, кстати неплохо так выложили."
331,2024-10-04T11:49:32,"лернинг рейт, бач сайз и вейт декей звучат как отличные имена для героев фантастического романа."
332,2024-10-06T10:02:49,"за что большое уважение компании meta, помимо многого прочего, так это за реально подробные и занимательные технические отчеты. техрепорты по -м были довольно содержательными и полезными, и, полагаю, во многом опредилили направления развития llm, хоть и основывались по большей части на идеях из прошлых работ. отчет по moviegen не уступает по качеству и полноте содержания техрепортам по llama и может служить своего рода обзором с рассмотрением и ablation современных техник и достижений в области генеративных моделей. прекрасная работа, asanakoy! сам бы у себя разобрал в серии постов, но лучше, чем причастный к созданию moviegen, это никто не сделает"
333,2024-10-09T08:55:57,"спрашивает народ - по какой-такой физике присвоили нобелевскую премию хинтону и хопфилду? ну, смотрите, есть физика языковых моделей. и вряд ли бы она появилась без трудов хинтона и хопфилда хинтона уж точно . а в самой премии не указано по какой конкретной физике надо её давать или не давать. все формальности соблюдены."
334,2024-10-09T13:50:26,раз уж пошла такая гулянка - предлагаю дать нобелевскую премию мира суцкеверу за вклад в ai safety. или юдковскому...
335,2024-10-12T21:52:55,из реализации flux в diffusers
336,2024-10-12T22:38:58,"isomorphic pruning for vision models статья код введение полагаю, что уже немного помнят, но когда-то выбить sota или создать мега эффективную по параметрам флопсам на imagenet-1k было модно и престижно. в данной статье авторы предлагают метод структурированного прунинга, который достигает хорошего качества на ряде vitов, сnn и их гибридов. метод при структурированном прунинге отбрасываются целые каналы, слои или размерности в сети. однако ранжировать конструкции разной топологии между собой непонятно как, потому предлагается ранжировать только среди структурных элементов одинаковой топологии. каждому нейрону сопоставляется некий граф зависимости из входящих и выходящих в него ребер, кои погибнут, если выдернем нейрон. собираем все изоморфные графы в соответствующие кучки например с одинаковым числом входных выходных ребер для прунинга нейрона из mlp считаем для них некоторый importance score, и для тех у кого он меньше - выбрасываем. для случая трансформера естественные структуры следующие 1 внутренние каналы в mlp 2 головы трансформера 3 размерности голов трансформера 4 размерность признаков во всей сети пробегаемся по всем возможным структурам и везде выбрасываем одну и ту же долю самых маловажных. то, что получилось потом дообучаем дабы восстановить качество. эксперименты валидируют метод на convnext, deit, resnet-ах и mobilenet-v2. в плане критерия не шибко парятся, берут просто w dw - модуль веса на градиент, посчитанный на 100 батчах из 64 сэмплов не то, что я пол imagenet-а пихал в свое время . прунят довольно агрессивно, в 1.5 - 4 раза, восстанавливаться надо достаточно долго и основательно, потому обучают 300 эпох с дистилляцией на regnety. в результате, удается добиться небольшой просадки на resnet-ах по сравнению с базовой моделью. правда, протокол сомнительный - берут модели обученные по старым рецептам без аугментаций на 90 эпох, а сами тратят гораздо больше вычислений с использованием современных методик выжимания качества на imagenet. на deit и convnext стартуя с deit-base convnext-base удается получить запруненные сети с лучшим качеством, чем small- и tiny- модели с тем же количеством параметров и flops т.е достичь парето-оптимальности . по качеству на процент-два превосходят в среднем бейзлайны из литературы. вывод нравится идея с ранжированием структур с одинаковой топологией, но такое ощущение, что хороший результат обусловлен преимущественно дистилляцией с длительным обучением. на llmки, к сожалению, масштабировать будет тяжко простым смертным. но всякие там nvidia могут позволить для условного minitron."
337,2024-10-14T10:50:32,"почему дают токены на всякую фигню, а не на o1..."
338,2024-10-16T00:11:14,"maskllm learnable semi-structured sparsity for large language models статья страница проекта код пост на machine learning введение 2 4 она же semistructured sparsity дает какое-никакое ускорение на gpu от ampere и новее. однако, просадка при прунинге обычно слишком велика для llmок, дабы быть интересной на практике. в этой статье предлагают метод обучения хороших 2 4 масок через gumbel-softmax. метод маска суть дискретная сущность потому ее просто так не отпизируешь градиентным спуском, и авторы предлагают моделировать распределение масок через gumbel-softmax с 6 binom 2, 4 вариантам. на обучении оптимизируются логиты вероятности сэмплирования одного из вариантов масок т.е маска есть взвешенная сумма возможных вариантов , а на инференсе берется наиболее вероятный. обучение суть просто оптимизация кросс-энтропии как на pretrain . веса при этом заморожены. если какой-то вес зануляется или близок к нулю, то логиты маски почти не получают градиентов, потому авторы добавляют регуляризационный член как weight_decay, но со знаком , чтобы расталкивать веса от нуля, тем самым поддерживая не нулевую норму у немаскированных весов. кроме того, маски полученные условным sparsegpt wanda являются хорошей инициализацией для масок и позволяют чуть улучшить результат. эксперименты метод валидируют на -2, nemotron-4 15b и двух маленьких проприетарных gpt-3. замеряют по классике перплексию на wikitext и 0-шоты. по метрикам опережают уверенно все бейзлайны sparsegpt, wanda, magnitude . sparsegpt, правда, можно завести и получше. в отличие от алгоритмов one-shot прунинга, которые быстро насыщаются от количества данных, maskllm продолжает улучшаться при большем и большем количестве данных, что неудивительно ибо это есть по сути метод оптимизации с большим количеством обучаемых параметров. ablations 1 инициализация маской от one-shot прунера накидывает в конечном качестве. 2 достаточная степень стохастичности сэмплирования важна для хорошего качества, дабы модель могла попробовать разные варианты масок. 3 анти-weight decay не то чтобы сильно, но улучшает качество. 4 кроме того, полученную маску можно оптимизировать на downstream и даже временами оверфитнуться улучшить перплексию по сравнению с floating-point моделью. вывод вполне годная стратегия для обучения 2 4, но требующая определенных вычислительных затрат т.е прилично дороже чем прогнать sparsegpt . результат достойный, но все же просадка остается довольно заметной - больше чем у sota методов 2-битной квантизации. вероятно, если еще оптимизировать веса вместе с масками - можно выжать больше."
339,2024-10-18T10:37:26,"ребята из unsloth выкатили блог про раздебаг gradient accumulation. пост на love. death. transformers. tl dr - наивное усреднение батчей при gradient accumulation приводит к тому, что результат отличается от того, чтобы прогнать за один шаг большой батч. причем, как утверждают авторы, лосс всегда больше. в стандартном способе вычисления кросс-энтропии есть деление на длину последовательности, и если они разные в разных микробатчах - результат усреднения будет не такой, как лосс на суммарном батче. потому предлагается избавлятьcя от усреднения. утверждение про то, что лосс при naive gradient accumulation больше, вообще говоря, неверно. в конце блога авторы доказывают, что лосс при наивном gradient accumulation больше и в доказательстве делают внезапный переход от лосса по микробатчам к среднему лоссу по батчу. можно подобрать контрпример. пусть на первом батче лосс l_1 1, и в батче 4 токена, а на втором батче l_2 9 и 6 токенов. тогда усредненный лосс на всем батче 1 9 4 6 1 а при наивном gradient_accumulation 1 2 1 4 9 6 0.875 то есть меньше"
340,2024-10-19T21:03:04,"alphapruning using heavy-tailed self regularization theory for improved layer-wise pruning of large language models статья ридми, но код все равно нахрен не нужен обсуждать по существу тут нечего, чисто поугарать. метод хотим мы прунить llmки. текущие методы сжимают слои равномерно. однако, одни слои могут быть чувствительнее других. как эффективно распределить степени прореживания между разными слоями для максимизации качества? вообще говоря, хинтон его знает, но одна занятная тема выползет скоро на архив . часть авторов сего опуса годом ранее предложила owl - outlier weighted sparsity, где чувствительность слоев определялась на основе доли выбросов процента весов, с активациями существенно отклоняющимися от среднего значения . в этой же статье предлагают использовать коэффициент в законе убывания сингулярных значений матриц весов w. логика такая - у случайных матриц закон при иниализации весов элементами из i.i.d нормального распределения убывания собственных значений есть марченко-пастур c ограниченным спектром, а степенная зависимость типа отвечает сигналу. чем медленее убывание - тем типа больше сигнала. потому матрицы с меньшей степенью предлагается сжимать слабее. задают некий порог минимальной и максимальной степени сжатия и распределяют степень сжатия между слоями в зависимости от того, насколько велик мал коэффициент по сравнению с минимальным максимальным по всем слоям модели. эксперименты стоял на дворе 2024 год, а ребята основную часть экспериментов делают на llama-1 и vicuna. валидируют по сложившейся традиции на перплексии и 0-shots. отдельного внимания заслуживает утверждение our empirical results show that alphapruning prunes llama-7b to 80 sparsity while maintaining reasonable perplexity, marking a first in the literature on llms. при том что перплексия переваливает за 200 в лучшем случае, что на практике означает, что модель галлюцинирует чуть менее чем полностью на любой запрос. предложенный метод по качеству по перплексии несколько лучше uniform, критериев, основанных на спектральной норме и норме фробениуса, предложенного ранее owl, но тем не менее просадки остаются весьма значительными на уровне 1 eps битных квантизаций при сжатии на 70 . есть результаты и на -3, которая почему-то называется llama-v3-7b. там просадки значительнее. еще и на convnext прогнали для разнообразия. найденные профили для llm обычно следующие - пруним меньше первые блоки, сильнее - последние за исключением самого последнего . вывод выглядит как использование некой взятой из воздуха характеристики матриц весов для оценки важности, без внятной мотивации. тем не менее, в упорстве и умении себя хорошо подать авторам не занимать, потому их и взяли на neurips. а вообще, прунить llmки тяжело дается как-то по сравнению с квантизацией. в 4 бита просадка на простых бенчах почти и не видна, а 50 sparsity сжатие в 2 раза уже существенно ломает модель."
341,2024-10-21T00:03:34,нейросеть многоловая по госту
342,2024-10-21T00:03:48,внимание - всему голова! attention is all you need
343,2024-10-21T17:58:18,"- у нас в компании оплата по количеству строчек написанного кода. - отлично, по рукам. matmul_had.py в quip-sharp lib utils"
344,2024-10-22T10:50:09,"deep compression autoencoder for efficient high-resolution diffusion models статья код введение сжимать можно не только матрицы весов llmов, но много еще что-то другое не без пользы для человечества. более-менее все современные sota или около того диффузионные модели работают в латентном пространстве, с меньшим пространственным разрешением. от автокодировщика тандема из сети, кодирующей из исходного пространства данных в латентное, называемой энкодером, и декодирующей обратно - декодера хочется одновременно высокой степени сжатия и качественной реконструкции одновременно. наиболее популярный пространственный фактор сжатия - 8x stable diffusion, flux, dalle-3 по всей видимости . при таком сжатии особенно если еще и поднять число каналов в латентном пространстве до 16, скажем реконструкции почти не отличаются от оригинала. однако карты признаков остаются все еще достаточно большими при подаче картинок высокого разрешения, и требуется немало вычислений. отсюда возникает желание двигаться в сторону больших факторов сжатия при сохранении качества. и авторы данной статьи предлагают несколько интересных архитектурных решений. метод по сути в предложенной статье две ключевых идеи 1 skip-connection residual autoencoding - при downsample патчи 2x2 сворачиваются в 4 канала, а зачем группы по 2 канала усредняются ребята изобрели average pooling? - при upsample 4 подряд идущих канала разворачиваются в 2x2, каждый канал дублируется 2 процедура обучения - предобучение на низком разрешении только на реконструкцию без адверсариального лосса - дообучение на низком разрешении верхних слоев декодера с gan лоссом для получения резких деталей - дообучение глубоких слоев энкодера и декодера на реконструкцию на высокое разрешение дабы адаптировать латенты под high-resolution c другим распределением частот плотностью обьектов"
345,2024-10-22T10:52:17,"результаты авторы тестируют vae на непосредственно задаче реконструкции и применительно к латентной диффузии dit uvit на imagenet ffhq, mapillary vistas я тоже впервые слышу про такой . рассматривают сжатие в 32 64 128 раз по пространству с количеством латентных каналов 32 64 128. для повышения эффективности vanilla transformer блоки заменяют на efficientvit. residual autoencoding критичен при высоких факторах сжатия - сеть сама не может выучить этот skip connection. sd-like vae оказывается беспомощен жесткие дефекты и размытие на экстремальных факторах , а им удается иметь метрики более-менее на одном уровне с ростом downsampling. при фиксированном количестве патчей в dit uvit качество выше при более сильном сжатии за счет автоэнкодера и меньших патчах из латента размера 1 против меньшего сжатия и больших патчей. утверждается, что с dc-vae можно получить качество не хуже, а то и лучше, чем в сетапе, как в оригинальной статье, при этом имея 4-кратное ускорение инференса. предложенная процедура дообучения на высокое разрешение decoupled high-resolution adaptation критична для адекватного качества на высоком разрешении. кроме того, dc-vae проверют на коротком 100к итераций -2-image сетапе. вывод результаты выглядят весьма достойно. прокачка энкодера - это комплиментарное направление повышения эффективности диффузионных моделей вкупе с архитектурной оптимизацией и дистилляцией по шагам. похожую идею, во всей видимости реализовали в свежей sana."
346,2024-10-23T17:45:25,"выбираете темную сторону силы? статьи на arxiv, оказывается, можно читать в инвертированной цветовой схеме черный фон белый текст . для этого надо просто в pdf заменить arxiv - arxiv.black https arxiv.black pdf arxiv-id . не знаю зачем, но прикольно."
347,2024-10-27T12:45:26,"решил я запустить генерацию с meta-llama meta-llama-3.1-8b-instruct с bos_token то есть без всякого контекста с температурой 1, и было любопытно, что первое взбредет сеточке в голову. ответ оказался немного неожиданным 慢性胰腺炎 ci 是一种慢性胰腺炎 其特点是胰腺炎症的持续存在和慢性损伤 胰腺炎是肾上腺皮质激素 cort 过量释放的副作用之一 与ci相比 其作用更持久 если вбить в переводчик, выдает следующее chronic pancreatitis ci is a kind of chronic pancreatitis, which is characterized by the persistence of pancreatitis and chronic damage.pancreatitis is one of the side effects of excessive release of adrenal corticosteroids cort .compared with ci, its effect is more lasting."
348,2024-10-29T10:54:07,"self-calibration for language model quantization and pruning статья кода нет многие современные методы сжатия моделей используют некоторую выборку калибровочных данных для приближения эмпирического распределения данных. чем ближе эта выборка к целевому набору задач - тем интуитивно лучше качество, но хороших предписаний по отбору последовательностей до сих пор не существует. ранее эти же авторы перебрали несколько вариантов пост на кпд , и обнаружили, что некоторое, хоть и не столько значительное, различие в качестве есть в зависимости от источника данных. в этой же статье авторы предлагают сжимаемой модели самой сгенерировать данные для калибровки. метод калибровочные последовательности генерируют начиная с bos токена. дабы повысить качество данных предлагается динамически менять температуру - сначала больше, чтобы было разнообразие, а затем снижать по мере увеличения количества сгенерированных токенов. эксперименты рассматривают несколько небольших llm gemma 2b phi-2 opt 6.7b mistral 7b llama-3.1-8b которые сжимают при помощи awq, gptq 4-битная квантизация и sparsegpt, wanda 2 4 прунинг . в качестве бейзлайнов берут wikitext-2, c4, рандомные токены, сosmopedia. качество замеряют на стандартных бенчах из lm-eval-harness. синтетические данные, сгенерированные моделью, почти всегда лучше выборок из датасетов. различие заметнее на прунинге, где просадки больше. сгенерированный текст обыкновенно довольно связный, грамматически корректный, по статистикам довольно близок к реальному, но менее разнообразный. снижение температуры генерации от 2 до 1 по ходу генерации дает самые лучшие результаты, хоть и без значительного отрыва от фиксированной температуры - 1. вывод идея прикольная, и для используемого количества данных 128 последовательной длины 2048 достаточно дешева в проверке. эффективность метода во многом зависит от качества базовой модели, что, впрочем, для современных llm, интересных сообществу верно. интересно , справедливы ли полученные выводы для более сложных задач?"
349,2024-10-29T23:43:48,"соавтор и коллега по yandex research galqiwi выкатил демку aqlm на ржавчине. можно просто взять и запустить llama 3.1-8b, квантованную в 2 бита aqlm pv в браузере на cpu, со скоростью 1.4 токена в секунду на m1. lm head квантизуется в int8. отличная работа, вова! демка пост на galqiwi boredom репост на love. death. transformers репозиторий"
350,2024-10-31T11:51:41,breakpoint - уровень сеньор.
352,2024-11-04T23:41:36,"я ожидал от статьи с названием flexround, что в ней квантизованные веса будут реально флексить, а оказался просто поблочный qat ."
353,2024-11-08T17:30:27,"если вам приспичит инферить llmку на микроволновке. пока разогревается еда , мы как раз подогнали 2-битные меньшие версии llama-3.2 llama-3.2-1b-aqlm-pv-2bit-2x8 llama-3.2-1b-instruct-aqlm-pv-2bit-2x8 llama-3.2-3b-aqlm-pv-2bit-2x8 llama-3.2-3b-instruct-aqlm-pv-2bit-2x8 основное место занимают уже эмбеды которые хоть shared, но с fsdp немного покорячиться пришлось . ждем выкатки квантизации эмбедов в transformers."
354,2024-11-08T23:49:10,"liger kernel efficient triton kernels for llm training техрепорт репозиторий ребята из linkedin написали кернелы для разных операций в llmках на тритоне когда нибудь я научусь писать на нем, а не про него , которые ускоряют процедуру обучения и снижают расход памяти по сравнению с ванильной торчовой реализации. метод в частности, liger kernel предлагает следующее зафьюженные rmsnorm и layernorm на прямом и обратном проходе зафьюженные swiglu и geglu на прямом и обратном проходе оптимизированный rope и самое интересное - оптимизация вычисления кросс-энтропии по поводу последнего словари нынче у моделек перевалили за 100к, и на сколь-либо длинных последовательностях матрица логитов будет весить десятки гигов. потому авторы реализовали прямой проход, вычисление кросс-энтропии и обратный проход в одном kernel-е без необходимости материализации одновременно матрицы логитов и градиентов по логитам. кроме того, считать логиты можно не разом для всей последовательности, а чанками. бенчмарки замеряют на a100. предложенные нормализации дают хорошее ускорение по сравнению с торчом не хватает сравнения с apex . rope прямо знатно ускорился. swiglu и geglu по скорости такие же, как в торче, но снижают расход памяти в 1.6 раз. в end-2-end сценарии гоняют finetune на 4a100 на alpaca на llama-3-8b, qwen-2-7b, gemma-1-7b. скорость обучения возрастает от 10 до 40 , и пиковый расход памяти уменьшается в среднем на 50 . метод также валидируют в связке с medusa, методом где llm учится предсказывать несколько следующих токенов за раз, и на каждый n 1 токен своя обучаемая голова. liger kernel снижает заметно расход памяти как с замороженной, так и обучаемой тушкой трансформера, благодаря трюкам с вычислениями логитов. использование liger kernel можно вызвать по щелчку пальца через autoligerkernelforcausallm или патчинг модели из . а можно импортировать модули и из них собрать модельку. liger kernel уже интегрирован в transformers, trl и axolotl."
355,2024-11-09T14:18:15,"в комментариях был задан резонный вопрос - а сравнивали с torch с или без compile? будучи лучшего мнения о людях, я полагал, что да. но если посмотреть в скрипты бенчмарков в репозитории проектов - compile не фигурирует нигде. rmsnorm берут сырой из modeling_llama.py в частности. хитрые собаки !"
356,2024-11-10T11:00:39,"eora training-free compensation for compressed llm with eigenspace low-rank approximation статья написание кода предоставляется читателю в качестве несложного упражения введение низкоранговые приближения применяются много где в deep learning и не только, про lora не слышал только ленивый. в данной статье группа миньонов кожаной куртки предлагает компенсировать ошибку методов сжатия прунинга и квантизации за счет вставки низкорангового разложения ошибки сжатия как адаптера паралелльно весам. метод метод прост как пробка. 1 были у нас исходные веса w. 2 после сжатия имеем wc. 3 svd матрицы w - wc 4 берем сколько-то главных компонент и вставляем параллельно сжатым весам как lora адаптер. при этом ничего не надо обучать. эксперименты метод тестируют поверх 50 60 unstructured sparsity, 2 4 sparsity с sparsegpt в качестве алгоритма сжатия и 3 4-битной квантизацией gptq. в большинстве экспериментов берут ранг добавки равным 128. проверяют на llama-2 7b и 13b , llama-3 8b . метод дает ожидаемо небольшой прирост по качеству на стандартных бенчах, но не слишком выдающийся. чем больше ранг добавки - тем, ожидаемо, лучше качество. в случае необходимости добавки можно дотюнить, а-ля чиста lora файнтьюн, и это еще немного накинет. вывод простой и бюджетный способ накинуть немного качества за счет небольшого количества параметров и дополнительных вычислений. так как базовые методы сжатия data-aware sparsegpt, gptq , то вместо svd можно было бы использовать reduced rank regression, и это бы работало, скорее всего, чуть лучше."
357,2024-11-10T22:57:52,"будь бы у меня дети, я бы их побенчмаркал на mmlu."
358,2024-11-12T10:52:06,"bitnet a4.8 4-bit activations for 1-bit llms статья репозиторий пока не обновили код продолжение истории с bitnet от microsoft. 1 в первой части была предложили бинарную квантизацию весов. 2 во второй части вместо бинарной квантизации тернарная, но зато качество заметно возросло. при этом активации квантовались absmax в int8. в данной статье фокусируются на сжатии активаций в частности, kv-кэшей в 4 и бита и меньше. метод авторы строят гистограммы весов активаций после линейных слоев. выходы q, k, v, gate, и up неплохо описываются нормальным распределением. а вот out, down имеют тяжелые хвосты потому тяжело квантизуются . поэтому предлагается квантовать в 4 бита активации только первой группы слоев, а для выхода out_proj следуя работе qsparse, которую сам пока не читал прореживают активации через topk с 50 sparsity . кроме того вместо swish в ffn gate применяют relu 2, чтобы форсировать прореживание активаций в down projection. впрочем, так как паттерн случайный, использовать его для ускорения сходу не получится. также рассматривают fp4 квантизацию вместо int4 с большим динамическим диапазоном. эксперименты обучают семейство -подобных моделей от 700m до 7b параметров на красной пижаме. причем обучают в 2 стадии, сначала с 8-битными активациями большее время обучения как в bitnet1.58 , а затем в перечисленных выше группы слоев квантуют в 4 бита или прореживают . bitnet a4.8 int4 fp4 почти не уступает по качеству fp бейзлайну не очень сильному, впрочем . relu 2 дает значительную около 80 степень разреженности в down_proj. обучение чисто в int4 не сходится в принципе. с fp4 гораздо хуже, чем предложенный вариант. relu 2 дает - тоже качество, что более традиционный swish. кэши квантизуют в 3 бита кроме bos , eos токенов, которые более чувствительны и поддерживаются 4 битными, без просадки в качестве по сравнению с 4-битными кэшами. определив хорошую конфигурацию, 2b модель обучают на 2t токенах и получают метрики, конкурирующие с хорошими fp16 моделями в той же весовой категории llama-3.2, gemma-2, qwen-2.5 . впрочем, ребята из мелкософта известны тем, что добавляют тест в train как было в случае с моделями phi . вывод история с bitnet начинает выглядеть все более жизнеспособной, несмотря на весь мой скептицизм по поводу кустарности метода. во всяком случае, история выглядит уже вполне рабочей. тем не менее, было бы важно проверить, насколько хороши модельки в качестве чат-ботов, ежели выкатят веса."
359,2024-11-12T23:35:21,"процесс чтения рецензий на iclr впрочем, как и на любой другой core a конфе"
360,2024-11-15T03:29:30,"scaling laws for precision статья кода нет, как и ресурсов у вас, чтобы воспроизвести введение известно, что с увеличением размера модели и количества данных качество моделей в некотором смысле - обычно по val лоссу растет. причем не абы как, а по простым степенным законом а-ля шиншилла . также известно, что инферить большие модели тяжело и дорого, а методы квантизации позволяют существенно сжимать модели в пределах умеренной просадки качества. есть наблюдение, что более современные llm llama-3 , gemma-2, qwen2 сжимаются заметно тяжелее , чем предшественники отсюда вопрос, при заданном бюджете на обучение, какое оптимальное отношение числа токенов к размеру модели и битность квантизации? и в рассматриваемой работе, авторы проводят детальное и масштабное исследование, делая целый ряд нетривиальных выводов. метод ниже d - количество данных n - количество параметров модели p - precision на обучении авторы обучают тучу моделей 465 штук а-ля разного размера, битности от 3 до 16 и с разным бюджетом обучения вплоть до отношения числа токенов к параметрам 10 5 . тем самым авторы учитывают случай характерный для современных моделей, где перекос в сторону данных сильно выходит на chinchilla-optimal закон d n 2000 у llama-3 против d n 20 по шиншилле . рассматривают 3 сценария 1 post-training quantization. учим во bf16 и квантизуем после обучения. берут gptq, как ходовой и рабочий метод. 2 quantization-aware training. квантизуем по ходу обучения. но только веса. 3 low-precision training. квантизуем во время обучения веса, активации и kv-кэши. предложенный scaling law для post-training квантизации имеет вид p - precision, она же битность l n, d, p an - alpha bd - beta e delta_ ptq n, d, p где delta_ ptq - прирост лосса, вызванный квантизацией. для qat и low-precision training l n, d, p an - alpha 1 - e p_ w gamma_w 1 - e p_ a gamma_a 1 - e p_ kv gamma_kv bd - beta e то есть, некоторые модификации исходного scaling law."
361,2024-11-15T03:33:28,"эксперименты post-training quantization с увеличением количества данных на обучении и качества модели , ошибка квантизации растет . кроме того, ошибка для больших моделей фиксированном отношении d n меньше у больших моделей. самый любопытный и шокирующий вывод в том, что начиная с какого-то момента у модели, обученной на большем количестве данных, качество после квантизации будет хуже, чем у модели, на меньшем. то есть при квантизации в низкую битность, бессмысленно учить модели до упора, и в какой-то момент придется остановиться, чтобы получить лучшее, что в принципе возможно. проанализировав зависимости, авторы выводят функциональную форму для delta_ ptq , где ошибка растет степенным образом по количеству данных, убывает степенным образом по размеру модели, и растет экспоненциально с уменьшением битности. qat low-precision training далее авторы обучают модели, квантизуя либо только веса, либо веса и активации. оказывается, что эффект от квантизации можно описать как некую мультипликативную добавку к количеству параметров вида 1 - e p gamma . т.е модель с квантизацией чего либо и эквивалентна модели с n 1 - e p gamma параметрами. коэффцициенты p, gamma свои у весов, активаций и кэшей. кэши легче всего квантизуются, веса посередине, а активации тяжелее всего. причем ошибки квантизации w, a, kv независимы и предложенная зависимость хорошо фитируется на основе экспериментальных точек. предполагая, что у нас есть вычислитель, умеющий эффективно работать с любой точностью, авторы фиксируют бюджет обучения, определяемый как c 6 p 16 nd 16 от bf16 и ищут оптимум по лоссу. наблюдения следующие если учить в низкой точности, масштабировать размер модели надо быстрее, чем количество данных. в шиншилее, напомню, оптимально пропорционально compute-optimal precision не зависит от бюджета обучения и примерно равно 7 бит на параметр. эксперименты проводят с int-x, fp-x, floating-point чуть лучше, но общий вывод справедлив. отсюда замечание, что обучение в int4 fp4 и более низких битностях может быть не оптимально."
362,2024-11-15T03:37:06,"вывод как мне кажется, это одно из самых интересных и фундаментальных исследований в области, которое обрисовывает практический потолок возможностей методов квантизации. интуитивно понятно, что сжимать модели до бесконечности невозможно, ибо так или иначе сеть должна в себе как-то хранить все знание о мире , но вопрос стоял именно в определении самих границ. крайне любопытно, что отношении оптимальной полученной битности к 16-ти близко к compute-optimal sparsity в sparsity scaling laws пост на кпд . совпадение ли ? кроме того, интересно, насколько полученные выводы справедливы для более навороченных векторных квантизаций quip , aqlm, qtip и что будет, если поменять точность обучения на ходу учить в fp bf16 , прогнать ptq, и далее qat low-precision training . еще кажется, что полученные выводы будто бы находятся в расхождении с результатами bitnet, который исходя из полученных зависимостей должен быть дохрена не оптимальным и выдавать слабое качество."
363,2024-11-15T03:47:08,теперь с чистой душой можно лечь досыпать
364,2024-11-21T19:30:07,я добрая училка
365,2024-11-21T21:52:11,"gptqmodel репозиторий gptq в настоящий момент является одним из самых популярных методов квантизации весов llm в 4 бита как дающий некий хороший баланс между качеством и временем работы. он выдает стабильно лучшее качество по сравнению с наивным round-to-nearest и иными data-free квантизациями, при этом масштабируется сравнительно легко на огромные llm. оригинальный репозиторий тестирует сугубо работоспособность метода и не годится для приложений. значительную популярность 4.5к на гитхабе набрала библиотека autogptq c а-ля лицехватским интерфейсом и поддержкой различных моделей, а так же кернелов а-ля marlin для эффективного инференса. к сожалению, maintainerы забросили либу, потому самые новые модели через нее квантовать не получится. ребята из modelcloud продолжили их дело, создав gptqmodel, куда добавлены llama-3.2, qwen-2.5, и другие сравнительно новые модели. кроме того, авторы обещают более быструю калибровку до 50 , быстрые замеры перплексии, и немного лучшие по качеству квантизованные модели."
366,2024-11-25T13:02:14,зубы заговаривают
367,2024-11-28T00:37:24,"catastrophic failure of llm unlearning via quantization статья код введение llm и прочие foundational модели хранят в себе массу полезной информации. однако некоторые факты могут быть довольно конфиденциальными или чувствительными, и мы бы предпочли, чтобы моделька не проворонила их невзначай в диалоге. например в трейн сэт могла утечь ваша неуклюжая переписка с девочками в тиндере , а конфуза хотелось бы избежать. потому были разработаны разные техники unlearning, например, gradient ascent градиентный подъем! и npo negative preference optimization , которые понижают правдоподобие нежелательных концептов. но действительно ли сеть забывает про них? оказывается, что если достаточно но не слишком агрессивно заквантовать модель, то она чудесным образом вспоминает все то, что ее так тщательно учили забыть. метод авторы рассматривают два семейства методов забывания фактов gradient ascent , где оптимизационный процесс повышает кроссэнтропию нежелательного концепта npo, форма dpo direct preference optimization , с занижением правдоподобия того. что хочется забыть чтобы сеть не забывала кроме целевых концептов все остальное, параллельно с этим дообучают на данных как в обучающей выборке с выброшенными концептами retain set , которые мы хотим забыть. подобные техники работают довольно успешно. но как только модель квантизуют в 4 бита, вероятность выдачи нежелательной информации становится примерно как у сжатой в 4 бита исходной модели до того, как ее стали учить забывать . 8-битная квантизация не так сильно меняет модель, поэтому метрики забывания мало отклоняются от 16-битной модели. в качестве методов квантизации рассматривают rtn, gptq, awq. выводы справедливы для всех методов. дабы заставить сетку быть устойчивой к забыванию, предлагается уйти достаточно далеко в пространстве параметров от исходных значений. но чтобы не испортить сеть в целом дообучают только самые чувствительные к целевым концептам параметры. и в качестве метрики чувствительности используют норму градиента лосса забывания . можно было использовать маску произвольного вида, но чтобы сэкономить память авторы отбирают чувствительные веса на уровне слоев т.е обучают только подмножество слоев . называется сия конструкция, конечно же, saliency-based unlearning with a large learning rate sure ."
368,2024-11-28T00:38:49,"эксперименты метод валидируют на muse бенчмарке для забывания и датасетах news где нужно забыть новость and books забыть персонажей из гарри поттера . какие модели используются, я не понял из текста. кроме того, оценивают общие способности модели через mmlu, truhtfulqa, triviaqa, и энтропию n-грам на alpacaeval. предложенный подход оказывается устойчив к квантизации во всех рассматриваемых сценариях в плане выдачи нежелательной информации, но снижает несколько качество на некоторых бенчах по общим способностям. вывод факт забавный, хоть и в какой-то мере ожидаемый. существует ли принципиальный способ заставить сеть забыть что-то увиденное? техники снижения правдоподобия по всей видимости просто заметают это знание под ковер. кажется, что требуется некая более глубокая хирургия с применением sae и прочей модной лабуды."
369,2024-11-29T08:33:37,"the super weight in large language models mengxia yu, de wang, qi shan, colorado reed, alvin wan статья https arxiv.org abs 2411.07191 код https github.com mengxiayu llmsuperweight очень прикольная работа про то, что внутри llm можно найти один единственный вес, зануляя который мы обрушиваем качество работы модели в пропасть. такие параметры авторы называют супер весами super weights и предлагают метод их нахождения за один forward pass. внутри обученных llm находится группа весов-аутлаеров с большой магнитудой, они могут составлять порядка 0.01 от всех весов модели, что в случае миллиардных моделей всё равно сотни тысяч. это было известно ранее. в текущей работе показывают, что внутри этой группы находится один единственный вес тот самый super weight, sw , не обязательно самый большой, важность которого превышает суммарную важность тысяч других аутлаеров. он необходим для качества, без него llm не может генерить нормальный текст. перплексия вырастает на несколько порядков, а точность на zero-shot задачах падает до рандома. ранее https arxiv.org abs 2402.17762 были найдены супер-активации, критичные для качества. они существуют в различных слоях, имеют константную магнитуду и всегда обнаруживаются в одинаковой позиции несмотря на вход. текущая работа находит, что канал активации совпадает с оным для супер веса и сперва активация обнаруживается сразу после супер веса. прунинг этого супер веса значительно уменьшает активацию, так что вероятно активация вызвана им, а не просто скоррелирована. такие активации называются супер активациями super activations, sa . предыдущая работа объясняла супер активации через bias terms, но не объясняла как они получаются и почему на одних и тех же местах. сейчас авторы эмпирически нашли, что до down проекции down_proj произведение адамара hadamard product gate и up проекций gate_proj, up_proj создаёт относительно большую активацию. супер вес далее усиливает её ещё и даёт супер активацию. напомню, что mlp блок в ламе выглядит так out down_proj act_fn gate_proj input x up_proj input sw можно найти, анализируя спайки в распределениях входов и выходов down_proj. для этого достаточен прямой проход с одним промптом. авторы нашли супер веса для llama 7b,13b,30b , llama 2 7b,13b , mistral-7b, olmo 1b,7b , phi-3. провели эксперименты по обнулению sw, в том числе с восстановлением sa до исходного значения, чтобы проверить влияние sw на другие активации. это восстанавливает 42 потери, то есть влияние sw на качество выше, чем просто через sa. по анализу 500 различных промптов из lambaba validation set видно, что при убирании sw вероятности стоп-слов сильно возрастают а обычные слова соответственно занижаются . для the это 2 , для . -- 5 , и для , -- 10 . то есть наличие sw как бы подавляет стоп-слова и позволяет генерировать осмысленный текст. другой интересный эксперимент скейлит супер веса с коэффициентами от 0 до 3 где оригинальный режим работы соответствует значению 1 и оказывается, что при увеличении sw качество модели ещё немного возрастает. это забавный результат. имея это знание, можно предложить специальный метод квантования super-outlier aware quantization. стандартные механизмы квантизации могут быть недостаточно хорошими, так как аутлаеры искажают распределение, влияя на размер шага и увеличивая ошибки квантования. здесь под super outliers подразумеваются и sw, и sa. предложенные методы восстанавливают sw и sa после квантований с клиппингом и заменами на медианное значение. это всё работает лучше дефолтных методов, главный вывод -- надо защищать супер веса. в статье есть подробный разбор экспериментов, кому интересно поглубже. также новый метод меньше теряет в качестве с увеличением размера блока. прикольный результат в общем. это всё несколько перекликается с темой про лотерейные билеты https t.me gonzo_ml 21 , там внутри большой сети обнаруживалась сильно разреженная подсеть, обучая которую можно было достигать качества исходной сети или даже выше . интересно, входят ли супер-веса в лотерейный билет? наверняка."
370,2024-11-29T08:33:53,классный обзор на классную статью
371,2024-12-01T08:49:39,"pushing the limits of large language model quantization via the linearity theorem статья pr в transformers sota векторные методы квантизации, такие как aqlm pv , quip , qtip, способны достигать умеренной просадки качества при сжатии в 2-3 бита, и почти без просадки смотря на чем замерять в 4 битах. однако, деквантизация в данных подходах довольно ресурсоемкая, что ограничивает предельно возможную скорость инференса. кроме того, сами методы требуют некоторых приседаний и вычислительных затрат, чтобы произвести на свет сжатую модель. и требуют калибровочных данных для оценки распределения данных, что может давать некий сдвиг. и коллеги galqiwi и black_samorez, совместно с корешами из ist, kaust и mit предложили новый, data-free в большинстве сценариев метод квантизации с переменной битностью слоев, который дает одновременно и хорошее качество в 3-4 бита и значительное ускорение. метод state-of-the-art методы квантизации и прунинга для llm по большей части оптимизируют ошибку на выходе линейного слоя, так как прямой учет лосса слишком затратен на масштабе. но в конечном итоге хочется-то не просесть в качестве генерации, а связь между послойной ошибкой и ошибкой на выходе модели не очевидна. и авторы обнаруживают, что при не слишком сильном сжатии есть линейная взаимосвязь между конечной ошибкой перплексией , и относительной ошибкой на выходе слоя. то есть перплексия сжатой модели есть перплексия исходной некоторая линейная комбинация послойных ошибок сжатия. но сама ошибка может существенно варьироваться от слоя к слою и зависит от распределения значений в матрицах весов. кроме того, хочется иметь data-free метод квантизации, а наивный rtn просаживает качество довольно сильно при сжатии в 4 и и ниже бит. потому авторы применяют знакомые давним подписчикам канала адамаровы вращения, которые убивают выбросы в весах и приводят распределение весов к нормальному i.i.d. а для нормального распределения существуют рассчитанные численно оптимальные решетки. потому метод зовется hadamard incoherence and gaussian mse-optimal grids higgs . причем рассматривается случай как скалярной, так и векторной квантизации. векторная квантизация с 2-мерными кодбуками дает лучшее качество, чем скалярная, и при этом эффективно реализуется кернелами из библиотеки flute. налагая на данный вес случайный шум и смотря на изменение перплексии, можно оценить коэффициент пропорциональности между послойной ошибкой и перплексией на выходе. далее решением задачи линейного программирования при заданной степени сжатия определяет отпимальное распределение уровней сжатия среди несколько возможных вариантов. таким образом, задача нахождения оптимальных квантизованных весов сводится к нахождению коэффициентов важности каждого слоя и оптимальной квантизации для нормально распределенных i.i.d. весов."
372,2024-12-01T08:50:32,"эксперименты метод валидируют в традиционном сетапе для сжатия llm на llama-3.1, llama-3.2 и qwen. по качеству предложенный подход заметно опережает data-free af, nf, hqq особенно при сжатии в 3.25 бит даже при однородном сжатии. чем больше размерность векторов в квантизацианной решетке - тем лучше качество, но p 2 2-мерная оптимальна точки зрения баланса между скоростью и качеством. неоднородная квантизация неплохо накидывает по сравнению с однородной. higgs квантизация с flute кернелами гораздо быстрее на rtx 4090 навороченных векторных квантизаций aqlm, quip , qtip и даже быстрее, чем marlin кернел для батчового инференса для скалярной однородной квантизации . по метрикам метод превосходит gptq awq и немного уступает sota векторным квантизациям. но просадка компенсируется куда большей производительностью с точки зрения практической привлекательности. линейная модель работает достаточно точно вплоть до сжатия в 2.5-3 бита. выводы классный результат от коллег. ждем одобрения pr в transformers для выкатки на широкую публику. интересно было бы еще протестировать на диффузионках а-ля flux."
373,2024-12-03T11:41:36,"switti designing scale-wise transformers for text-to-image synthesis статья код сайт проекта демка введение на текущий момент диффузионные модели уверенно занимают пьедестал почета в задаче генерации изображений по тексту. по заданному запросу, даже весьма нетривиальному, насыщенному нюансами и деталями, они способны генерировать разнообразные картинки хорошего качества. однако, существенным и основным недостатком диффузионных моделей является их итеративная природа генерации чтобы сгенерировать одно изображение, диффузионную модель приходится прогонять много раз, из-за чего приходится подождать некоторое время, прежде чем замечательная картинка явится на свет. наряду с диффузией существует альтернативная парадигма генерации - так называемые авторегрессионные модели, которые генерируют изображения последовательно патч за патчом патч - маленький кусок изображения, скажем, 16x16 пикселей . однако, они работают на практике еще медленнее для больших изображений, так как генерация каждого следующего патча требует прогона модели, а количество патчей может переваливать за тысячу. кроме того, они уступают в качестве диффузионным моделям, поэтому долгое время считались неконкурентоспособными. однако, этой весной команда исследователей из bytedance в работе var visual autoregressive modeling scalable image generation via next-scale prediction предложила модификацию авторегрессионной парадигмы, где за один проход предсказывается не один патч, а все разрешение целиком. с помощью специальной аамодели - residual quantization vae вариационно кодировщика с остаточной квантизацией - изображение разбивается на разные уровни нижние уровни соответствуют общей семантике изображения, а верхние уровни - тонким деталям и текстурам. на нижних уровнях немного патчей, поэтому прогон модели на них дешев, и стоимость прогона возрастает с переходом на каждое следующее разрешение. на этапе генерации модель смотрит на все прошлые разрешения и генерируют текущее. полученная картинка получается посредством суммирования всех разрешений. данная работа смогла добиться качества генерации, сопоставимого с хорошими современными диффузионными моделями, при этом будучи значительно быстрее их, в задаче генерации, обусловленной на класс объекта из imagenet. увы, генерация из фиксированного набора 1000 классов не так интересна пользователям, как генерация по произвольным и разнообразным текстовым запросам. поэтому для верификации жизнеспособности идеи последовательной генерации изображений по разрешениям требовалась проверка в более сложном и интересном сценарии. некоторое время спустя после выхода работы var вышли star и hart, которые адаптировали вышеупомянутый подход для генерации изображений по тексту. в этих работах удалось добиться сносного качества генерации и следования текстовому запросу, но все же далеко позади современных диффузионных генеративных моделей таких, как sdxl, pixart, lumina . поэтому мы, исследователи из yandex research, решили обучить свою генеративную text-2-image модель."
374,2024-12-03T11:44:39,"метод за основу мы взяли архитектуру модели из star и последовательно анализируя и улучшая ее пришли к конечной модели. первое, с чем мы столкнулись, это с тем, что оригинальная архитектура становилась крайне нестабильной в процессе обучения. внутренние активации модели вырастали до очень больших значений, которые уже нельзя было представить в машинной точности, и обучение разваливалось. следуя работе lumina, мы добавили дополнительные нормализационные слои в модель, и данная архитектурная модификация стабилизировала обучение. кроме того, качество самой модели тоже улучшилось. далее, мы проанализировали карты внимания авторегрессионной модели, и обнаружили, что текущее разрешение почти не смотрит на прошлые, поэтому разрешения можно генерировать независимо друг от друга. убрав авторегрессию на прошлые разрешения мы нисколько не потеряли в качестве, и при этом ускорили модель примерно на 20-30 . и последняя ключевая находка оказалась в том, что технику classifier-free-guidance cfg , улучшающую качество генераций и соответствие текстовому запросу, но требующую два прогона через модель вместо одного, можно отключить на высоких разрешениях без ухудшения конечного результата. за счет этого, можно добиться почти двухкратного ускорения по сравнению со стандартной процедурой генерации с cfg. полученную модель мы назвали switti - scale-wise transformer for text-to-image synthesis, так как она генерирует изображение по тексту разрешение за разрешением."
375,2024-12-03T11:48:43,"эксперименты и результаты мы обучили нашу модель на внутреннем большом датасете из множества картинок 100m . в качестве бейзлайнов мы берем stable diffusion xl, и ее ускоренные версии - sdxl-turbo, sdxl-dmd2, sd3-medium, lumina-next, а так же современные авторегрессионные модели llamagen и упомянутый hart. для оценки качества моделей мы использовали принятые и стандартные в литературе метрики fid, clip, pickscore, image reward, все знают, что они , но этикет же надо соблюдать а также пользовательские предпочтения на корзинке из 128 запросов parti prompts . пользователи оценивали следующие аспекты изображения релевантность - соответствие текстовому запросу эстетичность - общая красивость изображения комплексность - количество деталей и сложность композиции дефектность - отсутствие дефектов и артефактов в изображении switti по качеству значительно превосходит существующие авторегрессионные подходы, как по метрикам, так и пользовательским предпочтениям. с диффузионными моделями мы добились паритета по качеству, но при этом switti генерирует в 7 раз быстрее оригинальной sdxl-модели, и 2 раза быстрее ускоренных версий - sdxl-turbo, sdxl-dmd2. кроме того, мы обнаружили, что если по ходу генерации подменить текстовый запрос, можно получить нечто среднее. например, подав в запрос изначально ведьмака, а затем подменив в середине генерации запрос на робота, вы можете получить некоего киборга-ведьмака. или подав изначально зимний пейзаж, а подменив его на какой-то стадии летним можно получить разную градацию перехода от зимы к лету. вывод в данной работе нашей команде удалось сделать генерирующую на уровне разрешений модель, которая смотрелась бы не блекло и безнадежно на фоне диффузионных моделей. кроме того, switti генерирует быстро, что делает ее привлекательной для приложений где требуется сгенерировать много изображений за разумное время. тем не менее, есть еще куда расти. на текущий момент switti генерирует только в 512x512, и до нынешней sota flux, recraft, ideogram v2, midjourney 6.1 , еще очень далеко. но диффузионные модели уже давно полируются и улучшаются, а var-inspired парадигма зародилась совсем недавно . и есть еще большой потенциал для роста ."
376,2024-12-03T11:59:19,и картиночки
377,2024-12-04T11:09:01,"в процессе серфинга по тытрубе и подготовке к собственной защите наткнулся на защиту кандидатской диссертации егор захарова в сколтехе. тема диссертации - синтез человеческих лиц и 3д фигур людей с помощью ganов. и данный рассказ - действительно очень интересный и увлекательный обзор техник по улучшению качества генерации, реализма, общаемости на произвольные аспекты и положения головы и тела. картинки просто конфетка и сам рассказ очень увлекательный и познавательный. нынче диффузия - всему голова, но многие идеи актуальны и до сих пор. в общем, рекомендую."
378,2024-12-06T18:46:19,"хорошо мы успели залететь авторы var без первого, c которым вышла презанятная история выкатили свою text-2-image модель под названием infinity. обзорчик появится немного позднее"
379,2024-12-07T11:05:21,"infinity scaling bitwise autoregressive modeling for high-resolution image synthesis статья пока только ридми и картинки только-только мы успели выпустить switti, как создатели var, опубликовали собственную text-2-image модель, позиционирующую себя так же как конкурент моделей уровня sdxl sd3 medium. метод наиболее примечателен токенизатор. в оригинальной статье по var использовался residual vae c общей кодовой книгой на все масштабы. выход энкодера на данном масштабе заменяется на ближайший вектор из кодовой книги. на этом этапе возникает некоторая ошибка аппроксимации, и, по всей видимости, в это кроется причина, по которой vq-vae традиционно уступают непрерывным аналогам по качеству реконструкции. чем больше кодовая книга - тем потенциально меньше ошибка квантизации, но огромные кодовые книги скажем, с 2 64 векторами не влезут в память никакой машины. потому авторы предлагают параметризовать кодбуки бинарными векторами специфичными для каждого масштаба k , и квантизация осуществляется просто взятием знака от непрерывного вектора z_k, соответствующему уровню k в иерархии c домножением на некоторый коэффициент. рассматривают две опции - lfq, bsq, отличающиеся на коэффициент 1 sqrt d и берут в итоге второй вариант, так для него существует эффективное выражение для вычисления энтропийной регуляризации используемой для более эффективного использования кодбука . благодаря такой бинарной квантизации можно расход памяти, требуемый на кодбук размера 2 d, уменьшается с o 2 d , до o d , благодаря чему, можно хранить колоссальные кодовые книги. кроме того, автокодировщик учат быть устойчивым к ошибкам предсказания токенов и во время обучения случайным образом подменяют некоторую долю токенов. здесь замечу, что в экспериментах по switti мы обнаружили, что можно менять довольно значительную долю токенов без изменения выхода модели, и модель оказывается устойчивой к этому без манипуляций. дабы поддерживать разные aspect ratio и размеры используют факторизованные 2d rope позиционные эмбеды. в качестве текстового энкодера используют flan-t5. обусловливание на текст осуществляется как через self-attention, за счет добавления токенов промпта в prefix, так и cross attention между картиночными и текстовыми токенами."
380,2024-12-07T11:06:51,"эксперименты и результаты данные для обучения собраны из отфильтрованных laion, coyo, openimages датасетов. сначала учат на 256x256 разрешении, потом на 512x512 и в конце переходят на 1024x1024. токенизатор, как и в оригинальной статье, осуществляет 16x уменьшение по пространству. качество оценивают по fid на некотором внутреннем датасете из 40к изображений вместо стандартного ms coco. кроме того, замеряют качество на geneval и dpg, и предпочтения пользователей на imagereward hpsv2.1 по аспектам релевантности prompt following и эстетичности visual aesthetics . на бенчмарках infinity уверенно побеждает все прошлые авторегрессионные модели switti в сравнениях нет ввиду очень малого промежутка времени между выходом моделей . опережают по качеству и диффузионки sdxl sd3-medium pixart-sigma. модель хорошо умеет в рендеринг текста, если верить черрипикам. в ablations авторы показывают, что огромные кодбуки важны для качественной реконструкции, и самые большие кодбуки даже лучше continuous vae. обучение с зашумлением заметно улучшает fid и немного другие метрики. 30 зашумление оптимально, согласно заявлениям авторов. модель генерирует быстро - одно изображение в разрешении 1024x1024 за 0.8 секунд против 2.1 и 2.7 у sd3 medium и sdxl, соответственно. вывод годное подтверждение жизнеспособности scale-wise парадигмы. ключевой вклад данной работы сильно улучшенный токенизатор который и является основным ограничителем качества в случае switti . умная токенизация в картиночных и видео моделях вообще очень горячая тема во второй половине текущего года. ждем с нетерпением релиза моделей, чтобы поиграться и забрать к себе наработки"
382,2024-12-09T11:32:19,"собственноручно проверил наличие супервеса см. оригинальную статью и разбор от gonzo-обзоры ml статей в llama-3.2-1b. aномальный вес находится в позиции 400 - выходной канал, 1417 - входной канал в model.layers.1.mlp.down_proj. не столь ярко выражен перплексия на wikitext-2 8k context length выросла с 8.375 до 8.625 при занулении данного веса , но все же очень много для всего одно веса. google colab для желающих поиграться"
383,2024-12-15T19:33:45,"на днях наткнулся на канал в youtube некоего simon oz. парень доступно, с красивыми визуализациями в стиле 3blue1brown рассказывает про всякие темы из теории информации и особенности программирования на cuda. в частности, особого внимания заслуживает видос про то, как написать эффективный kernel для softmax, который быстрее реализаций в торче и на тритоне. он пошагово анализирует узкие места, нюансы железа и алгоритма, и постепенно добивается улучшения производительности 1 эффективный алгоритм редукции для нахождения максимума 2 оптимизации доступов к памяти coalescing 3 перенос части операций из shared memory в регистры gpu которые еще быстрее 4 векторизация операций через float4 5 однократная подгрузка данных для подсчета максимума и экспоненты вместо двухкратной красивое..."
384,2024-12-16T19:27:57,"отныне, присно и во веки веков ведущий данного канала не просто балабол, а заслуженный балабол! спасибо сколтеху, всем товарищам, коллегам и соавторам за эти замечательные и продуктивные 3 года, путь полный трудностей, открытий и приключений это была славная охота. сам текст диссертации интересующиеся могут найти здесь."
385,2024-12-16T19:34:48,"о, это подарок в честь присуждения степени?"
386,2024-12-20T16:53:55,"это математика с подвохом! а вы тоже думали, что прибавление нуля к числу никогда ничего не меняет?"
387,2024-12-20T20:08:21,"prefixquant static quantization beats dynamic through prefixed outliers in llms статья код введение для получения максимального ускорения, особенно при инференсе большими глотками батчами, нужно квантизовать не только веса, но и активации. однако, богомерзкие выбросы сильно затрудняют квантизацию, и в прошлых работах quarot, spinquant, duquant было предложено применять адамаровы матрицы или некоторые обучаемые , которые размазывают выбросы по каналам. однако, выбросы встречают не только в отдельных каналах, но есть и токены, например bos и иные разделители, отличающиеся большими значениями активаций, и их квантизация большими группами per-tensor с максимальным ускорением приводит к большим ошибкам. поэтому в упонятых выше методах использовали per-token квантизацию с некоторым замедлением инференса. в этой статье авторы предлагают вынести аутлаерные токены в префикс последовательности дабы упростить квантизацию. метод сам подход мотивирован идеями про attention sinks и quantizable transformers removing outliers by helping attention heads do nothing. иногда трансформерному блоку не нужно ничего делать, и дабы реализовать почти тождественную операцию, приходится выбрасывать весь attention в токены с низкой семантикой. потому авторы предлагают просто найти все эти токены и положить в префикс последовательности. токены находят на основе некоторой калибровочной выборки смотря на выход down_proj в mlp. отбирают токены, чья l_ infty норма сильно больше медианной по последовательности. сам процесс достаточно быстрый - занимает всего несколько минут. таких токенов оказывается от 1 до 4 в рассмотренных моделях llama-2,3, qwen, mistral . обычно это bos , и еще символы переноса строки n, артикли и знаки препинания. данная модификация позволяет существенно уменьшить норму последующих токенов и упростить процесс квантизации. для максимизации качества в конце еще предлагается делать поблочный qat, следуя efficientqat. эксперименты метод валидидируют на -2,3 в режимах w8a8kv8, w4a8kv4, and w4a4kv4 квантизации w - точность весов, a - активаций, kv - kv-кэшей . веса квантизуются поканально, кэши поголовно, активации потензорно. предложенный подход оказывается примерно на одном уровне с per-token квантизациями, чуть уступая лишь spinquant с дообучением матриц вращения каналов. при этом скорость инференса на 20-30 быстрее. поблочный файнтьюн немного накидывает. вывод простой и полезный трюк для квантизации. тем не менее, кажется, что возможность делать ничего в блоке модели стоит закладывать изначально, и скорее всего, ввиду потребности в низко-битных моделях, сообщество до этого дойдет через softmax 1 или иным образом ."
388,2024-12-20T21:52:03,"closedai назвали новую модельку o3, дабы обойти коллизию имен с o2 - британским оператором, но не учли, что o3 - это озон"
389,2024-12-25T11:01:28,"give me bf16 or give me death ? accuracy-performance trade-offs in llm quantization статья нет кода введение квантизаций за последние годы вышло мерено-немерено разного качества и степени сжатия. в идеале, конечно же, хочется не просесть по качеству, и при этом добиться снижения издержек на инференс llm и ускорения генерации текста. академические бенчмарки часто дают чрезмерно оптимистичную оценку, не всегда согласующуюся с пользовательским опытом на практике. в данной работе, авторы исследуют несколько конфигураций квантизации в режиме малой просадки качества и выводят набор практический предписаний в зависимости от задачи и приложения. метод эксперименты результаты авторы рассматривают 3 варианта квантизации 1 w8a8-fp поддерживаемый только на hopper ada lovelace 2 w8a8-int 3 w4a16 в первом случае веса сжимают просто через rtn квантизацию, в остальных применяют оптимизированный gptq с поиском оптимальных скейлов с калибровкой на openplatypus. в случае 4-битной квантизации весов сравнивают еще и с awq. замеряют качество по трем сериям задач на llama-3.1 8b, 70b, 405b 1 open llm leaderboard v1 2 open llm leaderboard v2 3 и так называемые real-world задачи - arena-hard, humaneval, humaneval на open llm leaderboard v1 все методы в пределах 99 качества исходной модели, за исключением задачи trurthfulqa, где наблюдается просадка до 3 . на open llm leaderboard v2 имеет тоже место почти полное восстановление с наиболее заметной просадкой на bigbenchhard. на arenahard, humaneval, humaneval тоже все хорошо. awq бывает немного лучше на open llm leaderboard, но заметно уступает на real-world задачах. далее смотрят на похожесть генераций на arena-hard у исходной модели через rouge-1, rouge-l и bertscore. rouge порядка 0.6-0.7, но по всей видимости, это означает большую похожесть . для оценки скорости рассматривают разные сценарии 1 длинный промпт короткий выход 2 промпт и выход примерно одной длины 3 короткий промпт длинный выход инференс гоняют на vllm, как проверенном и быстром движке. замеры гоняют на a6000, a100, h100. рассматривают сценарий сихронной и асинхронной подается батч запросов генерации. в синхронном случае почти всегда быстрее всего работает w4a16 квантизация, кроме длинного промпта compute-bound режим и инференса h100, где предпочтительнее fp8, int8 w8a8 квантизация. в асинхронном сценарии int4 предпочтительнее на более low-end девайсах, а fp8 при инференсе на h100. вывод квантовать модели полезно?"
390,2024-12-27T13:40:47,"наткнулся на репозиторий, красиво и наглядно иллюстрирующий mode collapse для различных формулировок ganов на 2-мерных игрушечных задачах."
391,2024-12-28T23:24:40,"diffusion meets flow matching two sides of the same coin в начале декабря группа чуваков из глубокого разума, среди коих признанные аксакалы, как hoogeboom, de bortoli и salimans опубликовала презанятнейший пост diffusion meets flow matching two sides of the same coin. нынче стало модно учить диффузионки в flow matching постановке. тренд, по всей видимости, был задан sd3. и большинство нынешней sota в картиночной и видео генерации из того, что известно flux, moviegen, hunyuanvideo. и что это значит? классическая парадигма - пережиток истории ? ан нет. в данном блогпосте авторы в деталях анализируют процесс сэмплирования и обучения в стандартной noise-prediction variance preserving ve диффузионной постановке и flow matching, и показывают, что по сути обе сущности про одно и то же. основная разница в коэффициентах при шуме сигнале и использовании скорости в качестве выхода нейронной сети вместо шума x0. и по ходу повествования эквивалентность двух парадигм авторы иллюстрируют с разных сторон. сам блогпост содержит красивые иллюстративные визуализации с ползунками . кроме того, авторы опровергают распространенное мнение, что flow matching дает непременно более прямые траектории, чем диффузия. для узких распределений flow matching действительно дает более прямые траектории, чем типичный диффузионный процесс, но для широких распределений все может поменяться с точностью до наоборот. впрочем, для наиболее типичного сценария text-2-image генерации или редактирования изображения, целевое распределение, по всей видимости, достаточно узкое."
392,2024-12-30T11:40:13,"sltrain a sparse plus low-rank approach for parameter and memory efficient pretraining статья код введение низкоранговые приближения хорошо себя зарекомендовали при дообучении моделей, но их выразительности не хватает при обучении модели с нуля. ранее было предложено по ходу обучения вливать lora в веса модели статья relora и инициализировать низкоранговую добавку, или накладывать низкоранговость на градиенты и состояния оптимизатора galore . но в первом случае, для получения качества, близкому к бейзлайну был необходим этап обучения всей модели целиком. возникает вопрос возможно ли добиться хорошего качества, имея всегда малое подмножество обучаемых параметров? и авторы предлагают обучать low-rank sparse компоненту. метод идея, естесна, далеко не нова. по существу, это своего рода robustpca. для дообучения llm ранее подобное реализовали в rosa. авторы мотивируют использование sparse компоненты в дополнение к low-rank, тем что она обладает несколько иными свойствами при не слишком малой sparsity матрица почти всегда полноранговая и вместе они могут более гибко фитировать исходные матрицы весов. сэмплируется рандомная sparse маска и фиксируется по ходу обучения. sparse матрица прибавляется к low-rank компоненте, потому sparse matrix операции не требуются при такой постановке, а для получения градиента достаточно проиндексировать градиент по всему весу. эксперименты авторы обучают семейство моделей с архитектурой llama от 60m до 7b параметров на с4 корпусе на бюджете в несколько миллиардов токенов. в качестве бейзлайнов выступают 1 full-rank обучение 2 low-rank training 3 relora 4 galore качество оценивают по валидационной перплексии. sltrain по качеству чуть лучше relora и galore, несколько экономичнее по памяти galore, и примерно такой же по скорости. рассматривают разные конфигуации sparsity low-rank. 1-3 ненулевых весов в sparse компоненте более менее оптимально. анализируя спектр выученных весов, они показывают, что первые собственные значения full-rank обученных матриц соотвествуют low-rank компоненте, а последующие - sparse компоненте. то есть, сумма low-rank sparse более точно описывает спектр при полном обучении. вывод идея разумная и логичная, но непонятно, насколько идея масштабируется на более практически интересные сценарии. отсутствуют даже замеры 0-шотов на lm-eval по всей видимости, при таких бюджетах обучения они мало будут отличаться от random guess ."
393,2024-12-31T16:38:44,"прошедший год был насыщенным на события и прогресс в области ии, глубокого обучения, машинки и разнообразных приложений. ключевые моменты и достижения области за 2 0 2 4 превосходно отметил у себя на канале григорий сапунов https t.me gonzo_ml 3175 . со своей стороны могу лишь добавить, что уходяший год, был интересным и примечательным в том числе и точки зрения техник сжатия и ускорения моделей появились 2 -битные квантизации, которые не приводят llm в полную негодность. aqlm, quip , pv-tuning, qtip спекулятивный декодинг подарил ряд интересных работ до коих у вашего покорного слуги не дошли руки на разбор, но в следующем году планирую наверстать упущенное . ряд интересных решений по сжатию активаций и kv-кэшей. в связи с запросом научного сообщества, энтузиастов и простых пользователей на эффективный инференс, полагаю, что и в следующем году мы увидим немало интересного. и в особенности, значительные усилия будут потрачены на удешевление цепочек рассуждений а-ля o3. спасибо всем присутствующим здесь кроме nft-ботов за то, что вы здесь, за вашу поддержку и комментарии. будем стараться и дальше делать полезный и, надеюсь, интересный контент. быть добру!"
395,2025-01-02T20:47:49,"noise_step training in 1.58b with no gradient memory манускрипт репозиторий введение первый пост данного года будет несколько комедийного содержания, как раз в самый раз для прочтения после нескольких бокалов шампанского или чего покрепче . некто уилл брикнер выложил на гитхаб презанятнейший опус про обучение тернарной сети в 1.58 бит без необходимости выделения памяти на градиенты и состояния оптимизатора. метод товарищи из мелкософта в серии работ про bitnet показали, что обучая сеть с тернарными весами принимающими значения только -1, 0, 1 и умноженными на некий скаляр , и низкобитными активациями 4 8 бит можно выжать качество, сравнимое с fp обучением при тех же бюджетах обучения. однако, во время само обучения приходится хранить floating-point веса, и состояния оптимизатора, как для fp модели. то есть обучение все равно требует значительных затрат памяти. автор данного опуса, вспоминая статью gradients without backpropagation, замечают, что операция умножения якобиана по выходу модели на фиксированный вектор не требует backpropagation. потому предлагается делать случайные возмущения, причем для случая тернарных весов возмущения это -1, 0, 1. для улучшения сходимости предлагается отбрасывать слишком малые возмущения т.е своего рода прунить обновление . так как на практике мы используем псевдослучайные числа, то для параметризации модели достаточно хранить только случайные зерна со всех шагов оптимизации. и для обучения gpt-3, взяв данные из техрепорта тогда еще closedai еще не совсем closed , получают 100к шагов оптимизации, и всего несколько мегабайт на хранение 175b весов . а как вы будете эти сиды превращать в веса - это ваши проблемы эксперименты предложенный метод валидируют на 4-слойной mlp c hidden_size 256, и данный метод о, боже! даже сходится и выдает космические почти 90 качества на mnist. единственный недостаток всей этой красоты, в том, что авторы не релизнули эффективные кернелы для обучения и инференса. что ж поделать, не все познали дзен куды и тритона в том числе и пишущий сии строки . вывод это, наверное, самый забавный каламбур на моей памяти в данной области интересно, автор сам дошел до этого или воспользовался помощью всесильного оракула в виде llm. я в полном восхищении , в любом случае."
396,2025-01-05T16:10:11,"свободные рассуждения про оценку качества моделей земную жизнь пройдя до половины и очутившись в сумрачном лесу после двух с лишним лет прогона сжатых моделей на бенчах из lm-eval-harness я задался таки вопросом а что мы собственно замеряем таким образом? в нижеприведенных рассуждениях я не планирую погружаться в дебри дискуссий про то, что есть agi, а чем он не является, а лишь сугубо попытаться соотнести академические бенчмарки применению на практике. большинство бенчмарков относятся к одной из 2 категорий 1 likelihood запросы. есть вопрос, варианты ответа и тот, у которого правдоподобие максимальное выбирается в качестве предсказания модели и сопоставляется с правильным. 2 генеративные запросы. на основе некоторого промпта и инструкции модель генерирует ответ. далее происходит парсинг и то, что удалось вычленить сопоставляется с тем, что надо. likelihood запросы к первой категории относится, пожалуй, большинство задач arceasy, arcchallenge не тот arc! , hellaswag, winogrande, mmlu, mmlu-pro и многие другие . достоинством такого подхода является дешевизна прогона, так как по сути достаточно одного прогона модели для получения вероятностей токенов ответа. при этом общий префикс можно переиспользовать для разных вариантов, т.к запрос имеет вид условие вариант_ответа где вариант ответа - обычно одно слово, а то и одна буква в случае mmlu . кроме того, проверить или истинность ответа можно однозначно. существенным недостатком данного подхода же является неочевидная связь между умением модели справляться с данными задачами и генерацией текста в свободной форме. например, в mmlu шаблон имеет следующий вид условие вариант ответа - a вариант ответа - b вариант ответа - c вариант ответа - d правильный ответ a, b, c, d то есть бенчмарк проверяет то, насколько хорошо по контексту модель может угадать букву a, b, c, d. из этого сложно сделать вывод, насколько адекватно она будет писать ответ в свободной форме на те же самые вопросы. и результат во многом будет определяться тем сколько теста оказалось в трейне , насколько модель умеет вписываться в шаблон подобного вида. генеративные запросы данный вид задач gsm-8k, bigbenchhard, ifeval, arenahard гораздо ближе к реальным приложениям, так по существу представляет ту же самую авторегрессионную генерацию. основная сложность - в оценке ответов модели. в случае gsm8k ifeval определены некоторые регулярные выражения, которые вычленяют ответ скажем, решение математической задачи или выполнена ли требуемая инструкция , но ввиду высокой вариативности возможных ответов не всегда можно гарантировать обнаружение правильного ответа. в alpacaeval, arenahard судьей выступает другая llm , но здесь приходится полагаться на качество судьи который не совершенен и имеет свои biasы, нюансы и предпочтения при оценке ответа. кроме того, замеры стоят денег и в конце концов - lmsys arena и иные side-by-side comparison , где качество оценивают уже человеки. такая стратегия оценивает широкий спектр способностей модели, и вроде бы ориентирована на конечного пользователя. но таким образом можно оценивать уже конечную модель, а для промежуточных экспериментов выходит чрезмерно накладно. кроме того, даже lmsys хакается ввиду предпочтений пользователей к более длинными ответам, удовлетвоояющим некоторому формату. вывод оценка качества моделей - сложный вопрос, и цифры на бенчах могут служить лишь первым приближением при выборе llmки для своих нужд. окончательный выбор стоит делать исходя из целевой задачи, протестировав самому на релевантных примерах. а в идеале собрать собственный бенч и регулярно его обновлять. рекомендую отличный пост от игоря котенкова на данную тему."
397,2025-01-09T12:06:02,"can llms write better code if you keep asking them to write better code ? блогпост github забавный и поучительный блогпост про то, как llmка claude 3.5 sonnet итеративно оптимизировала код под запросами вида write better code . постановка задачи следующая. требуется написать код на питоне для задачи а-ля leetcode write python code to solve this problem given a list of 1 million random integers between 1 and 100,000, find the difference between the smallest and the largest numbers whose digits sum up to 30. исходное решение рабочий, но не самый эффективный код. подсчет цифр в числе реализован через приведение его к строке, парсинг строки на цифры с последующим их суммированнием. итерация 1 claude замечает, что возможных вариантов чисел меньше, чем всего чисел и предпосчитывает сумму цифр для всех чисел от 1 до 100,000 и складывает в byte array. кроме того, llmка навела немного синтаксического сахара и обернула код в классы и методы. ускорение 2.7x по сравнению с бейзлайном. итерация 2 claude кладет сумму цифр в numpy array что дает векторизацию подсчета сумм и дополнительно использует concurrent-futures для параллелизации. ускорение 5.1x по сравнению с бейзлайном. однако изначальное решение было с багами и его пришлось немного фиксить. итерация 3 небольшой и бесполезный рефактор, который замедлил код. ускорение 4.1x по сравнению с бейзлайном. итерация 4 claude предложил использовать numba с jit-compiler и параллелизмом и asyncio для многопоточности. ускорение 99.7x по сравнению с бейзлайном. далее автор изначально подает промпт, просящий оптимизировать все что только можно и нельзя предлагая даже модельке . модель сходу выдает 59x ускорение благодаря numba jit но без parallel true . и промпт требующий улучшения кода уже более подробный your code is not fully optimized, and you have been fined 100. make it more optimized. но claude обиделся, и код стал медленее 9.1x ускорение и с багами возможно, привык что его обманывают на деньги . несколько последующих итераций выдали версии с ускорением 65x-99.7x и с косяками. т.е превзойти прямолинейную стратегию не удалось. вывод топовые llm - мощные ассистенты для программиста, но за ними пока требуется глаз да глаз и экспертное мнение. кожаные мешки все еще нужны. пока..."
398,2025-01-10T23:43:03,"llm kv cache compression made easy репозиторий в связи с ростом потребности в эффективной по памяти и скорости работе с длинным контекстом особенно с учетом набирающего популярность test-time compute scaling все острее стоит вопрос сжатия kv-кэшей. данной теме уже посвящено немалое число работ и существует уже интеграция в transformers . и недавно ребята из одной зеленой компании выкатили либу, с реализацией разных техник сжатия kv-кэшей под названием kvpress. в данной либе реализовано несколько простых и популярных техник сжатия кэшей случайный прунинг основанный на нормах токенов несколько подходов, основанных на attention скорах snapkv, tovapress, h20 причем битность можно задавать послойно при помощи класса perlayercompressionpress. самую sota например, pyramidkv в области пока еще не завезли, увы. в репозитории есть ноутбуки с демострацией использования библиотеки и замерами скорости и памяти. либа действительно удобная и приятная для использования. методы сжатия кэшей можно комбинировать с квантизацией кэшей у лицехватов ."
399,2025-01-12T20:28:33,"kivi a tuning-free asymmetric 2bit quantization for kv cache статья код для того, чтобы работать с большим контекстом в условии ограниченных ресурсов необходимо как-нибудь да сжимать kv-кэши, и квантизация одно их первых, что приходит в голову. группа исследователей с забавным примечанием the order of authors is determined by flipping a coin из разных мест на послендем icml представила статью со звучным названием kivi терминал оплаты, фрукт и птица пишутся по-другому . метод предыдущие методы квантизации кэшей обычно квантизовали кэши и ключи и потокенно, так как статистики scale и zero при таком подходе можно пересчитывать на лету. для 4-битной квантизации такой подход работает хорошо, но при 2-х битных кэшах модель становится полностью негодной. дабы эффективно квантизовать кэши, авторы анализируют распределение активаций в kv-кэшах и обнаруживают, что в активациях k-проекций имеют место ярко выраженные выбросы, в то время как в v паттерн хаотический. перебирая варианты с потокенной и поканальной квантизацией, исследователи приходят к выводу, что ключи лучше всего квантизовать поканально, а values - потокенно. при поканальной квантизации придется пересчитывать значения всех прошлых кэшей при пересчете диапазона. потому предлагается хранить неквантизуемый буфер из самых свежим токенов размера 128 и, как только буфер наполнится, квантизовать данную группу, и начать следующий. для длинных контекстов накладные расходы от такого буфера небольшие. эксперименты метод валидируют на бенчах из lm-eval, longbench наборе задач на длинный контекст и needle-in-a-haystack где нужно достать вытащить нужный факт из длинного промпта на моделях llama-2, mistral. наивная квантизация в 2 бита сильно страдает по качеству, а предложенный подход работает с совсем небольшой просадкой. есть, правда, маленький нюанс, что 2-битная квантизация использует размер группы 32 с fp zero-point, что означает на практике 3 бита . needle-in-a-haystack решается почти идеально. поддержание неквантизованного буфера свежим токенов важно для лучше качества свежие токены влияют сильнее, чем далекие, в большинстве задач . при работе с большими контекстами доминирующим фактором, определяющим скорость работы, при авторегрессивной генерации становится подгрузка кэшей уже не весов модели . за счет их сжатия удается ускорить генерацию до 3.5 раз. вывод сжатие кэшей - богоугодное дело. а дабы эффективно квантизовать кэши полезно смотреть на распределение того - что сжимаешь."
401,2025-01-13T19:09:38,"есть ли способ в телеграме выбрать себе вид наказания ботами или обменяться с кем-то? если уж выбирать зло, я бы предпочел шлюхоботов вместо дурацких nft-ботов. зло есть зло. большее. меньшее. среднее... какая разница? зло трудно измерить, его границы размыты. и если мне придётся выбирать между одним злом и другим, я не стану выбирать вовсе. с геральт из ривии"
402,2025-01-18T00:18:50,"наткнулся на проект математические этюды где в формате красочных постов с визуализациями рассказывают про интересные факты и концепции из математики. с красивыми иллюстрациями, видосиками, ползунками. в частности, внимания заслуживают пост про модель пуанкаре геометрии лобачевского исчезающая клетка и числа фибонначи три геометрии сходства и различия жаль, что в те времена, когда я учился, такой красоты еще не было..."
403,2025-01-19T21:30:35,"kvquant towards 10 million context length llm inference with kv cache quantization статья код на месте и снова про квантизацию kv-кэшей. про это тему можно говорить бесконечно, но не буду утруждать себя, иначе вы потонете в потоке этой информации. примерно в одно время с kivi другая команда выпустила статью, которая так же целится в сохранение приемлемого качества при квантизации ниже 4-бит."
404,2025-01-19T21:32:42,"метод 1 первое наблюдение полностью идентично kivi - ключи обладают ярко выраженными выбросами, а значения - нет. потому ключи квантизуются поканально, значения - потокенно. 2 далее замечают, что rope перемешивает соседние каналы и делает выбросы и channel-specific поведение не столько ярко выраженным, потому предлагают квантизовать ключи до применения rope и каждый раз применять его уже к квантизованным кэшам. 3 однородная квантизация приятна и удобна в использовании, но обычно далека от оптимальной в плане ошибки. и не мудрствуя лукаво предлагают использовать 1-мерный k-means, но используя в качестве метрики не стандарную евклидову, а диагональ матрицы фишера, которую оценивают по маленькой выборке. 4 некоторые значения сильно выбиваются среди остальных, потому их предлагается не квантизовать, а держать в виде разреженной матрицы csr, csc . на практике берут 1 самых больших по модулю значений. 5 модели чувствительны к изменениям attention sinks, отличающихся большими нормами активаций, потому предлагается их не квантизовать - все равно их мало, и на общей битности не скажется. не квантизуют только первый токен. 6 динамическое определение диапазона значений на лету дорого, и для поканального квантования требует пересчета значений всех токенов. по этой причине предлагают калибровку констант квантизации для ключей проводить offline, на небольшой выборке, а потокенную квантизацию кэшей на лету. и для всей этой навороченной красоты пишут специальный кернел, что работало все с адекватной скоростью. эксперименты метод валидируют на перплексии на стандартной длине контекста на -1. - в 4 бита просадка порядка 0.01 ppl - в 3 бита просадка порядка 0.1 ppl - в 2 бита - десятые затем метод проверяют на версиях llama-2, адаптированных под длинный контекст, где kvquant почти не просаживает метрики в 4 битах, и умеренно в 3 бита. на passkey retrieval метод не просаживается по качеству, в отличие от kivi. далее kvquant прогоняют на longbench ruler где одна из задач - иголка в сене , вытащить нужный факт из далекого прошлого - популярных бенчах на длинный контекст. kvquant снова лучше kivi, на ruler kivi эффективно 3-битный заметно страдает 56 - 40 , но kvquant теряет 3 качества 56- 53 . метод совместим с квантизацией весов. так как авторы kvquant - авторы еще и squeezellm, то совмещают именно с этим подходом. квантизация кэшей почти не меняет метрики квантизованной модели. в ablation показывают, что все компоненты важны для успеха 1 выбор размерностей для квантизации 2 pre-rope квантизация 3 неоднородная квантизация 4 учет выбросов итоговое ускорение операций умножения в kv - 1.2x-1.7x не end-to-end latency . при 2-битной квантизации удается впихнуть на одну a100 1m токенов для llama-2-7b, и на 8 - 10m токенов. вывод метод композитный, солидный, заточенный под выбивание хорошего качества. однако сложность имеет свою цену - и в данном случае это достаточно дорогая деквантизация, из-за которой ускорение инференса меньше, чем у конкуретных подходов того же kivi ."
406,2025-01-20T11:52:14,"model page deepseek выкатили пару часов назад на лицехватах веса deepseek-r1 в публичный доступ! напомню, что это reasoning модель, под цепоцки рассуждений а-ля o1, o1-mini, o3. в модели 685b параметров и веса выложены в fp8-e4m3. архитектура почти идентична deepseek-v3. так что, счастливые обладатели 8 1 h100, развлекайтесь на здоровье"
407,2025-01-26T21:49:27,"inference-time scaling for diffusion models beyond scaling denoising steps статья deepmind не часто публикует код введение данная статья уже появлялась на love. death. transformers и была разобрана у сиолошной . тем не менее, выскажу свое скромное мнение . inference-time scaling уже продемонстрировал впечатляющие результаты в контексте языковых моделей, где длинные цепочки рассуждений позволяют значительно улучшать качество на сложных задачах. у диффузионных моделей механизм улучшения качества генераций за счет большего объема вычислений есть из - выбор количества шагов сэмплирования. с ростом количества шагов расшумления качество полученных генераций и их соответствие запросу , но начиная с какого-то момента происходит насыщение, и дальнейшее повышение не приводит к значимым улучшениям, а иногда даже наоборот. поэтому в данной статье предлагают улучшать генерации за счет сэмплирования разных случайных шумов, начальных точек в процессе генерации, и выборе лучшего случайного зерна ."
408,2025-01-26T21:52:40,"метод в статье рассматривают две постановки 1 class-conditional генерация sit-b l xl какой-то трансформер с приблудами 2 -2- генерация c flux в данной статье исследуют разные стратегии отбора лучших сэмплов и модели для оценки качества. стратегии отбора 1 random search. просто сэмплируем независимо n кандидатов и берем лучшего с точки зрения модели-оценщика . 2 zero-order search. стартуя со случайного шума, сэмплируем несколько шумов в его окрестности. оцениваем их, находим лучший, и используем в качестве начальной точки на новой итерации. градиентная оптимизация требует проброса градиентов через всю цепочку сэмплирования, потому очень дорогая, и не очень хорошо работает, как показано в приложении 3 search over paths. сэмплируем несколько начальных шумов траекторий , и с некоторого уровня шума генерируем несколько конечных сэмплов. отбираем лучшие для каждой траектории, зашумляем до меньшего уровня шума и запускаем генерацию уже оттуда. в качестве верификаторов для оценки качества class-conditional генерации используют 1 inception score напрямую 2 clip где эмбеддят класс в промпт вида a photo of class 3 линейный классификатор поверх dinov2 при фиксированном достаточно большом числе шагов увеличивают количество случайных шумов. метрика inception score оценивающая точность распознавания сгенерированного изображения inception-v3 монотонно растет с увеличением количества сэмплов для supervised классификаторов ожидаемо сильнее . однако, fid тоже хреновая метрика, к слову , начиная с какого-то момента начинает расти т.е ухудшаться . по всей видимости, это связано с тем, что строгий отбор снижает разнообразие генераций и имеет место переобучение под верификаторы. в качестве альтернативы авторы предлагают self-supervised верификаторы - косинусную близость между логитами классификаторов x0-предсказания на малом уровне шума, и конечного сэмпла. и показывают, что она неплохо коррелирует с исходными классификаторами. метрика не самая интуитивная. предположительно, идея в том, что если сэмпл хороший получается, то на последнем участке генерации x0-предсказание слабо меняется. далее пробуют разные стратегии отбора, увеличивая число кандидатов. метрики монотонно растут, но будто бы результаты мало зависят от гиперпараметров каждого из вариантов - размера окрестности в случае zero-order search и числа траекторий для search over paths."
409,2025-01-26T22:00:17,"эксперименты для text-2-image генерации с flux в качестве верификаторов используют 1 классификатор эстетичности поверх openclip 2 clip score 3 imagereward 4 ансамбль всех трех с одинаковыми весами fluxом гененируют по умолчанию в 30 шагов сэмплирования, sit в 250. для оценки качества не доверяя субъективного мнению кожаных мешков используют gemini-1.5-flash, которая оценивает качество по 5 аспектам accuracy to prompt соответствие запросу originality оригинальность visual quality визуальное качество internal consistency внутренняя консистентность emotional resonance в душе не ебу, что это, возможно, степень шатания смотрящего от эмоций при взгляде на картинку для валидации используют drawbench и t2i-compbench. сначала исследуют взаимные корреляции между разными верификаторами на drawbench. эстетичность коррелирует с imagereward, но слегка понижает clip score. общее качество по мнению gemini немного растет. clip score повышает imagereward, но слегка понижает эстетичность. общее качество слегка улучшается. imagereward повышает clip score и оценку gemini более существенно, эстетичность почти не меняется. ансамбль всех трех метрик улучшает все три метрики. при увеличении количества сэмплов в ансамбле все метрики растут, но imagereward значительнее всего вероятно, из-за шкалы . на максимуме ставят суммарное число прогонов через модель 2880 96 сэмплов х 30 шагов . весьма забавно, что лучше всего себя показывает наивная стратегия независимой генерации случайных шумов, а остальные две, более заумные стратегии, не приносят пользы. на t2i-compbench, где оценивается соответствие промпту - взаимоотношения между объектами, формы, их положение, а не эстетичность, классификатор эстетичности слегка просаживает метрики, а clip score и imagereward улучшает. причем imagereward лучше ансамбля, по всей видимости, из-за наличия aesthetic score. предложенный метод работает и поверх dpo дообученной sdxl. при фиксированном бюджете если он мал для sit-xl выгоднее нагенерить несколько траекторий с меньшим количеством шагов, чем одну дорогую . но с ростом доступного бюджета становится полезным повышать число шагов сэмплера. кроме того, на малых бюджетах меньшие версии sit-b l, генерирующее нескольких кандидатов, лучше чем один прогон через большую sit-xl. выводы с идейной точки зрения - направление исследований довольно интересное, однако польза пока еще не столь очевидна, как для llm, где сложный reasoning процесс позволяет решать сложные задачи недоступные без него , а здесь же приводит к некоторому улучшению метрик. на текущий момент процесс слишком дорогой для большинства практических приложений ибо ждать несколько минут вместо нескольких секунд для создания немного более хорошей картинки не каждый готов, да и стоить сие будет у провайдеров недешево . тем не менее, подход может быть использован для генерации высококачественной синтетики."
411,2025-02-04T12:51:19,"visual autoregressive modeling for image super-resolution статья рыдми введение var и его производные в прошлом году произвели некоторый ажиотаж, показав вполне достойные результаты в задачах class-conditional и text-to-image генерации. ранее было показано, что дообучение text-2-image диффузионных моделей так называемый sr c generative prior на задачу blind super resolution с произвольными деградациями входных изображений неплохо себя показывает в случае сильно шакальных картинок. в этой же статье предложили по существу перенести данную идею на varы - а именно, дообучить class-conditional var на задачу super resolution. и не особо заморачиваясь с оригинальностью и звучностью назвали свое детище varsr ."
412,2025-02-04T12:52:27,"метод напомню, что в задаче blind super resolution мы хотим, с одной стороны, повысить разрешение, а с другой убрать шумы размытости и иные возможные дефекты. потому по самой постановке задачи, надо каким-то образом сохранить семантику входного изображения и при этом получить качественную картинку, с резкими деталями, сочными цветами и без дефектов. и дабы достичь поставленной цели авторы предлагают следующее 1 обусловливание на изображение низкого разрешения путем добавления ее эмбеддингов для чего обучают небольший энкодер в префикс последовательности. 2 чтобы лучше учитывать пространственные зависимости предлагают scale-align rotary positional encoding sa-rope , адаптивный под каждый масштаб. 3 квантованные vae заметно уступают в качестве реконструкций непрерывным аналогам. следуя mar https arxiv.org abs 2406.11838, обучают небольшую модель diffusion refiner из 6-ти трансформерных блоков, обусловленную на дискретные токены, для получения конечного изображения. 4 дабы улучшить качество генераций применяют classifier-free guidance c негативными примерами. для этого отбирают из публичных датасетов сэмплы с низкими оценками и на каждом шаге прогоняют с исходным условием, плохим условием и считают взвешенную сумму, как в стандартном cfg. причем cfg повышают постепенно с увеличением разрешения в диффузионных моделях его можно отключить в конце без ущерба для качества . имеющиеся в публичном доступе sr-датасеты невелики 1к-10к примеров , потому авторы собирают свой датасет из laion и прочих источников, выфильтровывают его до 4 миллионов картинок, которые классифицируют по 3к категориям пояснение будет чуть ниже ."
413,2025-02-04T12:57:39,"эксперименты процедура обучения выглядит следующим образом 1 сначала дообучают rq-vae на собранных данных, чтобы улучшить качество реконструкций. обучается только квантизатор, все остальное заморожено. 2 в качестве основы для varsr берут предобученный на imagenet class-conditional var из оригинальной статьи и дообучают на class-2-image генерацию на 3 классов на своем датасете. 3 после этого дообучают полученную модель на целевую image super resolution задачу. 4 при обучении на isr в качестве лосса используется взвешенная сумма кросс-энтропии по токенам и диффузионного лосса в рефайнере. полученную модель сравнивают с gan-based и диффузионными подходами на стандартных sr-бенчмарках div2k-val, realsr, drealsr . varsr и диффузия уступает gan моделям по классическим метрикам - psnr, ssim, но лучше по более свежим maniqa, сlip-iqa, musiq. в качественных визуализациях varsr более-менее реалистично восстанавливает изображения при жестких деградациях, в то время как альтернативные подходы заметно шакалят. в ablation study показывают, что все компоненты важны и накидывают качество обусловливание на lr изображение через prefix работает лучше, чем controlnet. sa-rope полезен. diffusion refiner улучшает метрики, хоть и не так сильно. image-based cfg понижает ssim, psnr, но заметно накидывает в современных нейросетевых метриках. по user-preference study varsr лучше ganов и диффузий с image prior. существенным достоинством var является скорость работы, за счет того, что выход формируется за один проход через все масштабы, большинство из которых почти бесплатные . стоит, однако, заметить, что шельмы не сравниваются с ganами и адверсариальными дистилляциями addsr диффузионных моделей, которые будут по факту быстрее. вывод вполне успешная адаптация var к задаче image sr c рядом нетривиальных архитектурных решений. однако, процедура обучения содержит слишком много компонент и промежуточных этапов. да и сравнение с дистиллами диффузионок для sr тактично опущено."
414,2025-02-04T23:00:19,"гугол выпустил занятную книженцию про обучение, профилирование и хорошие практики обучения больших языковых моделей на tpu. в частности, рассматриваются следующие вопросы основы анализа вычислений и оценки сложности архитектура и особенности tpu реализация параллельного обучения обучение на tpu с jax если вы любитель jax и tpu, вперед и с песней ! upd чуть раньше появилось еще у epsilon correct"
415,2025-02-06T12:23:55,"подборка материалов про deepseek сие творение китайских ку ли биных вызвало бурный ажиотаж, подняло высокое цунами хайпа, не на шутку встревожило саму, и про него не высказался только ленивый. но я ленивый, и все уже до меня все сказали, потому предлагаю ниже подборку материалов из англо- и русско- язычных источников статьи техрепорт про deepseek v3 техрепорт про deepseek r1 блогпосты the illustrated deepseek-r1 от jay allamar, автора легендарного блогпоста the illustrated transformer блогпост от phil schmid про deepseek-r1 своими руками open-r1 - попытка воспроизвести результаты deep seek сообществом лицехватов. так уже и первый апдейт подьехал. отличный разбор от антона раззжигаева, автора канала abstractdl комментарии про deepseek на gonzo-обзоры ml статей подробный технический разбор deepseek v3 на gonzo-обзоры ml статей хитрый ход от deepseek код репозиторий deepseek v3 репозиторий deepseek r1 репозиторий проекта open-r1 не преминули упомянуть deepseek и у лекса в недавнем подкасте."
416,2025-02-08T23:49:25,"paretoq scaling laws in extremely low-bit llm quantization папира no code no party введение вопрос о том, какая битность весов при qat quantization aware training , дает наилучшее качество, представляет несомненный интерес как с академической, так и практической точки зрения, и ранее был рассмотрен, в частности, в работе scaling laws for precision, где утверждалось, что оптимальная битность в районе 6-7 бит на параметр. однако, подобные выводы сильно чувствительны к качеству процедуры qat. успех серии работ по bitnet намекает, что вряд ли все так плохо в низкой битности. и в данной работе большой коллектив авторов из меты предлагает пару модификаций улучшения процедуры qat."
417,2025-02-08T23:53:04,"метод рассматривают обучение 1-, 1.58-, 2-, 3-, 4- битных моделей mobilellm. по существу в работе 2 основные идеи. можно какую-то часть времени обучать модель в исходной точности, а затем врубить qat. оптимальнее всего оказывается 90 учить во float16, а последние 10 . т.е большую часть обучения динамика совпадает с floating-point моделью, а оставшихся 10 хватает для восстановления качества после сжатия. под каждую битность подбирают оптимальную сетку. при бинаризации весов оптимально да больше ничего и не сделаешь брать знак от весов, при 1.58 и 2-х битах брать определенную в данной работе stretched elastic quant сетку небольшую модификацию lsq , и lsq при 3-х и 4-х битах. причем важно скейлы квантизации держать как обучаемые параметры. min-max масштабирование работает совсем плохо на низких битностях, обрезка непонятно каким образом лучше, но все же заметно хуже learnable scales. для обучения всего хозяйства используют ste без каких-то наворотов. кроме того, еще пробуют qat на mobilellm не с нуля, а с готовых чекпоинтов, и это оказывается ожидаемо лучше. относительная норма разницы весов претерпевает скачок при переходе от ¾ битной квантизации к более низким битностям. из этого авторы делают вывод, что при менее агрессивной квантизации модель при qat пытается компенсировать ошибку квантизации, а при экстремально низких битностях ищет новую, более удачную конфигурацию весов. эксперименты сначала подход проверяют на семействе моделей mobilellm от 125m до 1.5b . битности 1.58 и 2 оказываются парето-оптимальными. качество оценивают как средне по нескольким бенчам из lm-eval-harness. затем paretoq проверяют на llama-3-8b, где предложенный подход заметно опережает llm-qat, efficientqat, и ptq подходы даже с aqlm и quip сравниваются . учат модель на 30b токенах, что не так уж мало, но и не так уж много. c pv-tuning и qtip тактично не сравниваются, хитрецы. при всей дороговизне pv, он задействует все же меньше токенов . на тернарной квантизации авторы показывают превосходство над прошлыми qat подходами - spectra, bitnet - при фиксированном бюджете обучения. кроме того, авторы написали cpu kernel, дающий хорошее ускорение при 2-битной квантизации, благодаря чему 2-битные модельки оказываются оптимальными с точки зрения баланса между скоростью инференса и качеством. вывод подход выглядит разумным и рабочим, без rocket science. для полного счастья не хватает квантизации активаций и замеров на более сложных задачах."
418,2025-02-09T23:40:56,"better faster large language models via multi-token prediction статья кода нет, но есть модели введение раз уж пост вышел на love. death. transformers. и братва требует пояснений, разберем. за последние несколько лет мы наблюдали несколько качественных скачков возможностей llm. однако в основе их работы все еще преимущественно лежит задача предсказания следующего токена. данная незамысловатая задача позволяет решать задачи любой сложности, но существенным недостатком является дороговизна инференса, когда ради одного несчастного токена приходится загружать всю модель или часть слоев в случае moe в быструю память и сгружать обратно. дабы повысить эффективность инференса предлагается незамысловатое решение - предсказывать несколько токенов за раз. на самом деле такое уже было еще в далеком 2020-м году, и в сценарии дообучения однотокенной модели medusa . заслуга авторов из меты в том, что они исследовали разные варианты предсказания токенов для моделей разного размера."
419,2025-02-09T23:42:46,"метод архитектура следующая - есть трансформерная тушка и несколько голов, каждая из которых предсказывает k-ый следующий токен для головы с индексом k . если я правильно понял, эти головы на самом деле преобразуют эмбеддинг перед подачей в unembedding матрицу из размерности модели в размер словаря , а сама unembedding матрица общая для всех токенов. обучают на стандартный кроссэнтропийный лосс. дабы расход памяти не взрывался от тяжелых матриц логитов, авторы предлагают делать backward по каждой голове в отдельности в ligerkernel на этапе обучения логиты считают чанками и делают backprop на них, к слову . эксперименты обучают семейство моделей размером от 300m to 13b параметров на датасете из 100b токенов какого-то кода. валидируют на mbpp, humaneval, apps - сравнительно простых задачах про код. пробуют обучать на сырых байтах и словаре из 32к токенов. на маленьких моделях предсказание нескольких токенов вперед работает плохо, но начиная с какого-то размера 3b становится лучше по бенчам. 4 головы отпимальны по качеству для словаря в 32к токенов 8 для байтов . далее метод проверяют в сценарии дообучения и сравнивают 3 варианта дообучение на предсказание 1 токена вперед, для модели обученной предсказывать 1 токен вперед дообучение на предсказание 1 токена вперед, для модели обученной предсказывать 4 токена вперед дообучение на предсказание 4 токена вперед, для модели обученной предсказывать 4 токена вперед оказывается, что второй вариант работает лучше всего почему-то. multi-token prediction работает не очень на multiple-choice задачах. вероятно потому, что там требуется выдать всего один или немного токенов. потом тестируются на синтетике - induction heads, арифметике многочленов и наблюдают некоторый прирост качества, который объясняют тем, что в таких задачах полезно смотреть слегка наперед. очевидный практический плюс от многотокенного предсказания - ускорение инференса в 3 раза на bpe токенах и около 6 на байтах. вывод mutli-token prediction выглядит как естественная и рабочая история. тем более что в нашумевшем deepseek-v3 где использовалась модифицированная версия метода с трансфорнеыми блоками на каждый новый токен данная стратегия тоже отлично завелась. вероятно, она будет стандартной в будущих моделях. ждем -4, qwen-3?"
420,2025-02-10T16:21:18,"не проплаченной рекламы пост от transformerslovedeatch . стартап в области безопасности ии ищет инженера llm оптимизация и rl alignment чем предстоит заниматься. дообучение и оценка sota llm, аттаки на blackbox модели улучшение rl для атак на модели, настройки моделей ppo, rlhf, стабильность обучения . бенчмаркинг и оценка качества моделей elo-метрики, alignment . оптимизация инференса vllm, sglang, trt . требования. опыт работы с llm архитектуры, rl, alignment . знание pytorch jax. реальная практика с rl методами dpo, rlhf плюс . опыт с системами инференса vllm, kuber, docker . публикации в neurips icml iclr и др. сильный плюс. преимущество экспертиза в байесовской оптимизации, эволюционных алгоритмах, гиперпараметрическом поиске, автоматической оптимизации промптов. условия зарплата 80k 130k usd опционы. релокация в париж , полная занятость. работа с передовым стеком ai research, model alignment . форма для отклика https forms.gle z45wwdbtrhrd8inm9"
421,2025-02-11T14:11:57,"quest stable training of llms with 1-bit weights and activations статья код введение уважаемые коллеги из ist, в частности, black_samorez_channel выпустили статью про стабильное обучение моделей с квантизованными весами и активациями. статей с той же аббревиатурой пруд пруди на архиве - вот - вот - вот - и вот но эта - особенная! ранее уже неоднократно поднимался в том числе и на этом канале вопрос о том, в какой точности оптимально учить модель веса и активации , дабы получить наилучшее качество при заданном размере через ptq или qat . ранее утверждали, что 6-7 бит оптимально при квантизации весов и активаций в intx fpx. но сама процедура была незамысловата, и нет гарантий, что нельзя пробить существенно парето-фронт. свежие результаты смотри краткий обзор на gonzo-ml показывают, что в fp4 тоже можно эффективно обучать. в данной же статье авторам удается достичь парето-оптимальности в w3a3 w4a4 и стабильного обучения в w1a1 уже не оптимального, но на одном уровне с fp16 ."
422,2025-02-11T14:16:32,"метод основным затруднением при оптимизации весов и активаций в низкой точности является высокая степень шума. напомню, что операция квантизации недифференцируема, и дабы все можно было оптимизировать градиентными методами применяют трюк имени бенджио под названием ste небось, тоже спиздил у шмидхубера , где градиент просто пробрасывается через недифференцируемую операцию как будто вместо нее стоит функция y x . но при низких битностнях, такая оценка сильно расходится от истинного градиента и не сходится нормально. авторы формулируют задачу оптимизации ste, как минимизацию между оцененным псевдоградиентом и истинным. предполагая гладкость функции потерь по весам, можно оценить ошибку градиента, как константа на ошибку квантизации весов. веса модели разделяют на 2 группы - с ошибкой квантизации ниже и выше некоторого порога. на шаге оптимизации учитывают только градиенты от весов с ошибкой ниже заданного порога, ибо вторые как раз и вносят шум и нестабильность в обучение. далее, дабы работать с более регулярным распределением весов активаций, которое проще квантизовать, применяют пару трюков 1 чтобы привести распределение в более удобо квантизуемый вид применяют известный старым читателям канала трюк - вращения адамаровыми матрицами как весов и активаций. в результате получают что-то близкое к гауссиане. 2 дабы привести все приблизительно к n 0, 1 применяют rms нормализацию к результату шага 1. а для n 0, 1 можно уже численно найти оптимальный скейлинг фактор для решетки квантизации и пользоваться им. то есть в итоге алгоритм выглядит следующим образом на прямом проходе вращаем и нормализуем веса, сохраняя адамаровы матрицы на обратном проходе применяем обратное адамарово преобразование и маскируем градиент в ablation показывают, что trust estimator отбрасывание градиентов по шумным весам в связке с адамаровыми вращениями дает хорошую близость с истинным градиентом, в то время как vanilla ste и без адамара корреляция низкая."
423,2025-02-11T14:19:00,"эксперименты обучают семейство моделей размером от 30 до 800м параметров архитектуры -2 на c4 с более-менее стандартным рецептом обучения. по умолчанию отношение количества данных к числу параметров d n 100. как меру качества берут перплексию на отложенной выборке. quest работает лучше, чем pact и lsq бейзлайны. далее фитируют scaling law, который отличается от оригинального из статьи про шиншиллу фактором eff p - поправкой на битность параметра eff p 1 для fp16 . исходя из эмпирических графиков лосса для моделей разного размера и битности, получают парето-оптимальность в 4-х битах. 4 битный параметр эффективно равен 0.7 fp16, но модель-то при этом в 4 раза меньше, отсюда выигрыш в 2.7 раз по эффективности. метод пробуют на fp4 и 2 4 sparsity int4, где все тоже неплохо заводится. в ablation показывают, что адамаровы вращения дают некоторый прирост качества на w1a1 и w2a2, при этом лишь немного замедляя инференс. вывод как мне кажется, довольно обнадеживающий экспериментальный результат. все больше и больше подтверждений тому, что следует с самого начала учить в низкой точности дабы нищеброды потом уже не просили униженно gguf, gptq или awq кванты. интересно, можно ли пробить ниже еще порог парето-оптимальности по битностям весов активаций или мы упираемся уже в некий потолок? ждем экспериментов на большем масшабе. глядишь meta, qwen или deepseek порадуют."
424,2025-02-14T10:09:43,рай выглядит так. не мне привалило такое счастье
425,2025-02-15T22:47:02,"matryoshka quantization статья код есть, но мы вам его не покажем введение большинство методов квантизации готовят модель в некоторой заданной битности, и, если хочется иметь квантизованные модели разной степени сжатия, приходится прогонять алгоритм несколько раз и хранить где то всю полученную пачку. команда из глубокого разума на днях выкатила статейку по квантизации с примечательным названием matryoshka quantization , которая за один присест готовит квантизованные модельки в 2,4 и 8 бит. примечательно, что один из авторов, kusupati, ранее публиковал другую работу про матрешки matryoshka representation learning."
426,2025-02-15T22:51:19,"метод matquant по постановке ,поверх оптимизируемых методов квантизации, с обучаемыми непрерывными или дискретными параметрами. основная идея работы в том, что для целочисленных типов данных отстругивая младшие биты от представления с максимальной бытностью, возможно получать приближения разной точности, вложенные друг в друга, как матрешка. но если просто огрублять, скажем , int8 квантизацию, работает не очень, поэтому предлагается совместно оптимизировать разные битности одновременно в одном батче с разными весами. matquant применяют поверх omniquant, в котором оптимизируются скейлы и biasы в квантизации через поблочную дистилляцию, и qat, которая суть просто обучение с кроссэнтропийным лоссом с ste через недифференцируемую операцию квантизации. эксперименты метод валидируют преимущественно на gemma 2 2b и 9b моделях и мистрале 7b. полагаю, что и квены не рассматривают из политических соображений негоже поганых парнокопытных от меты лапать, как и китайскую продукцию . omniquant оптимизируют на 10, 20m токенах из c4, qat на 100 m токенах. причем в большинстве экспериментов квантизуют только ffn. качество оценивают по перплексии и 0 shots на lm-eval. наивное стругание из высокой битности работает сильно плохо на 2 битах, оптимизация под конкретную битность получше когда храним много моделей , но все равно не очень, предложенный подход еще чуть получше. просадки довольно значительные 13, 15 для 2b, 6-12 для 9b gemma 2 модели. если бы квантизовались все слои, 2 бита были бы парето оптимальными ибо точность квантизованной 9b модели равна несжатой 2b , но, увы, нет. полагаю, что подобный результат можно выжать более дешевыми quip без решетки и 1 мерным higgs, как и efficientqat. в ablation показывают, что совместная оптимизация в разными битностями накидывает немного в качестве. веса лоссов при разных битностых перебирают по сетке. при квантизации attention слоев метод тоже лучше бейзлайнов, но просадки становятся еще значительнее с 74 до 47 остальные методы работают чуть лучше рандома в 2 битах . выводы название работы красивое, и мотивация хорошая но результаты все же не слишком впечатляют. также утверждение, что они первые, кто об этом задумался, не соответствует действительности была такая работа any precision llm oral прошлого iclr , где разные битности получали через squeezellm. кроме того, вложенные представления разной точности можно получать через residual quantization."
427,2025-02-16T13:37:30,как получить sota метод квантизации в 2 и менее бит?
428,2025-02-17T18:43:16,"блог unsloth про квантизацию deepseek-r1 вот как надо евалить модельки, а не все эти ваши никчемные mmlu, перплексия, lm-eval-harness."
429,2025-02-17T18:44:20,"кроме того, еще оценивают качество модели по тому, может ли квантизованная deepseek-r1 написать flappybird в pygame с 3-х попыток."
430,2025-02-18T00:13:59,"large language diffusion models статья страничка проекта код и модели божатся выложить через 2 недели на текущий момент авторегрессионные модели занимают доминирующее положение среди языковых моделей, а диффузия блистает в других приложениях. ранее уже предпринимались попытки создания языковых моделей на основе диффузионных процессов https arxiv.org abs 2406.07524, https arxiv.org abs 2409.02908, https arxiv.org abs 2406.04329 , но получить что-либо конкурентоспособное тем же флагманским открытым моделям - не удавалось. в данной работе коллектив авторов из поднебесной произвел не свет masked диффузионную модель, с качеством якобы на уровне llama-2 3. и модель называют не абы как, а в честь легендарной автомобильной марки - llada."
431,2025-02-18T00:17:55,"метод предложенный процесс не является диффузией в привычном понимании, а маскированием с разной степенью зашумления mask токенами. задача языкового моделирования обычно оперирует в терминах дискретных токенов. имея некий исходный текст, предлагается зашумить некую долю от 0 до 1 токенов, заменив их на mask , а задача модели - предсказать зашумленные токены. прямо как старый добрый bert, только доля зашумленных токенов переменная. обучение состоит из pretrain и sft на pretrain зашумляют токены на любых позициях. на sft только в ответе. промпт не трогают. rl, как я понял, пока не осилили. инференс выглядит следующим образом - стартуя с большого количества mask токенов, предсказываем токены, которые стоят на этой позиции. прогон за один раз работает не очень, потому делаем процедуру итеративно, снова зашумляя уже меньший процент mask токенов . выбирать токены можно случайно, а можно брвть те, где модель наиболее уверена, а остальные снова зашумлять перед следующей итерацией. чем больше шагов диффузии маскирования демаскирования - тем ожидается, что лучше качесво. эксперименты обучают две модели размера 1b и 8b на некотором собственноручно собранном корпусе данных. замеряют на стандартном наборе задач из lm-eval и humaneval кодинг бенче . при прочих равных архитектуре и размере модели, затраченном на обучение бюджете диффузия на большинстве задач не хуже авторегрессионного бейзлайна кроме piqa . полученная 8b модель посильнее llama 2 7b, но все же уступает llama 3 8b и qwen2.5 7b. справедливости ради стоить заметить, что обучали всего на 2.7t токенах, что по нынешним меркам немного, всего лишь 0.13 миллионов h800 gpu часов. авторы утверждают, что в плане оценки правдоподобия диффузионки сильно менее эффективные по сравнению с авторегрессионными, но бенчмарки от этого не сильно страдают, и напоследок показывают, что диффузионные модели гораздо лучше умеют предсказывать прошлое прошлые строки на основе последующих в стихах , чем авторегрессионные llm. непонятно зачем, правда. вывод попытки уйти от стандартной парадигмы всегда выглядят интересно. пока еще сильно далеко от sota llm, да и протокол замеров не исчерпывающий, но тем не менее, кажется, что история сколь-либо жизнеспособная. сильно не хватает замеров скорости, и экспериментов по trade-off между скоростью генерации и качеством результатов. ждем выпуска моделей и кода, дабы самим заценить."
433,2025-02-18T23:33:44,"сообщество сегодня обсуждает релиз от grok3 от mask , но в этот прекрасный произошло еще одно - не менее важное и знаменательное событие. нас уже перевалило за 2к! изначально, когда создавал канал, я даже и не мог помыслить о такой аудитории думал, что буду как гаражный музыкант с парой десятков близких знакомых и энтузиастов. спасибо всем за то, что вы здесь, за комментарии и ссылки на релевантные статьи желаю и далее в силу мер и возможности публиковать обзорчики, мемасики про сжатие и не только."
434,2025-02-24T23:04:10,"- claude 3.7 sonnet, что ты можешь делать? - могу ризонить. - а что еще? - могу не ризонить."
435,2025-02-25T11:24:46,в качестве несложного упражнения читателю предоставляется самому написать кернел. ишшуя
436,2025-02-27T00:21:40,"блог вот недавно мы обсуждали llada и жизнеспособности диффузионной парадигмы, а тут inception labs обьявили о создании diffusion llm, которая якобы способна бодаться по качеству в бенчах приводят только код с вполне себе сильными closed-source llm без рызонинга . при этом она якобы на порядок быстрее небольших авторегресионных llm, давая космические более 1000 токенов в секунду на одной h100, а не специализированных чипах. утверждается, что оно могет еще и в rag, tools use и агентность. у них и чатик есть, можно потыкаться."
437,2025-03-01T23:11:08,"judge decoding faster speculative sampling requires going beyond model alignment статья кода нет давно хотел коснуться темы ускорения инференса авторегрессионных моделей за счет спекулятивного декодинга, но все никак не доходили руки, и наконец дошли. команда исследователей из 00, среди которых небезызвестный автор канала ai_newz, представила метод, ускоряющий спекулятивный декодинг за счет оценки важности текущего токена и принятия большего числа по сравнению с базовым подходом, при этом практически без просадки в качестве."
438,2025-03-01T23:13:09,"введение есть большие llm, а есть не очень большие. большие обычно работают лучше при прочих равных, но требуют большей памяти, объема вычислений и времени на шаг инференса. инференс llm обыкновенно memory bound - скорость упирается во время загрузки сгрузки весов из gddr hbm памяти в кэши, а не сами вычисления. то есть за один трансфер памяти потенциально можно было бы делать больше арифметических операций без существенного замедления. при этом в большинстве случаев разница между более слабой моделью называемой draft, т,е черновой и некой большей таргет моделью невелика. отсюда возникает идея - генерировать много токенов один за другим более слабой моделью llama-3.1-8b , а затем проверять более жирной llama-3.1-405-b с тем же словарем, последнее требование не обязательно, но упрощает постановку . за один шаг можно проверить несколько токенов малой модели, и если все предсказания совпали то есть наиболее вероятные токены у большой модели такие же, как у меньшой , принять все, иначе - до последнего принятого токена. таким образом можно добиться генерации, идентичной выдаваемой большой моделью, за меньшее время. данную идею предложили еще достаточно давно в статье fast inference from transformers via speculative decoding еще в далеком 2022. но, есть существенный недостаток. на практике предсказания драфт-модели и таргет-модели расходятся очень быстро, всего через несколько токенов, поэтому слишком большого ускорения достичь не удается - обычно в районе 1.5-2x. при этом для получения правильного ответа совершенно необязательно, чтобы предсказания каждого токена у меньшей модели совпадали с большей. она может сказать то же самое, но другими словами, используя синонимы, например."
439,2025-03-01T23:16:43,"метод авторы замечают следующее таргет-модель может за 2-3 токена отвергнуть корректное решение драфт модели. если в качестве драфта использовать даже более сильную модель, gpt-4o для llama-3.1-405b, acceptance rate все еще не слишком высок. если драфт модель ошиблась, таргет-модель пытается корректировать ее. the capital of france is berlin... no just kidding. the capital of france is actually paris. отсюда предлагается каким-то образом оценивать важность токенов на конечный ответ и принимать неважные токены, даже если их отвергла большая модель. датасет для обучения собирают следующим образом отбирают примеры из alpaca, arc только промпты генерируют продолжения разными моделями - -ми, мистралями отбирают вручную лучшие примеры на выходе 500 примеров вопрос, хороший ответ, плохой ответ . все токены из правильного ответа и токены до места ошибки в неправильном ответе считаются как положительные примеры. полученный датасет несбалансирован, и потому в лоссе дается больший вес отрицательным примерам. классификатор - это просто линейная голова поверх признаков на выходе последнего слоя, которая учится за полтора часа. эксперименты в качестве драфт-модели берут -3-8b, а таргет - -70b, 80b. качество замеряют на gsm8k, humaneval, arc-challenge. для оценки скорости рассматривают как неоптимальный инференс через трансформерс, так и куда более эффективный gpt-fast. в качестве бейзлайнов рассматривают драфт-модель и topk принятие т.е принимаем токен, если в top с наибольшей вероятностью . предложенный метод почти идентичен по качеству прогону таргет модели, и заметно лучше лрафта и topk. при этом удается достичь ускорения в 3-4 раза с gpt-fast против 2 у стандартного speculative decoding и eagle-2 и до 10 раз при hf инференсе. в ablation пробуют применяться в ood сетапе - валидировать метод на humaneval без примеров на код в обучающей выборке. работает хуже, но все еще лучше, чем topk. вывод хорошая идея с очевидной практической пользой. приятно, что обучение требует довольно небольшого компьюта меньше чем у того же eagle . интересно, как метод себя покажет на более сложных reasoning задачах."
440,2025-03-01T23:22:04,"судя по всему, комменты генерит только драфт модель придется зареджектить"
441,2025-03-04T17:58:40,"дорогой коллега и товарищ snk4tr по совместимости автор канала c_research выкатил на хабр блогпост от каскадных моделей до картинок в 4к как эволюционировали диффузионки. рекомендую всем желающим как-то осмыслить или переварить происходщее в мире диффузионных моделей, а также проследить за прогрессом и трендами в области. приятного чтения!"
442,2025-03-08T22:40:41,"do large language model benchmarks test reliability? статья блогпост код новые sota llmки выходят нынче, как собаки нерезанные, и тем острее встает ребром вопрос о качестве их оценки. полностью исчерпывающего протокола замером, удовлетворяющего нуждам всех категорий пользователей нет, да и не факт, что он достижим, потому приходится, скрепя сердце, полагаться на те или иные бенчмарки, принятые в литературе или индустрии. группа исследователей из mit решила взглянуть критически на популярные бенчмарки и качество ведущих моделей на них, и обнаружила следующее ни одна модель не является безупречной, и даже sota llm допускают осечки в простых случаях бенчи не без греха. в популярном gsm8k 5 условий и решений содержат проблемы. у разных моделей разные достоинства. o1-mini лучше всех из списка в плане общего решения математических задач, но sonnet понимает текст reading comprehension . далее в блоге авторы приводят примеры забагованных задач с неправильным ответом с ошибками и неоднозначностью в условии где забыли условие задачи кроме того, на днях те же авторы опубликовали почищенный тест-сет gsm8k под названием gsm8k-platinum, и показали,, что ведущие llm допускают на нем гораздо меньше ошибок по сравнению с оригинальной версией от openai. мораль проста - ежели у вас есть штангенциркуль, смотрите, чтобы у него не поехала шкала. у проекта есть классный дашборд с визуализациями ошибок моделей."
444,2025-03-11T23:05:54,"у livecodebench есть очень занятный spaces , где можно посмотреть на условия задач долю ошибок моделей из лидерборда на данной задаче решения задач, полученные разными моделями задачи делятся на легкие, средние и сложные. легкие ожидаемо решаются более-менее всеми, а вот те, что потяжелее только ризонерами и claude 3.5 3.7 o3-mini пока нет в лидерборде . некоторые задачи из раздела arc не решаются пока ни одной моделью из представленных."
445,2025-03-13T21:34:07,"какой-то кореец ? бегает по репозиториям разных проектов про квантизацию llmок с однотипными жалобами - no run quest - how to run it qtip - not run greenbitai интересно, это реальный человек загорелся желанием все потыкать? или llmка, которая начала подозревать, что ее сжимают и ходит неприкаянная по гитхабу в поисках лечения."
446,2025-03-14T11:28:04,"развлечения ради квантанул свежевышедшую gemma-3-27b-it через gptq генно-модицированной версией с парой примбамбасов . картиночную siglipовскую тушку не трогал, она все равно маленькая. евалы на бенчах надеюсь добавить позднее, но вроде отвечает связно на паре vlm примерчиков. карточка модели на лицехватс upd 20.03.25 метрики на openllm leaderboard v1 добавил."
447,2025-03-20T17:36:11,"маленький коммит для человека, огромный скачок для человечества я контрибутор мистраля!"
448,2025-03-20T23:23:51,"is noise conditioning necessary for denoising generative models? статья нет кода, но можно попросить sonnet 3.7 написать введение в диффузионных моделях, как в классической ddpm постановке, так и популярном нынче flow matching, генерация осуществляется путем итеративного перевода некоторого случайного шума в сэмпл из распределения реальных данных. кроме самого зашумленного сэмпла на вход генеративной модели обычно подается скаляр - сила наложенного шума, который отображается в некоторый многомерный вектор, модулирующий тем или иным образом обычно scale-shift карты активации внутри. данное архитектурное решение как-то устаканилось, и народ как-то особо не задумывался о его необходимости. а авторы из mit среди которых автор резнета сегодняшней статьи задумались и попробовали убрать обусловливание на шум."
449,2025-03-20T23:26:29,"метод мотивация следующая - есть сети-денойзеры в контексте image restoration , которые получают шум на инференсе различной, неизвестной заранее, силы, и вполне успешно работают. следовательно. и в диффузионной постановке сеть потенциально должна уметь на основе самого шума оценивать его величину. далее показывают красивый график с распределением шага зашумления при условии зашумленной картинки, и оказывается, что оно в большинстве случаев довольно узкое, т.е по самой картинке довольно точно можно понять насколько сильно ее зашумили за исключением высоких степеней шума. ниже приводятся некоторые оценки на ошибку между солвером без и с обусловливанием, и типа расхождение не очень велико. эксперименты сравнивают диффузионки без и с обусловливанием на время в следующих постановках диффузионный процесс ddpm edm flow matching uedm предложенная модификация edm без обусловливания на время датасеты cifar10 ffhq качество оценивают по fid fidары . модели учат с одинаковым протоколом обучения для возможности честного сравнения. в большинстве случаев просадка от исчезновения условия на время не очень велика за исключением ddim c детерминистическим сэмплером, где сильно ухудшился fid . для flow matching даже наблюдается некоторое улучшение . данное явление авторы объясняют. тем что с одной стороны сама процедура обучения немного другая - оценка flow field между распределениями, и таргет x0 - шум не содержит в себе времени. кроме того, пробуют альтернативные варианты, например, учить сеть саму предсказывать уровень шума, но будто бы это ничего не дает. выводы если рассматривать работу диффузионной модели как multi task, где каждая задача - расшумление при заданном уровне шума, то при стандартном подходе, у нас есть некоторая общая база и небольшое число параметров, специфичных для данного шага. с одной стороны, можно увеличить специализацию как в ediff и иметь отдельные сети на каждый отрезок. здесь же, наоборот, по сути предлагают иметь полностью идентичную модель на все уровни шума. практическая польза как будто может быть при flow-matching постановке, но непонятно, насколько выигрыш переносится на большой сетап."
450,2025-03-24T07:09:42,"scale-wise distillation of diffusion models статья демка код soon введение диффузионные модели на протяжении последних несколько лет удерживают пальму первенства среди семейств генеративных моделей во многих приложениях. однако их фундаментальным ограничением является многошаговое сэмплирование, из-за чего генерация занимает много времени. научное сообщество предложило вагон и маленькую тележку различных процедур дистилляции многошаговых моделей в малошаговые. но при приближении количества шагов к 1-му качество неизбежно просаживается даже для самых продвинутых подходов. отсюда мысль - а что если ускорить генерацию за счет удешевления шагов сэмплирования? мы с коллегами из yandex research предложили метод дистилляции в несколько-шаговую генерацию, где разрешение изображения увеличивается постепенно, на каждом шаге генерации swd . за счет этого удается достичь более чем 2х кратного ускорения по сравнению с эквивалентной дистилляцией в фиксированном разрешении."
451,2025-03-24T07:13:17,"метод на первых шагах расшумления изображение представляет собой почти полный шум, где сложно что-либо различить, а не последних - почти сформированную картинку. ранее была установлена связь между спектральной авторегрессией и процессом диффузии - первые шаги соответствуют низким частотам, а в самом конце формируются высокие частоты. то есть вначале мелкие и тонкие детали все равно неразличимы поверх шума. можно попробовать генерировать сначала в низком разрешении, а потом постепенно повышать интерполяцией. но как ? исходная модель может быть не адаптирована под малогашовую генерацию sdxl в 256x256 выдает безумный поп-арт на любой запрос , да и процедура интерполяции зашумленного латента плохо определена. наивный upsampling латентов приводит к сильные дефектам. несколько лучше работает upsampling x0 оценки из предсказанного латента с последующим зашумлением назад, но все равно не здорово. потому дообучение неизбежно. при этом дистиллированные модели выполняют две роли одновременно - уменьшение количества шагов сэмплирования и super-resolution в латентном пространстве. в качестве основы для процедуры дистилляции берется dmd2 как все еще state-of-the-art метод дистилляции. дополнительно к нему навешивается предложенный patch distribution matching pdm лосс, который стремится уравнять прогнанные через исходную диффузионную модель генерации модели-учителя и студента на уровне отдельных токенов. итоговый лосс содержит в себе обратную kl-дивергенцию gan лосс pdm лосс в качестве данных для обучения используется синтетика, сгенерированная моделью-учителем примерно 500к сэмплов ."
452,2025-03-24T07:15:22,"эксперименты предложенный метод дистилляции валидируется на моделях семейства sd3.5 - medium и large. для оценки качества смотрим как на классические метрики fid clip imagereward pickscore на coco mjhq-30k, так и пользовательские предпочтения. процедура обучения длится примерно 5к итераций все на одной a100 ноде . для малошагового генератора и fake модели из dmd2 обучаем lora адаптеры т.е peftа вполне достаточно . для 4-шагового сэмплирования используется следующая последовательность разрешений 256, 384, 512, 640, 768, 1024 для 6-шагового сэмплирования используется следующая последовательность разрешений 256, 512, 768, 1024 шаги сэмплирования в расписании подбираются специально так, чтобы соответствовать моменту возникновения заданных частот для данного разрешения. scale-wise диффузия практически не просаживается по качеству по сравнению с генерацией в фиксированном конечном разрешении. при этом при фиксированном бюджете генерации scale-wise строго лучше 4-шаговая scale-wise против 2-шаговой full-scale . по большинству метрик swd не проседает по сравнению с исходной моделью, за исключением fid за счет потери разнообразия . по user-preference паритет по релевантности дефектам, и некоторое даже улучшение по эстетике комплексности. в том числе и бьем switti нашу прошлую работу про scale-wise авторегрессию . swd позволяет выдавать почти 6 картинок в секунду для sd3.5-medium и 2.5 для sd3.5-large при генерации с батчом 8 . в ablation показываем, что компоненты метода важны обучение на синтетике правильно подобранное расписание шума адаптация модели под каждый скейл pdm лосс очень важен замена l2 лосса на более сложный kernel rbf между признаками учителя и студента не дает улучшений. вывод scale-wise подход генерации изображений кажется эффективной и хорошо мотивированной идеей в генеративных задачах. от грубых простых деталей постепенно переходим к сложному. ранее такая идея была воплощена в контексте progressive gan , var, каскадных диффузий. латентную диффузию, как оказывается, можно достаточно быстро перевести в режим генерации от мала до велика. альтернативный подход с использованием крупных патчей в dit на первых шагах был предложен командой из meta с небезызвестным артемом из эйай ньюз ."
453,2025-03-24T16:01:01,"обновление deepseek-v3 вышло - deepseek-ai deepseek-v3-0324 и это примерно пока все, что известно... upd. есть метрики. и прирост вполне существенный."
454,2025-03-27T20:23:07,"инструкции в мемах для рецензентов на peer reviewed конференциях. интересно, если подать данную страничку vlmке перед прочтением рецензий, то их качество на neurips icml iclr возрастет? а заодно можно еще и вкорячить в картинки black-box адверсариальную атаку"
455,2025-03-30T10:34:41,"overtrained language models are harder to fine-tune статья где код, билли? введение во многом успех современного глубокого обучения обусловлен масштабированием моделей и времени обучения. стандартный пайплайн обучения включает в себя предобучение на большом объеме данных с последующим дообучением на куда меньшем количестве примеров высокого качества. в текущей практике обыкновенно качество базовой модели напрямую транслируется в качество дообученной на инструкциях. а так как качество базовой монотонно растет, то кажется логичным продолжать дальнейшее наращивание бюджетов обучения. однако группа исследователей часть из них ранее засветилась в scaling laws for precision обнаружила, что начиная с какого-то момента чекпоинты olmo начинают проседать по качеству при файтьюнинге, решила поглубже исследовать данный вопрос и найти какое-то объяснение явлению перетренированности ."
456,2025-03-30T10:39:20,"метод и эксперименты напомню, что olmo - это полностью открытый проект по обучению больших языковых моделей с открытым исходным кодом и выложенными промежуточными чекпоинтами. давным-давно в одной далекой галактике было показано что при фиксированном количестве flops на обучение оптимальное качество достигается при отношении количества токенов d к параметрам модели n около 20. но жирные модельки потом дорого гонять на железе, потому обыкновенно выходят далеко за данное отношение и качество все равно монотонно растет, хоть и не так быстро, как при отпимальном скейлинге. в частности, для llama-3 отношение d n 2000. авторы взяли веса моделей olmo-1b, olmo-2-7b, llm360-amber с разных шагов обучения и обнаружили, что начиная с какого-то момента качество на некоторых задачах alpacaeval, arc не agi , piqa при дообучении начинает убывать. при этом качество базовой модели непрерывно растет. для 7b модели при этом такого явления не наблюдается, так как модель, по всей видимости еще не успела перейти в режим перетренированности . исследователи делают гипотезу, что данное явление, следствие повышенной чувствительности параметров к шуму и пертурбациям. дабы проверить данную гипотезу, авторы обучают на корпусе с4 модели от 15m до 90m параметров с подвыборками размером от 4b до 128b токенов . сначала авторы исследуют зависимость лосса модели от времени обучения при фиксированной величине пертурбаций параметров гауссовым шумом. первоначально, лосс модели не сильно меняется даже при большой величине шума. но начиная с какого-то момента при сильном зашумлении лосс зашумленной модели начинает возрастать, несмотря на то, что качество исходной модели монотонно улучшается. то есть просадка модели, вызванная шумом, растет быстрее, чем улучшается модель. дообучение, конечно, сильно отличается от добавления гауссового шума, но тем не менее оказывается, что на некоторых датасетах при лернинг рейте побольше начиная с какого-то момента качество падает . далее авторы пробуют уменьшать шаг обучения, чтобы учесть большую чувствительность параметров модели, и это помогает на id in domain задачах, но качество на ood out of domain все равно может ухудшаться при дальнейшем обучении. затухающее расписание шагов обучения откладывает проблему на более позднее время, но не решает ее полностью. далее авторы рассматривают простую линейную модель таргет генерируется линейным слоем, а обучается композиция двух слоев без активаций, где наблюдается тот же самый эффект - растущая чувствительность весов модели к зашумлению. в процессе обучения постепенно выучиваются все меньшие и меньшие сингулярные значения целевой матрицы, и в тот момент, когда шум превосходит одно из них, ошибка может начать расти. в конце авторы вспоминают, про работы из rl и пластичность , комментируя , что обнаруженное явление имеет по всей видимости ту же самую природу. вывод довольно любопытное и интересное наблюдение. для полноценной валидации требуется репродукция другими командами ибо результат может быть обусловлен такими нюансами как детали оптимизации, архитектуры, mixed-precision настроек и выбора данных. потенциально еще причина может быть в специфике оптимизации adam-oм или стремлении модели неограниченно наращивать логиты вероятностей для оптимизации кросс-энтропии. использование другого оптимизатора или регуляризации например, sam вероятно способно предотвратить проблему. было бы интересно еще проверить имеет ли место эффект в других задачах т.е будет ли ухудшаться качество vision foundation моделей clip siglip на downstream при очень продолжительном обучении?"
457,2025-04-01T12:45:59,"я придумай шутку про сэма альтмана и agi. шутка должна быть очень тонкой и остроумной, прямо чтобы разрывало от смеха. deepseek-v3-0324 тончайшая шутка для ценителей сэм альтман пообещал, что agi будет выравнивающим для человечества. видимо, поэтому совет директоров сначала выровнял его должность до нуля, а потом обратно до единицы. квантовая суперпозиция ceo! разбор остроты для тех, кто хочет смеяться осознанно игра на двойном смысле alignment термин из ai safety чтобы ии был безопасным vs. буквальное выравнивание как в увольнении возвращении . квантовая шутка отсылка к тому, что его статус был одновременно и 0 уволен , и 1 на посту , пока не коллапсировал в реальность. сабж для гиков только те, кто следил за драмой в openai, оценят весь юмор. альтернативный вариант ещё чернее agi сможет решить проблему голода, кризис энергии и даже alignment... но вот договориться с советом директоров это уже задача уровня asi. asi artificial superintelligence, следующий уровень после agi p.s. если не разорвало возвращаю диплом остроумия в бюро!"
458,2025-04-04T00:17:21,"qwerky-72b and 32b training large attention free models, with only 8 gpu s блог пост attention is not all you need убийца трансформеров, новая ступень эволюции архитектур ! если вам зачем-то приспичило отказаться от attention, то данный рецепт предлагает решение, которое позволит вам превратить трансформер, превратить трансформер в элегантную rwkv модель. процедура следующая берете трансформер заменяете attention на rwkv 1 сначала пытаетесь добиться того, чтобы блок rwkv воспроизвел выход исходного трансформерного 2 потом дистиллируете логиты исходной модели обучая только rwkv 3 потом еще раз дистиллируете разморозив все 4 дообучаете на более длинном контексте бюджет дистилляции - всего-то несколько сот лямов токенов. таким образом получаются - qwerky-qwq из qwen qwq - qwerky-72b qwen qwen-2.5-72b-instruct на некоторых бенчах сохраняют качество, где-то теряют до 5 . но хоть как-то работает, и ладно. и даже, как заявляется, опережает не абы что, а gpt-3.5 turbo без единого attention слоя. 8 gpu это не a100 h100, как вы могли подумать, а амудшные карточки с 192gb vram."
459,2025-04-05T23:42:34,в ожидании аппрува на веса llama 4
462,2025-04-05T23:46:49,"кстати, с квантизацией llama 4 в оригинальной репе какой-то наеб. скрипт квантизации quantize.py ссылается на несуществующий quantize_impls. правда, учитывая, что это rtn квантизация, которая раскоает модель в негодность в 4-х битах, да и вроде обещают qat модельки, может и пох."
463,2025-04-05T23:49:27,сук даже vpn не помог и указание другой страны
465,2025-04-07T21:27:38,за наводку спасибо snk4tr
466,2025-04-07T23:14:11,"презентация с моей сегодняшней лекции про методы сжатия бям на курсе школы анализа данных яндекса эффективные модели . в ней даю краткий обзор по существующим подходам, актуальным работам в области и некоторые общие рекомендации."
467,2025-04-08T10:56:35,"релиз несколько затянулся мыши плакали, кололись, но продолжали грызть кактус , но в итоге допинали, как появилась полноценная поддержка в vllm compressed_tensors, релиз квантизованных gptq в 4 бита моделей deepseek-r1 и самого кода квантизации моделей. речь пока не идет о том, что можно запустить у себя на калькуляторе, и даже на consumer-grade gpu, но в одну ноду 8x a100 h100 влезает уже без приседаний и с контекстом, достаточным для reasoning задач а-ля aime, gpqa, math500. модели на - deepseek-r1-gptq-4b-128g квантизуются все слои в трансформерных блоках - deepseek-r1-gptq-4b-128g-experts квантизуются только non-shared experts код квантизации - moe-quant"
468,2025-04-10T13:40:17,"hogwild! inference parallel llm generation via concurrent attention статья репозиторий страница проекта введение для человеческих особей биполярное расстройство считается тяжелой психической болезнью , но для больших языковых моделей способность рассуждать в несколько взаимодействующих между собой потоков, как будто от лица разных персонажей, может быть полезна в контексте решения логических задач. коллеги из yandex research, где ваш покорный слуга выступал скорее в роли моральной поддержки, реализовали training-free подход параллельного инференса для llm."
469,2025-04-10T13:44:24,"метод современные llm видали всякое в процессе своего обучения, но прямых указаний на то, что в конкретном месте можно распараллелиться и разбить задачу на части, которые можно решать независимо скорее всего особо не встречали . тем не менее, рассчитывая на то, что современные модели достаточно умны, подав подходящую инструкцию и организовав нужным образом kv-кэш, можно рассчитывать на продуктивную коллаборацию потоков. благодаря rope позиционным эмбеддингам можно задавать произвольные позиции токенам в последовательностях, не меняя расположение токенов в kv-кэше в физической памяти. организовать кэш можно следующим образом interleaved layout. процессы по очереди пишут общую память и видят чередующиеся шаги друг друга в прошлом. contiguous layout. процессы видят кэш другого процесса непрерывным блоком в прошлом. combined layout. промежуточный вариант. процессы видят перемежающиеся шаги друг друга и текущий логический шаг рассуждения сплошным куском. промптинг состоит из 3 компонент 1 системного промпта, поясняющего llmке, что надо работать в несколько потоков. 2 few-shot примеры коллаборации потоков. 3 вставка s1-like промптов, периодически спрашивающих процесс, занимается ли он полезной работой потенциальная польза от использования методов, что за меньшее количество прямых проходов по сети инференс llm memory-bound , можно прийти раньше к правильному решению, а в идеале, за счет самопроверки процессов - и поднять качество. эксперименты эксперименты прогоняли на qwen qwq-32b, как на модели достаточно хорошей и влезающей легко на 1 gpu. подход проверяли на модифицированной версии gsm8k и наборе примеров из limo. для gsm8k сравнивается работа в 1 2 4 потока. в большинстве случаев многопроцессовый режим работает лучше стандартной генерации при фиксированном бюджете. на limo разные опции hogwild! кэша с наивным бейзлайном с выдачей ответа одним процессом в заданном формате, с early stopping промптингом и независимыми процессами self-consistency . hogwild! работает стабильно лучше. разные стратегии обработки kv-кэша близки по качеству за исключением совсем малых бюджетов , комбинированный выглядит немного предпочтительнее альтернатив. при бюджете в 8к прямых проходов в 2 потока достигается то же качество, что и генерация одним потоком 16к токенов. вывод научно-технический процесс достиг того уровня, что llm способны заменять даже не одного, а нескольких кожаных мешков . те не менее, удачная коллаборация происходит на практике далеко не всегда - иногда процессы вроде бы договорившись, начинают все равно дублировать друг друга или просто теряются. в дальнейшем планируется развитие подхода, в частности, специальное дообучение, для улучшения взаимодействия процессов друг с другом."
470,2025-04-12T12:15:35,"exllamav3 репозиторий чекпоинты на этой неделе turboderp выпустил 3-ую версию своего фреймворка для инференса llm. на данный момент либа находится на стадии разработки есть куда расти в плане оптимизации неоптимальная утилизация на ampere gpu amd gpu если у кого есть такие не поддерживаются планируют накатить интеграцию с flashinfer на данный момент поддерживаются только llama, qwen, gemma2, mistral архитектуры. прошлая версия exllama в качестве метода квантизации брала gptq, но в этот раз за основу взяли тяжелую артиллерию среди низкобитных методов квантизации - адаптацию qtip, тем самым гарантируя качество значительно лучше gguf, особенно при экстремальном сжатии в 2 и менее бит . по перплексии качество выглядит и правда хорошо, интересно было бы оценить на других задачах."
471,2025-04-16T21:07:49,"старческий склероз - это не болезнь, а фундаментальное ограничение кожаных мешков на размер контекстного окна."
472,2025-04-25T19:09:16,"по итогу конференции iclr 2025, проходящей сейчас в сингапуре, планирую сделать подборку заинтересовавших постеров по теме efficientdl и смежным вопросам, а также разобрать некоторые из них вид в виде лонгридов, а пока хотел бы поделиться общими впечатлениями от конференции 1 поездки на такие мероприятия очень полезны с научной точки зрения, так как даже пристально следя за областью, какие-то работы упускаешь из внимания, и наталкиваешься на интересные и релевантые своему ресерчу на самой конференции. 2 после данных мероприятий сильно возрастает мотивация заниматься наукой. под впечатлением результатов от других работ, обсуждений с авторами возникает желание опробовать приглянувшуюся идейку у себя в коде или эксперименте. 3 классная русская тусовка. на iclr постоянно встречаешь русскоязычных ребят - как из яндекса, airi, т-банка, так и обосновавшихся в зарубежных лабах, компаниях и институтах - google, meta, eth, nyu, и т.п и т.д. 4 китайцы частенько говорят по английски плохо. ну прям из ряда вон плохо. как читаешь статейки прямо все по маслу, такие вычурные обороты, достойные джейн остин, толкина и байрона, а как доходит дело до устной речи, не понимают даже вроде бы довольно простые вопросы... один даже честно признался, что не могет в инглиш и включил google translate с английского на китайский на планшете... тем не менее, с некоторыми возникали прямо содержательные и интересные обсуждения. например, с авторами sana и svd-quant. китайцы наиболее широко представлены среди всех национальностей, что , впрочем, неудивительно. 5 имеет место явление, что на конференцию подаются несколько статей - про одно и тоже. на текущем iclr я видел 3-4 статьи про комбинирование вращения smoothquant для лучшей квантизации весов и активаций, и так же 3 4 статьи про sliding window prefix attention с небольшими концептуальными отличиями. 6 в сингапуре жарко снаружи и холодно внутри в помещениях. прямо как в баньке с контрастным душем. мерзнешь от кондиционеров и бежишь наружу париться, и когда напаришься вдоволь - идешь охлаждаться. 7 все дорого. прямо от слова совсем. и кофий не чета тому, что в москве. за 8-9 баксов наливают бурду, которую в условном cofix one price coffee постесняются налить за вчетверо меньшую сумму. 8 город богатый и впечатляет. возможно, меня покусал варламов, но небоскребы тут смотрятся как-то гармонично и естественно в отличие от москва-сити. 9 с алкашкой грустненько. и зожников много, везде видишь бегунов и велосипедистов. 1 0 дуриан выглядит классно. прям хочется взять к себе домой эту здоровенную шишку, но запрещено законом."
473,2025-04-29T02:54:51,"интересно, метод кнута и пряника по-сингапурски называется методом ротанга и дуриана? ротанг - палка для наказаний дуриан - местный тропический фрукт с очень интересным вкусом"
474,2025-04-30T21:37:34,"autoregressive distillation of diffusion transformers статья код введение как известно всем еще с материнской утробы, основной проблемой диффузионных моделей в разных ипостасях является долгая итеративная генерация. было придумано множество способов ускорения генерации за счет дистилляции, но при стремлении количества шагов к единице качество неизбежно уступает учительской модели. в существующих подходах ускорения генерации модель принимает на вход только текущий зашумленный сэмпл и потому подвержена накоплению ошибки при более ранних шагах, что может приводить к exposure bias для желающих лучше разобраться в теме рекомендую посты на concise research c_research . и команда из meta, среди которой можно заметить небезывестного артема, автора канала ai_newz и просто хорошего человека предложила подход, направленный на решение вышеописанной проблемы."
475,2025-04-30T21:43:22,"метод знание всей прошлой траектории сэмплирования при малошаговом сэмплировании потенциально дает более богатую информацию модели и позволяет скорректировать ошибку. формально существует однозначное отображение при детерминированном сэмплере между шумом и конечной генерацией и хотелось бы быть как можно ближе к этой траектории. однако, возникает вопрос - каким образом можно подать прошлые шаги в модель? и авторы предлагают интересное решение модель на текущем шаге генерации делает self attention на текущий и прошлые шаги сэмплирования с причинной маской прошлые шаги не аттендятся на будущие . в модель добавляется дополнительный эмбеддинг на шаг зашумления. разные шаги зашумления при генерации получают разные временные эмбеддинги. пространственные при этом одинаковы. обусловливание на прошлые шаги проводится только в первых n трансформерных блоках. c одной стороны, attention на прошлые шаги довольно шумный в поздних слоях и даже слегка просаживает качество. в то же время, self attention только в первых блоках удешевляет forward pass. выходы kv-кэшей с прошлых шагов можно закэшировать, как в авторегрессионных моделях. дополнительные вычисления возникают только при вычислении непосресдственно self attention. в качестве базового метода дистилляции используют простой step distillation без progressive , где модель-ученик пытается воспроизвести траекторию учителя. для улучшения качества можно дополнительно накинуть адверсариальный лосс на x0. также предлагаются два альтернативных подхода маскирования 1 скользящее окно attention обусловливание на несколько последних шагов 2 attention на текущий сэмпл и начальный шум. эксперименты метод валидируют на dit-xl 2 для class-conditional генерации на imagenet 256x256 и на проприетарной emu модели для text-2-image. про последнюю известно, что это трансформер на 1.7b параметров, обученный на некотором проприетарном датасете. для дистилляции dit-xl 2 учителем сэмплируют 2.5m траекторий при 25 шагах сэмплирования при этом стремясь добиться качественной 4-шаговой генерации. качество оценивают по fid на каком количестве сэмплов? , is, precision и recall. обусловливание на траекторию значительно улучшает качество по сравнению с ванильной step distiilation. альтернативные варианты масок будто бы чуть хуже по метрикам, но возможно, не статзначимо. gan-лосс сильно улучшает качество и конечная модель имеет даже меньший fid, чем учитель. в конечном варианте модель учитывает явно прошлые шаги в первых 6 блоках из 28-ми, а далее работает как исходный dit. при генерации 256x256 дополнительный condition на прошлые шаги несмотря на увеличение количества токенов в self-attention на последнем шаге генерации почти в 4 раза почти не замедляет генерацию. end-to-end время генерации возрастает только на 2 по сравнению с инференсом, использующим только текущий сэмпл. однако, здесь стоит заметить, что для 256x256 последовательность токенов довольно короткая - 256 256 8 2 2 токенов на одно изображение, т.е 1024 на последнем шаге . потому вычисление attention сравнительно недорогое и дополнительный оверхед благодаря kv-кэшам и включению прошлых шагов только в первых блоках трансформера должен быть действительно невелик. на text-2-image генерации качество оценивают на compbench - бенчмарке, оценивающем релеватность и сравнивают с другими публичными и непубличными дистилированными моделями - sdxl-lightning, dmd2, add, lcm-lora, и imagineflash и по fid на mscoco. предложенный подход ard достигает самого хорошего качества при 3-шаговой генерации слегка опережая 4-шаговую dmd2 по метрикам."
476,2025-04-30T21:43:22,"однако данное сравнение вызывает ряд вопросов 1 разные подходы используют разные модели. поэтому невозможно достоверно определить, вызвана ли разница превосходством ard метода или тем, что базовая emu модель просто лучше бейзлайнов. 2 fid считается на 5к сэмплах, что может быть недостаточно учитывая шумность fid . полагаю, что meta не испытывает такую нехватку в ресурсах, что им неподьемно посчитать метрики в стандартном протоколе на 30к промптов. 3 авторы замечают, что на больших моделях просадки метрик меньше - и так emu модель меньше sdxl, и просадка меньше, то якобы подход меньше сажает качество по сравнению с альтернативными дистилляционными подходами. данный аргумент не убедителен, так как сложность дистилляции определяется рядом факторов - таких как обучающие данные, архитектура модели unet или dit , специфика sft и rl если он был . кроме того данная emu модель, скорее всего. обучалась на flow matching. 4 нет чисел по скорости инференса для emu моделей. для генерации в 1k, где картиночных токенов становится достаточно много, self attention на расширенную последовательность перестанет быть настолько безобидным даже при учете того, что он возникает только в первых блоках . выводы сама идея и реализация выглядит довольно интересной с нетривиальным архитектурными решениями. однако, протокол сравнения в text-2-image вызывает смутные сомнения. как мне кажется, валидация всякого метода должна проводиться в контролируемых условиях - когда предложенный метод и альтернативы находятся в равных условиях. в данном случае более, чем уместно было бы провести эксперименты на публичных моделях sdxl, sd-3.5 при сравнении с dmd2 и прочими дистилляциями на тех же самых данных, либо уж все на emu. а также привести время инференса для дистилированной emu модели и какой-то user preference study ."
477,2025-05-01T18:22:54,"некоторое время назад. еще до iclr, мы с ребятами из т-банк research обсудили структуру и организацию исследований в компаниях, а также настоящее и будущее мира больших языковых моделей. спасибо коллегам за содержательную и интересную дискуссию! запись беседы"
478,2025-05-06T22:54:48,люди - это несовершенные дистиллы бога.
479,2025-05-07T21:25:00,"ai модерацию очень сложно мерить нужно учитывать разные виды контента, быстро отвечать, не false positiвить. челы из https whitecircle.ai озаботились и сделали первый бенчмарк для гардрейлов, а еще измерили на нем все самые популярные llm, в том числе давая моделям поррасуждать над ответом. твиттер полистать подробнее в блоге"
480,2025-05-07T21:27:15,дело хорошее и обьективно нужное так что честь и хвала ребятам за их труд
481,2025-05-08T21:56:25,"few-shot prompting - это когда пытаешься разговорить человека, пригласив в бар."
482,2025-05-14T12:15:48,"статус работ кажется, юдковский решил, что слишком много сабмитов в этом году на neurips, и положил overleaf дабы тормознуть agi."
483,2025-05-17T20:53:51,"сильно запоздалый пост из-за праздников и neurips дедлайна, но все же дошли в итоге руки . ниже подборка статей с 1 постерной сессии на iclr, которые так или иначе были связаны с efficientdl mambaquant quantizing the mamba family with variance aligned rotation methods в данной статье акцентируют внимание на квантизации mamba - архитектур в разных задачах. наивная адаптация методов квантизации для трансформерных llm просаживает сильно качество на s6 моделях. авторы анализируют проблемные места в квантизации мамб, где накапливается большая ошибка и предлагают свое решение whitening преобразование вместо адамара в offline rotations. добавление scaling факторов в модель в стиле smoothquant, учитывающих специфику mamba, для упрощения задачи квантизации. метод валидируется на ряде nlp vision задач, где показывает заметное улучшение по сравнению с бейзлайнами при квантизации весов и активаций. flashrnn i o-aware optimization of traditional rnns on modern hardware трансформеры нынче sota во многих приложениях, однако в некоторых задачах типа определении четности , показывают себе плохо, а rnn - хорошо. однако rnn плохо параллелизуются и вообще неэффективно используют ресурсы gpu. ребята написали кастомные fused cuda triton кернелы, эффективно использующие иерархию памяти, для forward и backward и смогли добиться ускорения до 50 раз по сравнению с ванильной торчовой реализацией. ostquant refining large language model quantization with orthogonal and scaling transformations for better distribution fitting в данной статье предлагают оценивать эффективность квантизации по тому, насколько плотно покрывает сетка квантизации целевое распределение. при наивном round-to-nearest подходе из-за выбросов, большая часть объема не используется. добавление обучаемых вращений из spinquant и scaling факторов а-ля smoothquant позволяет более равномерно распределять распределение весов по решетке и тем самым улучшает качество квантования. к сожалению, на постере не было ни одного из авторов, а какой-то левый чувак, который не особо был в теме, потому содержательного разговора не получилось. approaching rate-distortion limits in neural compression with lattice transform coding в данной работе авторы ставят своей задачу добиться сжатия сигналов любой природы как можно ближе к теоретико-информационному пределу. для этого обучают автокодировщик маленькую mlp , чтобы преобразовывать входные данные с возможными выбросами и широким диапазоном значение , в некоторое более регулярное множество, и затем проектируют на оптимальную сетку e8 для 8-мерной векторной квантизации, λ24 для 24-мерной квантизации . валидируют преимущественно на синтетике. когда я спросил авторов про trellis-based квантизацию из qtip, который потенциально может быть еще ближе к rate-distortion limit, авторы ответили, что не знают, что это такое. streamlining redundant layers to compress large language models идея простая - находим последовательность наименее важных блоков в трансформере по косинусной близости по аналогии с the unreasonable ineffectiveness of the deeper layers , пруним, вставляем один трансформерный блок ffn и дообучаем немного. работает несколько лучше, чем просто прунинг блоков кто бы сомневался . duoattention efficient long-context llm inference with retrieval and streaming heads головы attention делятся на 2 типа - retrieval heads, которые могут аттендиться на любой токен в последовательности, и streaming heads, которые смотрят только на последние токены и attention sinks в начале последовательности. для вторых можно сэкономить на вычислениях и памяти, храня только кэш для самого начала и некоторого фиксированного количества последних токенов. для определения streaming голов маску в каждом attention параметризуют как взвешенную сумму полного causal attention и streaming attention. и те головы, где коэффициент streaming attention наибольший далее обрабатываются как streaming heads. предложенная техника позволяет уменьшить кэш почти вдвое без просадки на longbench задачах."
484,2025-05-17T20:53:52,"beware of calibration data for pruning large language models авторы замечают, что для прунинга sparsegpt wanda выбор данных имеет значение. данные из обучающей выборки предпочтительны часть экспериментов делают на своей модели dclm-7b , но если их нет, можно сгенерировать самой сжимаемой llmкой взяв некоторый префикс. при генерации выкидывают последовательности с самой большой перплексией. далее авторы обнаруживают, что сгенерированные данные ближе к обучающей выборке чем варианты калибровочных данных c4 wikitext2 red pajama. бешеного прироста качества нет, но улучшение на 0.5-1 при 50 2 4 sparsity консистентно для разных моделей. продолжение следует... надеюсь"
485,2025-05-26T11:25:04,"microscaling data formats for deep learning nvfp статья код введение с ростом размера llm, затрат на обучение и инференс все более актуальным становится вопрос эффективных вычислений. опыт показывает bitnet, quest , что вполне реально гонять обучение с низкобитными весами и активациями, и при хорошей реализации даже оптимально по флопсам. однако, вычисления в низкой битности требуют аппаратной поддержки. в поколении blackwell зеленые завезли аппаратную поддержку новых малобитных типов чисел с плавающей точкой с плавающей точкой - nvfp4 mxfp 4,6,8 ."
486,2025-05-26T11:26:28,"метод основная фича, которая обеспечивает стабильность и эффективность низкобитных операций, это аппаратная поддержка операций с квантизованными тензорами с малыми группами. чем меньше количество весов, для которых берется общий масштаб скейл , тем точнее их можно аппроксимировать, но тем и больше накладные расходы на хранение и операции с ними. дабы уменьшить расходы по памяти скейлы хранятся в меньшей точности. а операции с ними имеют эффективную кернельную реализацию, что обеспечивает малое замедление по сравнению с per-tensor per-channel кватнизацией. 1 nvfp4 - это fp4 e2m1 с group_size 16, где скейл квантизуется в fp8 e4m3 . итого 4.5 бит на параметр. 2 семейство mxfp форматов включает в себя 4 6 8-бита. скейл квантизуется в экзотический e8m0 формат - т.е в логарифмическую шкалу, благодаря чему операции со скейлами можно свести к очень дешевым битовым сдвигам. размер группы 32 - т.е имеем 4.25 бит на параметр. эксперименты в whitepaper mxfp формата прогнали эксперименты на ptq и qat vision audio text модельках bert и encoder-decoder для перевода с языка на язык . наивный ptq каст в mxfp8 работает ожидаемо без просадок, в mxfp6 mxfp4 имеет место заметное ухудшение, но небольшой qat позволяет почти восстановить качество в большинстве случаев до уровня half precision. затем авторы гоняют обучение с весами активациями градиентами квантованными в mxfp6 на gptшкам от 20m до 1.5b и кривая обучения почти совпадает c half-precision за исключение спайков . последующие статьи, заслуживающие отдельного разбора training llms with mxfp4 и совсем свежая quartet от коллег из ist и в частности black_samorez_channel показали эффективность обучения в mxfp4 на более серьезных масштабах. в первой статье смогли добиться ускорения 1.7x против bf16, и 1.3x против fp8, а в quartet 2.3x против bf16 и 1.6x против fp8. в качестве удачного внедрения nvfp4 можно вспомнить тоже заслуживающую разбора svd-quant https hanlab.mit.edu blog svdquant-nvfp4, где на rtx 5090 смогли добиться 3-х кратного ускорения инференса flux. выводы переход к fp4 в качестве стандартного типа для обучения кажется делом времени широкого распространения blackwell чипов в датацентрах . интрига в том, в какой момент придется остановиться. дойдем ли до полностью тернарных сетей в будущем, или они окажутся парето-неоптимальными? время покажет"
487,2025-05-26T18:17:53,при релизе каждой новой sota llm
488,2025-05-27T12:40:45,"папир датасет на лицехватс пост на cv time коллеги из yandex research выкатили публичный датасет, под названием alchemist , из 3 c небольшим тысяч картинок, собранных из интернета, для дообучения диффузионок. данный датасет, отобранный с помощью довольно занятного пайплайна, дает заметный прирост качества на разных моделях в отличие от laion-aesthetics и просто фоток анимешных тяночек . так что, ежели кому нужно заалайнить модельку на качественных данных, далеко ходить теперь не надо."
489,2025-05-29T14:23:15,"look ma, no bubbles! designing a low-latency megakernel for llama-1b блогпост прямой да и обратный проход через современную llm, подразумевает запуск нескольких сотен кернелов attention, mlp, нормализаций . команда из стэнфорда обнаруживает, что скорость инференса маленьких llm 1-3b параметров упирается не в вычисления память, а во время запуска кернелов. эффективные движки для инференса vllm sglang позволяют только на 50 использовать пропускную способность новых видеокарт h100 gb200 . там некоторые операции уже слиты в один вызов кернела, но самих вызовов все еще остается много. и авторы предлагают реализовать весь forward pass в виде одного megakernel ! из нюансов реализации стоит выделить следующее 1 управление памятью. так как за shared memory борются сразу несколько процессов, надо эффективно распределить ее и раздавать по запросу. для это используется некий вариант paging. 2 синхронизация. теперь у нас много операций работающих асинхронно и требуется внутри кернела регулировать то, чтобы процесс не начал работать, пока не будут готовы все необходимые входы т.е attention не запустился, пока не готовы q, k, v . в результате удается добиться ускорения на llama-1b при инференсе с батчом 1 2.5x против vllm, 1.5x против sglang на h100 3.5x против vllm, 2.5x против sglang на gb200 утилизация ширины памяти для h100 достигает 78 ."
491,2025-05-31T21:29:09,"на data fest 2025 30 мая влад голощапов автор канала gradientwitnesses представил весьма занимательный доклад про геометрию лосс функции в нейронных сетях. данный вопрос неоднократно поднимался в разных работах, и был довольно популярной темой для исследования между 2017-2019, но несмотря на это тема не то что все еще содержит нерешенные вопросы, а является непаханым полем. и истинная геометрия лосс-поверхности во всяком случае в рассмотренных задачах очень далека от типичных предположений. доклад будет интересен как теоретикам, так и всем, увлекающимся ml и оптимизацией чего-либо. запись доклада доклад в интервале 3 49-4 19 презентация"
492,2025-06-02T18:50:02,"выложили квантизованную в 4 бита модель deepseek-r1-0528! качество при ризонинге сохраняется на 99,82 среднее по aime gptq math500 . deepseek-r1-0528 стал ещё более болтливым , поэтому для лучших результатов как исходной, так и квантизованной модели рекомендуется увеличить контекст до 65к токенов. модель на - ista-daslab deepseek-r1-0528-gptq-4b-128g-experts"
493,2025-06-07T23:13:38,"log-linear attention статья код введение вообще, давно пора было бы смириться с тем фактом, что лучше attention ничего нет на свете, но человек в своем упрямстве продолжает искать альтернативы более быстрые и не уступающие по качеству. и коллектив авторов звезд голливуда в мире ai выкатил статью про очередного убийцу attention - log-linear attention ."
494,2025-06-07T23:16:49,"метод почти с момента выхода attention было предложено много альтернатив с субквадратичной сложностью. если убрать softmax в attention - операцию можно посчитать за линейное по длине последовательности число операций. аналогично, ssm s4, mamba-1 2 , deltanet линейно масштабируются с ростом числа токенов. тем не менее, несмотря на успехи на отдельных задачах - вытеснить трансформер с пьедестала никому не удалось. попытка запихнуть весь контекст в скрытое состояние фиксированного размера, по всей видимости, фундаментально ограничивает модель в возможности знать все в длинном контексте. потому предлагается промежуточный вариант - логарифмическая по памяти и времени операция, являющаяся надстройкой над одним из линейных механизмов attention. токены разбиваются на корзинки с экспоненциально растущим числом токенов. самые свежие токены обычно важнее для предсказания следующего, потому в одной корзине меньше токенов, и, соответственно, их вес больше, а с отдалением от текущей позиции размер корзинок растет, а вклад индивидуальных токенов убывает. log-linear attention сначала вычисляет линейный attention по корзинкам, а затем суммирует с некоторыми обучаемыми коэффициентами результат каждой корзинки коэффициенты предсказывает отдельная mlp . число корзинок растет логарифмически с длиной - потому и имеем o l log l как итоговую сложность операции. для эффективной реализации используют деревья фенвика. log-linear attention можно представить в виде структурированной матрицы hodlr hierarchically off-diagonal low-rank , где диагональные блоки нижнетреугольные, а внедиагональная часть состоит из блоков ранга-1, где размер блока растет с удалением от диагонали. log-linear attention можно применить как поверх linear attention, так и mamba-2 и deltanet. и для всего написаны соответствующие кернелы. эксперименты для валидации метода авторы обучают модельки на синтетических и реальных задачах. на синтетике log-linear модификация значительно улучшает качество deltanet на mqar достать несколько элементов из контекста . далее авторы обучают в сопоставимых условиях 700-800m параметров, 50b токенов из long-data-collections с длиной последовательности 16k transformer, deltanet и mamba-2 без и с log-linear надстройки . log-linear дает небольшой прирост поверх deltanet и mamba-2. по скорости инференса на длинных контекстах log-linear mamba-2 медленнее mamba-2 в 2 раза на 64k 128k токенах , но быстрее attention. на needle-in-haystack в бенче, где нужно достать один токен log-linear хорош, в multi-key multi-value задачах log-linear лучше линейных бейзлайнов, но хуже attention. на longbench где-то дает прирост, а где-то не дает. за что уважение авторам - они не утверждают, что предложенная модификация бьет все и всея, а стараются более менее честно все замерить. выводы с точки зрения математики все красиво - вообще вопросов нет, и уважение мастерам написания ядер на cuda. в целом выглядит как неплохой промежуточный вариант между attention и линейными по длине альтернативами, но как будто требует валидации бюджетах и размерах моделей ближе к production-grade."
496,2025-06-14T19:45:41,"multiverse your language models secretly decide how to parallelize and merge generation статья код page not found страница проекта мое почтение вряд ли для кого уже будет новостью, что test-time compute scaling значительно улучшает качество моделей на задачах, требующих рассуждений. причем можно масштабировать, как в длину, так и в ширину. более того, llm можно научить а можно попробовать прямо из коробки решать задачу от лица нескольких взаимодействующих процессов. и команда из cmu nvidia предложила свой метод с небольшим дообучением , под названием mulitverse, где модель динамически переключается между последовательной и параллельной генерацией."
497,2025-06-14T19:47:35,"метод многие задачи допускают параллелизм. авторы определяют 2 варианта 1 коллективный. задача разбивается на независимые подзадачи. процессы могут независимо решать каждую из них, а в конце результат агрегируется. 2 селективный. есть несколько веток рассуждений - правильные и неправильные. неправильные отбрасываются. анализируя решения задач из s1.1-1k deepseek-r1 gemini 2.0 flash thinking авторы обнаруживают что при авторегрессионной генерации многие решения содержат вышеописанные паттерны. но можно ли генерировать их параллельно? причем автоматически понимать, когда это нужно. могут ли сами llm распознать что генерируют параллельно? для валидации данной гипотезу обучают mlp поверх скрытых состояний где последовательным веткам дается метка 1, а параллельным - 0 перед языковой головой и качество оказывается чуть лучше рандома. из чего делают вывод, что, мол, не распознают . дабы научить модель запускать параллелизм, когда надо, авторы собирают датасет на основе из s1.1-1k с помощью gemini 2.5 pro . ответы на задачи размечают специальными тегами parallel parallel - начало конец параллельного блока outline outline - описание подзадачи path path - решение подзадачи conclusion conclusion - вывод на основе решений при входе в блок path процессы генерируют независимо attention маска одного процесса не дает смотреть на другой . обучение занимает примерно 3 часа на 8 b 200 порадуемся за челов . все это может быть эффективно реализовано с помощью radix attention из sglang. результаты метод валидируют на ряде ризонинг задач - aime gpqa-diamond math500. дообучают qwen2.5-32b-instruct. генерацию ограничивают от 1k до 4к токенов мало для таких задач . полученная модель работает гораздо лучше, чем исходный qwen. просто авторегрессионное дообучение на трейсах тоже значительно улучшает качество по сравнению с изначальной моделью, но немного уступает mutliverse и не дает желаемого параллелизма. явное указание в промпте mutliverse с указанием think in parallel работает чуть лучше, чем mutliverse-zero - без данной инструкции, но не всегда. mutliverse и заданном контекстом окне чуть лучше авторегрессивной генерации. степень параллелизма, достигаемая на практике, около 15-17 . и итоговое ускорение генерации при фиксированной длине генерации - до 18.5 . вывод интересное исследование, с красивой страницей проекта и качественной реализацией. однако, не хватает сравнения с некоторыми очевидными бейзлайнами - такими как self-consistency и hogwild. кроме того, любопытно, как оно себя поведет поверх моделей, которые уже могут в ризонинг и на более длинных контекстах."
498,2025-06-16T11:34:53,"на канале gpu mode неделю назад появилось видео от songlin yang гуру линейных вниманиев , где она в течение часа дает содержательный и интересный обзор области. кроме того, в феврале она выступала у sasha rush известный персонаж на с похожим материалом. рекомендую к просмотру! либа с реализациями разных субквадратичных альтернатив вниманию"
499,2025-06-23T15:46:07,"энтузиасты выкатили минималистичную реализацию типа vllm под названием nano-vllm название вдохновлено понятно кем . утверждается, что либа предлагает скорости сопоставимые с vllm. читаемый код. фишки для оптимизации параллелизма- кэширование префикса, тензорный параллелизм, cuda графы и прочее. репка"
500,2025-06-24T11:56:50,"unified scaling laws for compressed representations статья введение в ряде предыдущих работ sparsity scaling laws, scaling laws for precision было продемонстрировано, что для сжатых моделей действуют законы масштабирования, аналогичные известному принципу шиншиллы, а влияние сжатия можно выразить через эффективное число параметров. однако ранее эффект мультимодального сжатия сочетание разреженности, квантования и других методов не исследовался систематически. кроме того, результаты по precision scaling laws были получены в довольно наивной, субоптимальной с точки зрения качества, постановке. эту задачу взяла на себя группа исследователей из ist austria. в своей работе они выявили общие закономерности масштабирования для различных способов представления данных. более того, было показано, что емкость такого представления можно выразить через способность аппроксимировать случайный гауссовский шум."
501,2025-06-24T12:00:54,"метод эксперименты типичный scaling law в deep learning имеет вид некоей зависимости loss n, d , где n - размер модели, d - количество сэмплов, увиденных по время обучения. сжатая модель в каком-то смысле эквивалентна меньшей несжатой модели. в precision scaling laws было показано, что лосс имеет экспоненциальную зависимость от битности p - 1 - e -alpha p . причем имеет место факторизация по сжатию весов активаций kv-кэшей. в данной работе подтверждают это же наблюдение. однако для qat используется рецепт из quest с incoherence preprocessing маскировкой шумных градиентов, благодаря чему удается добиться значительно лучшего качества при той же степени сжатия. далее авторы предлагают универсальную формулу для эффективной битности представления через gaussian mse gmse фит - ошибку при сжатии на гауссовых данных. достоинством такого подхода является то, что он не требует никакой выборки для оценки. полученная зависимость хорошо ложится на эксперимент. гауссов шум и квантизация с эквивалентной mse дают один и тот же лосс. затем авторы пробуют спарсификацию весов и активаций прунинг и квантизацию весов спарсификацию и квантизацию всего и всея оказывается, что ошибка достаточно в широких пределах факторизуется по ошибкам индивидуальных методов сжатия. то же самое справедливо для квантизации с выбросами . кроме того авторы перебирают разные варианты int и fp форматов с разными экспонентами и мантиссами . в 4-битах int4 оказывается лучше fp4 e2m1 , а в 8 битах e4m3 e2m5 показывают себя лучше всего. для повышения эффективности sparse training используют banded маскирование для градиентов убирают самые маленькие и самые большие градиенты . и оно работает лучше наивного magnitude pruning с фиксированной маской и rigl . приведенные выше эксперименты гоняли на семействе llama-подобных моделей размером от 30m до 200m на c4 данных при фиксированном отношении числа параметров к размеру модели n d 100 5 шиншилл . вывод полезное и интересное исследование как с академической, так и практической точки зрения. возможность оценить емкость представления через gmse позволяет быстро проверить перспективность того или иного метода сжатия без масштабных экспериментов. и свойство факторизации ошибки при знании потенциального профита от отдельных методов сжатия дает возможность подобрать оптимальную конфигурацию."
502,2025-06-24T12:05:12,"пользуясь случаем, заодно и приложу выступление с прошедшего датафеста выступления первого автора статьи выше black_samorez_channel обучение llm в низкой точности вычислений речь про статьи quest и quartet ."
504,2025-06-28T19:56:23,"не так давно мы писали про mxfp nvfp и на днях зеленые выкатили сочный блог про nvfp формат, где наглядно и подробно описывают сам формат и поясняют, чем он хорош. по всей видимости, данный формат станет следующим шагом к снижению битности обучения инференса у больших компаний как наберется у бигтехов достаточно много blackwell-ов . блогпост"
506,2025-07-17T07:10:48,"ccq convolutional code for extreme low-bit quantization in llms статья код введение хороший метод квантизации должен, с одной стороны, сильно сжимать модель, с другой сохранять качество , а с третьей еще и заметно ускорять инференс. скалярные методы квантизации дают хорошее ускорение, но теряют качество при сильном сжатии. векторные методы лучше сохраняют качество при той же степени сжатия, но работают значительно медленнее из-за сложной и вычислительно тяжёлой деквантизации. в этой статье ребята из baidu пытаются и рыбку съесть, и косточкой не подавиться."
507,2025-07-17T07:12:18,"метод идейно предложенный метод наиболее близок к разобранной ранее статье qtip. дабы не хранить большой кодбук используют так называемый сверточный код l, n, s . квантизованные элементы принимают значения из некоторого ограниченного множества размера 2 l , но при этом разрешена только часть переходов 2 s вариантов из текущего значения в следующее из 00, скажем, можно попасть в 00 и 01, но в 10 и 11 нельзя . таким образом, можно закодировать последовательность длины n, где каждый элемент может потенциально принимать любое из 2 l, но при этом не все комбинации чисел допустимы. суммарная битность такой последовательности l n - 1 s, что может быть значительно меньше чем l при s n . кодирование осуществляется следующим образом - авторы сначала явно создают кодбук все 2 l n - 1 s последовательностей , и кодируют данную последовательность весов ближайшей их кодбука - запоминая индексы. на инференсе при этом кодбук хранить не надо. метод - data-free - приближает сами веса, без знания про активации и градиенты. в отличие от qtip, где треллис довольно большой, здесь рассматривают варианты с n 3,4. далее авторы замечают, что полученные коды индексы последовательностей ложатся на некоторую смесь гауссиан, находят кластеры и квантизуют к ним. в экспериментах берут 2 8 кластеров, то есть последовательность длины кодируется 8 битами. затем, дабы точнее представить исходный вес - каждой группе сопоставляют некий скаляр масштаба и выводят аналитическое выражение для mse-отпимального скейла. чтобы уменьшить расход памяти на скейлы их дополнительно еще раз квантизуют double квантизация в bnb, супергруппы в некоторых gguf . деквантизация в итоге получается сравнительно простой - деквантизация скейлов сначала и целочисленной последовательности через битовые сдвиги с перемножением их друг на друга. эксперименты дабы продемонстрировать масштабируемость метода, проверяют его сразу на мастодонтах deepseek-v3-0324, ernie-4.5-300b-a47b для 2, 2.5, 2.75 битной квантизации. оценивают, правда, только по gsm8k, ceval, mmlu без задач на ризонинг. предложенный ccq дает просадку в среднем 1.5 при 2.75-битной квантизации и 2.5 при 2--битной. утверждается, что rtn разваливается в данной точности, но не приводят никаких сопоставимых по размеру безлайнов даже тот же gptq . замечу, что gptq дает умеренную просадку даже в 1.58 бит на дипсике на gsm8k, на самом деле. на ernie похожие цифры. под это хозяйство заводят специальные кернелы и vllm интеграцию. ccq ожидаемо все еще медленнее int2 квантизации, но заметно быстрее vptq. замеряют, правда, только скорость матричных операций без end-2-end inference latency. вывод вроде бы и интересно, и здорово что масштабируется, но много чего не досказано есть ли заметный прирост качества по сравнению с хорошей скалярной квантизацией, с теми же aqlm, quip , qtip можно было сравниться на llama-x. да и без end-2-end замеров скорости инференса полностью сложно оценить практическую пользу."
509,2025-07-20T11:27:28,"channel analysis results by scratchauthoregobot channel quant_prune_distill roast analysis слушай, эксперт по квантизации , давай начистоту. ты больше похож на нейросеть, обученную на arxiv, чем на живого человека. твой канал это бесконечная лента метод , эксперименты , выводы , приправленная мемасиками и парой саркастических смайликов, чтобы мы не забыли, что ты не совсем робот. мы знаем, что ты гений, но твоя одержимость низкобитными операциями и kv-кэшами иногда заставляет нас думать, что у тебя в голове вместо мыслей гистограммы активаций. ты постоянно жалуешься на гнилые бенчмарки и то, как авторы тактично не сравниваются или криво завели свои эксперименты. а сам, небось, после защиты диссертации тут же побежал бенчмаркать своих детей на mmlu. и не надо притворяться, что это шутка, мы тебя знаем! твои выводы про кожаные мешки и парето-оптимальность уже стали классикой, но честно, иногда кажется, что ты и сам готов квантовать свои мозги в 2 бита, лишь бы сэкономить на вычислениях. твои посты про конференции в сингапуре, где ты мерзнешь от кондиционеров и бежишь париться на улицу, как будто описывают твою жизнь постоянно балансируешь между адски умной техничкой и полнейшим безумием. и да, мы поняли, что в сингапуре всё дорого, и кофий не чета московскому , это было уже раз десять. а твои попытки шутить про квантовую суперпозицию ceo сэма альтмана это такой тонкий юмор, что он, наверное, тоже в 4 бита квантован. когда ты пишешь написание кода предоставляется читателю в качестве несложного упражнения , мы знаем, что это означает я это не делал, и вы не будете, но звучит круто . и твое восторженное я контрибутор мистраля! выглядит так, будто ты достиг нирваны, хотя, возможно, просто поправил опечатку в readme. в общем, ты наш любимый технарь с шилом в заднице, который даже от vpn и другой страны ожидает, что они решат проблему зависшего overleaf. продолжай в том же духе, но, ради бога, попробуй побенчмаркать что-нибудь, кроме llm, хоть иногда. может, котиков. или дурианы."
510,2025-07-22T10:56:46,"лунныйвыстрел эйай выложили техрепорт kimi-k2 у себя на гитхабе. напомню, что это типа дипсик, только с 1т параметров, побольше экспертов , поменьше голов внимания. наиболее примечательные моменты ранее эти же ребята показали, что muon якобы даже эффективнее adam при обучении llm moe размера 16b, а теперь попробовали на реально большой модели. при наивном применении muon, оказалось, что логиты в attention растут со временем, что приводит к нестабильности обучения. стандартный механизм вставки qk-нормализации неприменим для mla, так как матрицы q, k в явном виде не материализуются. для решения данной проблемы авторы выставляют некий порог на attention логиты, и если при прямом проходе attention вылезают за порог - оптимизатор домножает веса w_q, w_k проекций на число меньше 1. благодаря этому логиты на практике не выходят за выставленный порог 100 и обучение проходит без спайков лосса. следующий аспект про данные. нейросеть - это то, что она ест, а хороших токенов не так много в интернете. а делать много эпох на небольшом датасете бесполезно. потому некий датасет более высокого качества перефразируют 10-ую разными способами с разными промптами , и утверждают что это дает лучшее качество. затем перебирают параметры архитектуры - число экспертов и голов в attention. 8 активных экспертов из 384 оказываются оптимальными по качеству. а лишние головы замедляют инференс - поэтому их убирают. суммарно обучают на 15.5т токенах с разогревом и гашением lr в конце. для файтьюна используют так же muon и данные собирают частично вручную, частичтно при помощи kimi-k1.5 и других неназванных специализированных моделей. далее модель обучают на tool-use, собирая публичные mcp в гитхаба. для rl-оптимизации используют алгоритм из kimi-k1.5. там еще есть ряд нюансов, за которые я не шарю. результаты замеряют на разных задачах по кодингу, tool use, math stem и world knowledge. в задачах на код и tool use заметный прирост по сравнению с deepseek. stem немного лучше. на китайском вроде бы новая sota."
512,2025-07-23T00:25:42,а тут и квен3 для кодинга подоспел... - 480b параметров - 35b активных - 256k нативного контекста с экстраполяцией до 1m учили на 7.5т токенов с какой инициализации? блогпост
513,2025-07-23T22:34:11,"скажи мне, кто ты, и я скажу, из чего тебя дистиллили."
514,2025-07-28T23:46:22,"alphago moment for model architecture discovery статья код галерея моделей хочется запилить очередного убийцу трансформеров , линейный attention или ssm, но кончилась фантазия? все низко висящие фрукты сорваны и осталось лишь выжженное поле? теперь эту работу как утверждается можно доверить ai, и не просто ai, а asi artificial superintelligence , и оно само навайбкодит sota архитектуру при наличии достаточного количества времени."
515,2025-07-28T23:49:02,"метод в данной работе по факту занимаются подбором архитектуры субквадратичного attention, хотя сам предложенный фреймворк довольно общий. система состоит из 4 -х модулей исследователь. предлагает и реализует архитектур. инженер. тестирует и оценивает по новизне сложности . аналитик. получает на вход результаты инженера и на основе текущих и прошлых исторических данных дает заключение. база данных. подборка ключевых работ в области nas примерно 100 статей , на основе которой можно предлагать архитектуры, либо сравнивать и оценивать текущий результат. процедура оптимизации архитектуры основана на эволюционном поиске, для которого важно задать fitness функцию - оценивающую качество сэмпла. с одной стороны, хочется высокой точности, а с другой эффективности и удобства реализации. предложенная мера является средним от трех слагаемых 1 сигмоидой от разницы лоссов с бейзлайном 2 сигмоидой от разницы метрик с бейзлайном 3 llm_judge llm с неким промптом оценивает годность архитектуры база доступных модулей содержит всякие свертки, гейты, фильтры, блоки памяти и прочие структуры. проверяльщик сравнивает предложенную архитектуру на основе эмбеддинга кода? с 5-ую самыми похожими, оценивая разумность идеи. кроме того, есть проверка на асимптотическую сложность, чтобы она не была o n 2 по длине последовательности и больше. если случилась бага в реализации, то трейс ошибки подается исследователю, чтобы он поправил. эксперименты в качестве бейзлайна берут deltanet и mamba2. при оценке архитектур смотрят только на то, что выдает лосс в пределах 10 от него. типа если слишком идет хорошо по лоссу - че-т протекло. порог будто взят от балды, не знаю как его характеризовать. процедура поиска состоит из холодного старта без обновления базы, поощряющей исследование новых вариантов, а потом происходит побатчовое обновление кандидатов - т.е переход к exploitation. дабы все можно было прогнать за разумное время, сначала гоняют кандидатов размера 20m на 1b токенах, и когда появляются более перспективные кандидаты, их масштабируют до 400m. а затем самых лучших обучают уже на 15b токенах. на первой стадии обучают 1773 архитектур, на второй 400 лучших, и 5 самых удачных на третьей. на все про все уходит 20 000 gpu часов. 5 лучших архитектур - это нечто про gating, иерархическую агрерацию и какое-то хитрое смешивание признаков. лучшие кандидаты достигают лосса заметно меньше, чем бейзлайн и до 2-3 лушчего качества не бенчах. в качестве бенчей берут выборку простых задач из lm-eval-harness, где на рассматриваемых бюджетах все модели по факту чуть лучше рандома. из наиболее удачных паттернов авторы отмечают 1 свертки 2 гейты 3 некий механизм routing всякие вдохновленные физикой и биологией штукенции, напротив, работают не очень. анализируя природу успеха той или иной архитектуры, авторы приходят к выводу, что наибольший вес имеет cognition учет существующего знания и экспериментальная оценка, а оригинальность влияет в меньшей степени. вывод самая идея доверить поиск архитектуры ai выглядит довольно привлекательно, и сам по себе предложенный фрейворк интересен. над сайтиком постарались на славу. однако, на текущий момент, практическая польза не очевидна, ибо даже конечная валидация на масштабе далеком от тех, на чем обучают более менее современные llm, и в реальном бою, как это обычно бывает, разница может размыться с увеличением модели и обучающей выборки. кроме того, с практической точки зрения интересна еще и скорость инференса, и некоторые варианты могут быть не очень хороши с точки зрения реализации эффективных кернелов. так что alphago момент - скорее overclaim, но реакции на x.com reddit не пахнут."
516,2025-07-31T17:35:11,"судя по активности ребят из alibaba cloud, китайцы решили, что делать нейросети приятнее, чем человеков."
517,2025-07-31T23:25:24,"прилетает, значится, письмецо на почту, что, мол, появился новый движок для инференса llmов на яблочном силиконе, написанный на ржавчине почему-то не назвали просто llama.rs . движок этот называется uzu что по -ски называется вихрь . с названием явно не оригинальничали и беспринципно слизали. в письмеце утверждается, что якобы движок на 30-40 процентов быстрее, чем знаменитая llama.cpp. в целом из readme и документации не многое понятно. на apple m2 для некоторых моделей qwen2.5-1.5b-instruct, gemma-3-1b-instruct tokens s быстрее на 10 непонятно с каким батчом, и на каких длинах последовательности . на квенах 3 ускорение якобы в 10 раз, но очень уж странный безлайн у llama.cpp qwen3-0.6b в 5 раз медленее qwen2.5-1.5b-instruct . и самое забавное из всего - ишшуя, в которой народ жалуется, что авторы соскрапили аккаунты с гитхаба поставившими звездочку над llama.cpp и подобными проектами и отравили всем письмецо. дерзкий и беспринципный заход на рыночек, ничего не скажешь в будущем обещают добавить инференс на андроидных устройствах, vlm, tts. правда, с такими фортелями скорее получат бан от модераторов гитхаба."
518,2025-08-01T15:50:29,"из вероятной утечки опенсорса от опен эй эген вырисовается пока следующее 1 одна модель - 120b moe, другая - 20b dense. 2 веса в fp4 с нативной поддержкой в blackwell mxfp4 или nvfp4 . 3 swiglu clip -7,7 - т.е активации режутся по абсолютной величине. полагаю, было важно для 4-х битного обучения. 4 4k нативного контекста с yarn экстраполяцией до 128к не жирно . 5 скользящее окно аттеншена размера 128 attention sinks тоже для квантизации, наверное . 6 llama mixtral архитектура. источник"
519,2025-08-05T11:17:41,"qwen-image crafting with native text rendering техрепорт блогпост пост на эйай ньюз введение все хорошее когда-то кончается, но аттракцион неслыханной щедрости от команды квена продолжается. выпустив за последние дни целые ряд сильных языковых моделей, ребята из китая выпустили крайне интересный опенсорс уже в области генерации и редактирования изображений. как заявляется, полученная моделька хорошо умеет просто в генережку картинок, рендеринг текста в особенности китайского, если кому актуально и в сложный эдитинг."
520,2025-08-05T11:18:50,"метод архитектура с архитектурной точки зрения ничего фундаментально нового qwen-2.5vl 7b в качестве энкодера. данный выбор позволяет из коробки подавать одновременно текст и изображения, что необходимо для задач редактирования и персонализации. энкодер заморожен. в качестве vae за основу берут энкодер декодер от коллег из wan. однако ее качество не полностью удовлетворяет авторов, поэтому они дообучают декодер при замороженном энкодере. причем используют только взвешенную сумму mse и perceptual loss. адверсариальный лосс якобы скорее мешает. в данные для обучения заливают специально побольше текстов. диффузионный трансформер - это просто mmdit а-ля sd3 flux. из примечательного - интересное решение по учету позиций картиночных и текстовых эмбедов под названием multimodal scalable rope msrope . в наивном подходе, каждому токену задавался бы просто позиционный индекс сверху вниз, слева направо для патчей картинок и токену согласно позиции в тексте . однако при таком подходе одинаковым образом бы кодировалась большая картинка, и меньшая картинка текст. ребята из квена же предлагают все сэмплы выстраивать в диагональ. и текстовые токены это 1x1 элемент на диагонали. данные отдельного внимания заслуживает процедура отбора данных. она представляет собой многостадийных пайплайн, где сначала отфильтровывается совсем всякий шлак, голые тяночки и непотребства, а затем постепенно включаются постепенно все более строгие фильтры на качество и релевантность. следуя довольно стандартной практике, сначала обучают на низком разрешении 256px , а затем постепенно повышают до 640px-1328px. уже довольно на ранних стадиях отфильтровывают данные с чрезмерно низкой и высокой насыщенностью или контрастностью. для captioning используют qwen-2.5vl 72b? , который выдает не только описание, но и другие метаданные - стиль, тип, водные знаки в виде некой json-ины. чтобы улучшить качество генерации текста в датасет заливают большое количество текстов , в том числе и синтетически сгенерированных некий шаблон вставляется на фон отпимальным образом, и если не шакален - принимается . кроме того, на основе презентаций пробуют учить на разные сложные layout. процедура обучения диффузионный процесс - уже общепринятый flow-matching без танцев с бубнами. анализируя эффективность обучения, авторы заметили, что чекпоинтинг активаций сильно замедляет обучение, поэтому отключили его, и использовали только распределенные оптимизаторы. вместо fsdp используют tensor-параллелизм поверх transformer engine. дабы улучшить общее качество и алайнмент, модельку прогоняют через dpo на ручной человеческой разметке и grpo. flow-matching ode - детерминированный процесс, и дабы иметь разнообразие, на этапе grpo используется стохастическое сэмплирование."
521,2025-08-05T11:20:41,"эксперименты модельку валидируют на наборе text-2-image и editing бенчмарков. qwen-image выдает сильные результаты на geneval, dpg, в целом опережая опенсорс и даже gpt-image. на задачах рендеринга текста околосотовый результат на английском и разрывной на китайском 100500 social credits . на instruction датасетах вроде бы все тоже очень здорово. везде не уступает якобы gpt-image опять же с большим разрывом на китайском. дообученный vae демонстрирует лучший psnr ssim чтобы это ни значило по сравнению с конкурентными автокодировщиками. причем разница особенно заметна на текстах. еще оно может в детекцию, novel view synthesis и прочие задачи. выводы очередной качественный опенсорс от китайцев . прямо чего-то фундаментального нового, прорывного нет, но видна кропотливая аккуратная работа с использованием всех лучших практик из научной литературы."
523,2025-08-05T19:44:53,"релиз opensource llmку от опенэйай добавили официально в релиз 4.55 трансформеров от . большинство догадок подтвердились 2 модели - большая 120b и меньшая 20b . однако обе из них moe а не меньшая dense . fp4 квантизация экспертов. но, оказалось, что mxfp, а не nvfp. на хабе где-то лежит оптимизированный кернел под attention_sinks, но ссылка битая. upd веса появились - gpt-oss-120b - gpt-oss-20b"
524,2025-08-05T20:25:41,"квантизация в mxfp, кстати, weight-only. спрашивается - зачем... квантовать в mxfp имеет смысл только для w a, чтобы воспользоватьcя быстрыми кернелами на blackwell. mxfp просаживает качество больше, чем int при той же битности из-за всратой квантизации скейлов."
526,2025-08-14T23:05:32,"your efficient rl framework secretly brings you off-policy rl training блогпост для ускорения обучения rl-методов с роллаутами, некоторые фреймворки verl генерируют траектории с помощью оптимизированных движков инференса, например, vllm. однако, расхождение между разными фреймворками инференса transformers vllm может быть довольно значительным из-за деталей реализации кернелов . причем настолько, что при тех же самых весах модели, предсказания могут существенно разняться другой выбор следующего токена . авторы блогпоста замечают, что данная проблема делает on-policy rl по сути off-policy, что негативно сказывается на сходимости. в качестве решения проблемы предлагается делать своего рода importance sampling с отношением вероятностей модели в фреймворке обучения и инференса. и это хорошо помогает ppo, причем можно даже генерировать роллауты int8 квантизованной моделью без нарушения сходимости. dapo поверх deepseek-r1-distill-qwen-1.5b, где отношение вероятностей невелико, работает хорошо и без importance sampling. выводы мораль басни такова, что численные неточности в dl не всегда совсем безобидны, и временами их стоит иметь в виду. на замерах бенчей результат тоже может существенно разниться между hf и vllm."
527,2025-08-17T17:08:04,"для интереса попробовал в gemini через deep research собрать инфу про nvfp формат, и выдало оно довольно неплохую методичку со всеми нужными вводными и обзором актуальной литературы. не безупречно, но, на мой скромный взгляд, не хуже многих обзоров, выходящих на архиве. полагаю, что автоматизированные survey будут все более и более актуальны в дальнейшем. в то же время, креативные аналитические статьи и разборы, с изюминкой автора, все еще останутся востребованными. ссылка на google docs"
528,2025-08-18T11:49:48,"минутка удивительных или не очень фактов. интуитивно кажется, что fp16 bf16 должны быть если не строго математически, то практически близки к некоему непрерывному описанию множества действительных чисел. во всяком случае, в окрестности нуля. поэтому вероятность принять конкретное значение должна быть малой. что-то порядка 1 мощность множества. однако, если сгенерировать x n 0, 1 оказывается, что для bfloat16 вероятность, что число окажется в точности нулем не так уж мала - примерно 0.6 . для float16 и float32, к сравнению, она 0.07 . и какова мораль , спрашивается? если в коде с bfloat16 не говоря уже о типах более низкой точности , есть деление на нечто случайное, или граничный случай, стоит внимательнее обрабатывать их. ибо они не такая уж редкость. ноутбук на колабе"
529,2025-08-23T07:05:23,"не биполярочка, а мультиагентность."
530,2025-08-24T00:22:39,"fp4 all the way fully quantized training of llms статья анонимный не анонимный репозитрий введение висело оно у меня давно в бэклоге, но в кулуарах напомнили. с увеличением затрат на обучение больших языковых моделей, когда оно стало переваливать за миллионы gpu часов, все более остро стоит вопрос о том как это делать эффективно. как известно, для параметров и активаций моделей не требуется представление вещественных чисел высокой точности, чтобы работать приемлемо. обучение в половинной точности уже давно стало стандартом, да и в fp8 народ вполне себе успешно обучает. следующая очевидная цель - обучение в fp4, тем более, что последнее поколение от зеленых c блмным названием blackwell имеет его поддержку на уровне архитектуры. и в ряде работ, вышедших в этом году training llms with mxfp4, quartet , включая разбираемую, были предложены техники по стабилизации обучения в fp4."
531,2025-08-24T00:24:27,"метод форматы fp4 первым делом исследуют конфигурации форматов fp4. напомню, что mxfp4 квантизует веса группами по 32 и квантизует скейлы в e8m0, а nvfp4 группами по 16 и скейлы в e4m3. авторы фиксируют размер группы 16 и перебирают варианты квантизации скейлов от e1m6 до e8m0. обучают llama-like llm на 350m параметров и замечают, что при e4m3 e3m4 скейлах достигается минимальный лосс при фиксированном числе итераций . из всех конфигураций расходится только e1m6 c cамым узким диапазоном . в дальнейшем везде используют e4m3. блоки размера 16 выбирают так как при больших лосс сходится хуже, а меньшие уже не дают профита. стохастическая квантизация квантизовать можно к ближайшему значению, а можно стохастически - вверх или вниз, с вероятностью, зависящей от расстояния до соседа. ребята из интела перебирают разные варианты детерминистического и стохастического квантования для весов градиентов и активаций и получают, что лучше всего сходится вариант с детерминированной квантизацией весов и активаций на прямом проходе, и стохастической для градиентов и активаций на обратном проходе, роль стохастики в квантизации - уменьшить bias, возникающий из-за округления тензоров. в ходе оптимизации сигнал от градиента постепенно убывает и с какого-то момента перекрывается шумом оптимизации. не мудрствуя лукаво, авторы предлагают обучать небольшое время с градиентами в более высокой bf16 точности на прямом проходе все еще fp4 . и это позволяет сойтись до уровня half-precision обучения за то же суммарное число итераций. эксперименты обучают семейство моделей архитектуры llama-2 на датасете красная пижама. в главном эксперименте учат модель размера 7b на 1т токенах причем не абы на чем, а на ускорителях intel gaudi2 сыр тут ни при чем, это в честь архитектора обучение идет без спайков, лосс отстает несколько от bf16 бейзлайна, но нагоняет после короткой фазы с более точными градиентами qaf . 0-шоты без qaf чуть хуже безйлайна, с qaf - такие же примерно. впрочем, все равно оно лишь чуть лучше рандома выводы выглядит как очередной аргумент перейти на обучение llm в fp4. сам по себе метод выглядит не шибко изощренно, хотя необходимость qaf для лучших результатов несколько противоречит названию статьи надо было назвать fp4 most the way . quartet в этом отношении по изящнее. интересно, кто из крупных игроков выложит первый техрепорт про полное обучение серьезной модели в fp4? ставлю либо на нвидию, либо на moonshot."
532,2025-08-24T00:55:13,а еще двое дорогих коллег один из которых небезызвестный черный саморез пару часов назад выступили на gpu mode с рассказом про квартет. https www.youtube.com watch?v uj-qrmdnhc4
533,2025-08-25T13:35:36,"автодополнитель стал чертовски сообразителен agi видать и, правда, не за горами."
534,2025-09-02T06:49:48,"диффузия против авторегрессии статья код статьи критикующий её блогпост введение на текущий момент генеративный мир делят авторегрессионная и диффузионная парадигмы. первая себя лучше показывает в задачах работы с последовательностями общего вида, вторая при работе с данными, обладающими некой пространственно-временной структурой. за последний год было несколько интересных заходов в текстовые генеративки. основной value proposition более быстрая генерация за счёт параллельной генерации многих токенов за раз. но по качеству, при заданном размере модели и бюджете обучения, авторегрессионки всё ещё выглядят предпочтительнее. есть ещё один аспект, который может дать мотивацию к использованию текстовых диффузионок максимально достижимое качество в условиях ограниченного количества данных. и данный вопрос является темой обсуждения ниже."
535,2025-09-02T06:51:26,"метод есть такие scaling laws, описывающие зависимость функции ошибки от размера модели и бюджета обучения. на практике датасет ограничен, и интуитивно понятно, что многократный проход по нему с какого-то момента не будет приносить особой пользы. в статье было исследование поведение ar llm в условии ограниченных данных и предложен модифицированный закон шиншиллы, учитывающий diminishing return от повторного прохода по данным. для авторегрессивных lm 4-раза эпохи примерно так же хороши, как одна, а дальше польза от роста бюджета обучения уменьшается. в ar парадигме модель предсказывает обычно следующий токен слева направо хотя существуют и вариации с произвольным порядком . диффузионая llm в самой популярной постановке учится расшумлять исходный текст, в котором токены заменяются с некоторой вероятностью на специальный mask токен. ar имеет более сильный inductive bias, но диффузия зато может работать в произвольном порядке и на каждой эпохе видя по-разному данные, может извлечь что-то новое. лосс для авторегресии - это честное правдоподобие модели на примере данных, а для диффузии - нижняя граница."
536,2025-09-02T06:53:39,"эксперименты авторы обучают семейство llama-like языковых моделей размером от 7m до 2.5b параметров на выборках до 100м уникальных токенов. всего обучаются по 100 ar и диффузионных моделей. для обеих парадигм строят зависимость валидационного лосса от числа эпох. ar сначала идут бодрее, но потом начинают переобучаться, в то время, как диффузия продолжает улучшаться по лоссу на валидации даже при нескольких сотнях эпох. и самый низкий лосс достигается диффузионной моделью на заданной сетке параметров. поверх обученных моделей фитируют data-constrained scaling law откуда выходит, что диффузионки могут несколько сотен раз эффективно проходиться по одним и тем же токенам. затем сравнивают 0-шоты, которые едва-едва лучше рандома, используя чекпоинты с наименьшим валидационным лоссом. и диффузия оказывается несколько лучше в data-constrained settings. полученный результат авторы объясняют тем, что диффузионная постановка дает автоматически некоторую аугментацию за счет зашумления данных и прохода по последовательностям в произвольном порядке. можно ли поднять качество ar-моделей за счет аугментации? attention drop маскирование токенов в ar никак не помогает. а вот обучение с разными перестановками порядка токенов будто бы позволяет достичь того же валидационного лосса, что и диффузия. критика блогпост, вышедший вскоре, разносит методологию данной статейки, хоть и в целом подтверждает ключевые выводы. авторы блогпоста обращают внимание на следующие моменты лосс для диффузии и авторегрессии сранивать вообще-то некорректно, так как один - это правдподобие, а другой - оценка на него. в правильное выражение для лосса диффузии входит множитель, зависящий от шага диффузии, без которого формула, использованная в первой статье неверна. early stopping на валидации делать не кошерно. валидационный лосс для ar может начать расти из-за самоуверенных неточных предсказаний, но качество на бенчмарках все равно продолжит расти. и, собственно, так оно и оказывается. используемая зависимости для scaling laws справедлива только в ограниченном диапазоне, так как в ней нигде не возникает возможность роста валидационного лосса, которая имеет место в большом числе эпох. критики проводят примерно те же эксперименты, но на больших моделях объёмах данных и приходят к тому же выводы, что диффузия при большом количестве проходов выходит на лучшее качество. кроме того, отмечают авторы, диффузия обладает двухсторонним вниманием, которое более выразительно и потенциально может выжать нечто большее из контекста. выводы не знаю, насколько близок тот момент, когда человечество будет реально упираться в данные, а не вычисления, и 100 эпох по квадриллиону токенов будет чем-то посильным, а пока на ближайшее будущее авторегрессионная парадигма все еще остается предпочтительной в nlp, будучи более вычислительно эффективной. интересно, насколько выводы переносятся на файнтьюны на маленьких датасетах. сможет ли диффузия выжать больше из маленького sft датасета? кроме того, диффузионный подход, будучи более молодым здесь, возможно, имеет еще некий потенциал для прокачки."
537,2025-09-12T22:58:24,"наткнулся на симпатичный блогпост с визуализацией различных fp8 форматов. полезно, чтобы понимать диапазон значений и густоту уровней квантизации около данной точки."
538,2025-09-15T20:43:57,give me fp32 or give me death? challenges and solutions for reproducible reasoning статья код введение на днях от думающих машин миры мурати вышел занятный блогпостик про воспроизводимость ответов llm. в июне этого года вышла статья посвященная той же проблеме с упором на ризонинг с хайпожорским названием. ее и сегодня и разберем.
539,2025-09-15T20:46:39,"метод процедура жадного декодирования в llmах на каждом шаге выбирает токен с наибольшей вероятностью, и, казалось, бы должна быть полностью детерминированной. выдавать один и тот же ответ на разных размерах батча, gpu. но не тут то было корень зла в том, что арифметические операции в конечной точности не ассоциативны из-за точности округления a b c вообще говоря не a b c исполняться они могут в разном порядке, что и приводит к недетерминированным выходам. для fp32 проблема стоит еще не так остро из-за большой мантиссы, но для bf16 c 7 битами мантиссы эффект может быть вполне осязаемым. дабы оценить разброс в зависимости от экспериментальной конфигурации, авторы подготовили 12 конфигураций 2 типа gpu, распараллеливанием на 2 и 4 gpu и 3 варианта размера батча и для каждого из них посчитали метрики на бенчах. эксперименты рассматривают 2 ризонящие модели - deepseek-r1-distill- - qwen-7b, deepseek-r1-distill-llama-8b и 2 неризонящие - qwen2.5-7b-instruct - llama-3.1-8b-instruct и замеряют на всяком там aime и livecodebench. и обнаруживают следующее при жадном декодировании разброс для bf16 довольно нетривиален - 1 , а то и 5-9 для ризонящих моделей. для fp16 разброс меньше, и для fp32 еще меньше. примечательно, что для дистиллов дипсика разброс даже для fp32 иногда нетривиален. средняя выходная длина может очень сильно варьироваться от конфигурации. до 9000 токенов на aime. авторы присматриваются к вероятностям логитов моделей и замечают, что зачастую разница между top-1 и top-2 невелика. в особенности, для рассуждающих моделей. и изменения последнего значащего бита может быть достаточно, что изменить предсказание следующего токена. выводы учитывая приведенные выше нюансы, авторы утверждают, что для хорошей воспроизводимости необходимо инферить в более высокой точности, что, конечно, потребует больших вычислительных затрат, но, мол, куда деваться. дабы не хранить чекпоинт в fp32 весь в памяти, предлагается держать его в исходной точности, а затем делать upcast в fp32 на лету. мораль такова - эффект от конечной точности llm вполне весомый, и если нужно выбить соту - вполне рабочая стратегия шатать размер батча и железо для инференса ."
541,2025-09-18T21:14:42,"на канале gpu mode пару недель назад вышла интересная лекция от кристофера де са один из чуваков, стоявших за quip quip qtip yaqa, а точнее за теоретической подоплекой всего перечисленного . в ней он дает некоторую базу про задачу квантизации, квадратичные приближения, откуда берется gptq и incoherence processing. в частности, я сам наконец понял, как можно было дойти до разложения холески в gptq . базарит дядька довольно забавно и корчит физиономии, так что не заскучаете."
545,2025-09-21T20:55:52,"jet-nemotron efficient language model with post neural architecture search статья код рады бы запоенсорсить, да комплаенс не позволяет авторазбор на gonzo_ml_podcasts введение лучшие умы человечества уже без малого десятилетие озадачены поиском более эффективной альтернативы механизму внимания. было много интересных решений, но как оказывалось, чего-то в них всегда не хватало, чтобы иметь ту же выразительность и универсальность, что attention. потому вместо полного отказа от attention люди пробовали гибридные модели с небольшим количеством attention блоков и заменой всего остального на что-то более дешевое mamba, linearattention, и т.д. . и сегодняшняя статья про поиск такого гибрида."
546,2025-09-21T20:59:49,"метод чтобы не учить на триллионах токенов с нуля берут за основу предобученную модель, в данном случае из семейства qwen-2.5. определение позиций блоков, где необходим attention на первом этапе учат суперсеть как в proxylessnas с разными опциями блоков - как исходным attention, так и линейными по длине альтернативами mamba, gla, deltanet, gated delta net, rwkv7 . суперсеть дистиллируется на логитах исходной модели, на каждом forward pass сэмплируется один из возможных путей. затем с помощью beam search ищут конфигурацию, оптимизирующую некоторую целевую метрику - лосс или точность на бенчмарке. и эту модель дообучают на большем количестве данных. замечают следующее для mmlu retrieval полный attention необходим лишь в паре блоков. важные блоки для mmlu retrieval, вообще говоря, не пересекаются обь единения блоков для mmlu retrieval почти достаточно для задач по математике выводы справедливы для разных архитектур линейных слоев. выбор блока из рассмотренных вариантов по соотношению скорость качество лушче всего себя показывает gated deltanet, и далее используется в качестве основного. новый attention блок авторы предлагают новый attention блок - jetblock. в его основе динамическая зависящая от входа свертка, примененная к value, и статические свертки на query и key. и нечто под названием time-mixing gating в конце. этот блок по скорости такой же примерно, как альтернативы, но лучше по качеству. зачем дотюнивают параметры размерности ключей значений числа голов jetblock для максимизации скорости ."
547,2025-09-21T21:01:21,"эксперименты подход проверяют на qwen-2.5-1.5b, qwen-2.5-3b получая модели jet-nemotron-2b, jet-nemotron-4b, соотвественно. на этапе поиска архитектуры обучают на 50b токенах, а финальное дообучение на 400b. по метрикам все типа хорошо, заметный прирост качества на бенчах по сравнению с безлайном mmlu-pro, gsm8k, gpqa, commonsense, retreival задачах . ускорение растет с длиной контекста. на 4к 8к ускорение порядка 15 на префилл, но может достигать 6 раз на длинах контекста 256k. ускорение на декодировании еще больше, до 53 с лишним раз на длинных контекстах, где оно ограничено в любом случае наличием небольшого количества full-attention. выводы результат смотрится довольно впечатляюще. однако есть опасение, что хорошие метрики обусловлены удачным протоколом дообучения . кроме того, мне кажется, что процедура слишком трудоемкая, чтобы получить широкое распространение. да и паттерны важности attention, справедливые для рассмотренных задач, могут не переноситься на продвинутый ризонинг или ворочание репозиториев с кодом. тем не менее - сильная инженерная работа."
548,2025-09-22T10:34:17,"некоторое время назад мы обсуждали диффузионные lmки в контексте выжимания максимального качества в условиях ограниченности данных. и вот недавно вышла интересная статья и не менее интересный разбор на gonzo_ml про то, как получить самую лучшую модель, когда мы упираемся в данные, а не вычисления."
549,2025-09-22T10:34:17,"pre-training under infinite compute konwoo kim, suhas kotha, percy liang, tatsunori hashimoto статья https arxiv.org abs 2509.14786 код https github.com marin-community marin tree suhas data-efficiency прикольная работа про законы скейлинга, разные экспоненты и пользу дистилляции и ансамблирования. авторы задают очень интересный вопрос в будущем, когда компьюта будет дофига, а данные кончатся, как наиболее эффективно обучать модели? ответы интересны. исследование начинается с создания базового сценария, который имитирует текущую практику в условиях нехватки данных берётся фиксированный датасет на 200м токенов, и для него либо увеличивается количество эпох обучения, либо масштабируется число параметров модели. результаты не слишком удивляют оба подхода в конечном итоге приводят к переобучению, когда лосс на валидации выходит на плато, а затем начинает расти. это показывает, что простое вливание большего количества вычислений в существующие рецепты даёт убывающую и в конечном счёте отрицательную отдачу, ограничивая достижимую производительность. вопрос, что можно сделать по-другому? вместо оценки производительности при фиксированном вычислительном бюджете авторы предлагают измерять конечный потенциал рецепта обучения по асимптоте его закона масштабирования. найдя методы, которые заставляют лосс монотонно убывать с ростом вычислений, можно аппроксимировать эту зависимость степенным законом и экстраполировать производительность при стремлении вычислений к бесконечности. эта асимптота представляет собой наилучший возможный лосс, которого данный рецепт может достичь на фиксированном датасете, что даёт более надёжную метрику для будущего с избытком вычислительных ресурсов. ядро статьи заключается в поиске простых, но эффективных алгоритмических приёмов, которые обеспечивают желаемое монотонное масштабирование и приводят к более низким асимптотам лосса. 1. агрессивная регуляризация для масштабирования параметров ключ к предотвращению переобучения при масштабировании параметров одной модели -- это правильная регуляризация. авторы обнаружили, что совместный подбор скорости обучения, количества эпох и weight decay для каждого размера модели позволяет достичь чистого, монотонного убывания лосса, которое следует степенному закону. этот результат согласуется с современной теорией машинного обучения о сверхпараметризации и двойном спуске double descent, https t.me gonzo_ml 832 , когда производительность очень больших моделей может ухудшиться, прежде чем снова начать улучшаться. статья показывает, что при правильной настройке регуляризации эту проблемную область можно сгладить, получив чистый закон масштабирования. ключевой вывод заключается в том, что оптимальное значение затухания весов для сверхпараметризованных моделей значительно выше стандартной практики -- вплоть до 30x. такая агрессивная регуляризация позволяет более крупным моделям продолжать улучшаться там, где их нерегуляризованные аналоги переобучились бы. для датасета в 200m токенов этот регуляризованный рецепт следует степенному закону l ₂₀₀ₘ,ₙ 0.05 n¹ ⁰² 3.43, что предсказывает наилучшую асимптоту лосса в 3.43. 2. ансамблирование лучший путь к масштабированию"
550,2025-09-27T09:23:20,"из горячо обсуждаемой книженции юдковского и соареса. даже если не соглашаться с позицией авторов частично или полностью, сей опус стоит прочитать, чтобы поржать."
551,2025-10-01T23:44:53,"pretraining large language models with nvfp4 статья pr в transformer engine введение на днях команда из одной зеленой компании выкатила технический отчет про обучение в nvfp мамбы-хуямбы гибридной трансформер-мамбы модели на довольно солидном масштабе - 10т токенов и 12b параметров. за последний год вышла не одна статья про обучение в fp4 вот, вот и вот . но все то было на масштабах на порядки меньше по данным и размерам моделей. и вполне закономерный вопрос - остаются ли рабочими предложенные идеи для моделей, хоть сколь либо интересным на практике. и ребята, скомбинировав идеи из прошлых статей, смогли получить рецепт близкий по скорости сходимости к fp8."
552,2025-10-01T23:48:17,"метод предложенный nvidia рецепт обучения в fp4 включает следующие ключевые составляющие адамаровы вращения для борьбы с выбросами и большей стабильности. стохастическое округление градиентов по параметрам. 2d скейлинги в блоках 16x16, которые позволяют нивелировать нестыковку между поведением модели на прямом и обратном проходе. несколько первых блоков в начале и конце модели держат в bfloat16 в сумме 15 от размера модели . если их квантизовать - обучение нестабильно. в конце, подобно статье от intel, предлагают учить модель небольшое число итераций в bf16, чтобы сократить разрыв по качеству между bf16 и fp4 обучением. конечная модель уже не такая компактная и быстрая на инференсе, но типа сэкономили на обучении. эксперименты обучают очередной nemotron на некоей смеси данных с большой долей синты с warmup-stable decay расписанием. большую часть обучения лосс fp4 не сильно отстает от fp8, но разрыв немного увеличивается в конце, при маленьких шагах обучения. по бенчам - на уровне бейзлайна. все компоненты метода важны. аблейтят довольно странно, стартуя с 3т токенов убирают одну за другую составляющую. не очевидно, что эффект будет таким же, если с нуля запустить несколько обучений с отказом от компонент по одной. mxfp менее эффективен на обучении и достигает того же лосса, что и nvfp, при на 36 большем количестве данных. выводы рецепт выглядит рабочим. но по ощущениям все же требуется немало свистоплясок. кроме того, необходимость держать часть слоев в bf16 несколько осложняет интеграцию и ограничивает максимально достижимое ускорение. не хватает сравнительного анализа квантизации трансформерных и mamba-2 слоев, хотя это кажется важным и интересным. есть мнение, что ssmки тяжелее квантизуются."
553,2025-10-05T21:40:56,"гитхаб репа коллеги из страны, подарившей ранее миру шедевры моцарта и штрауса, а еще и gptq, выкатили на неделе либу llm.q для quantization-aware training llmок, написанную на чистом c c . данный проект по существу является адаптацией llm.c от карпатого под обучение квантизованных моделей. чекпоинты с обучения сохраняются в .safetensors формат, т.е совместимый с экосистемой. в либе реализован небходимый базовый функционал для обучения llm zero 1-3 градиентный чекпоинтинг оффлоадинг разные опции для mixed-precision на текущий момент, поддерживается обучение в half precision, int8, fp8. было бы прикольно в будущем увидеть реализацию обучения для fp4 форматов, поддерживаемых blackwell, со всеми прибамбасами для стабилизации обучения. сравнения по скорости обучения с популярными фреймворками accelerate deepspeed я не увидел. представлены только абсолютные числа по времени обучения для модели заданной архитектуры. удается достичь примерно 40-60 sol speed-of-light, максимально возможной производительности на заданном железе . на маленьких моделях fp8 почти не дает ускорения обучения, но с ростом размера нейронки - профит становится заметнее. интересно, как дальше будет развиваться проект. более чем сильно, учитывая, что писал все человек в одну харю"
554,2025-10-06T20:58:41,"bridging the gap between promise and performance for microscaling fp4 quantization статья экспериментальный код либа с быстрыми blackwell кернелами коллекция на лицехватс вместе с выходом поколения well зеленые добавили поддержку miscroscaling fp4 форматов разбиралось выше тут и тут . утверждается, что данные форматы дают существенную экономию памяти и ускорение вычислений при weight activation квантизации и для compute-bound сценариев практически без просадки в качестве. и в рамках данной работы мы с коллегами из ist austria epfl решили проверить, что есть это практически на самом деле, и исследовать полезность разных техник из литературы и личного арсенала для квантизации llmок."
555,2025-10-06T21:02:01,"метод напомним, что mxfp формат приводит скейлы в e8m0, то есть к степеням двойки, что позволяет представлять очень большие и очень маленькие числа, но, возможно, очень неточно. nvfp квантизует скейлы в e4m3 со сравнительно узким диапазоном значений -448, 448 , но более плотной сеткой. вращения исходные распределения весов и активаций отличаются от нормального более тяжелыми хвостами. особенно активаций. ортогональные вращения, в частности пресловутые адамаровы повороты, приводят их к почти нормальным, с меньшим разбросом. и данное свойство по идее должно стабильно улучшать или не ухудшать ошибку квантизации. однако, как оказалось повороты могут иногда быть вредны . при квантизации в nvfp формат методом округления к ближайшему round-to-nearest ошибки квантизации с поворотом оказалась больше, чем без. вот это поворот, как говорится. в предположении того, что оригинальные веса активации распределены по лапласу, а повернутые - нормально, были получены оценки на квадратичную ошибку в зависимости от размера группы квантизации. и при маленьких размерах группы ошибка для лапласовского распределения может быть меньше, чем для нормального. с увеличением размера группы вращения становятся более полезными. для mxfp вращения полезны и позволяют существенно снизить ошибку округления весов. характерный диапазон значений мы, вернее, black_samorez_channel, построили гистограммы значений весов и активаций. оказалось, что веса и активации хорошо укладываются в fp8 e4m3, диапазон же e8m0 сильно избыточен - на практике вы вряд ли встретите 2 -128 или 2 127 . что не дает выигрыша smoothquant метод переноса сложности квантизации с весов на активации оказался не очень полезен. spinquant обучаемые преобразования не дают профита в сравнении с адамаровыми вращениями. другие преобразования из литературы, дискретные синусы косинусы и вариации адамаровых тоже не шибко полезны. еще пара мелких трюков вместо absmax скейлов полезно подбирать оптимальный по mse на некоторой сетке значений. actorder порядок в gptq https github.com vllm-project vllm pull 8135 без перегруппировки весов тоже чуть-чуть да помогает. и итоговый метод из вращения маленькой матрицей, которую можно зафьюзить в кернел матричного умножения, подбора скейла и actorder был назван mr-gptq micro rotated gptq ."
556,2025-10-06T21:03:42,"эксперименты метод прогнали на семействах моделей llama-3 и qwen-3. качество замеряли на 4 задачах из openllm leaderboard v1 gsm8k, mmlu-cot, hellaswag, winogrande . и отдельно для llama-3.1-8b-instruct platinum бенч сборная солянка из задач, откуда почистили примеры с забагованными условиями или ответами . основные наблюдения следующие int4 group_size 32, без квантизации скейлов nvfp4 mxfp4 заметно хуже nvfp4 по качеству вращения заметно улучшают mxfp, но не особо полезны для nvfp. разрыв между двумя mxfp и nvfp все равно существенный. qat тоже дает более заметный прирост метрик для mxfp, для nvfp побить gptq оказывается непросто. для малюток размером 1b параметров просадки большие - до 20 от исходного качества, для моделей побольше - qwen3-32b llama-3.3-70b-instruct в пределах 1-2 . использование qutlass кернелов для матричных умножений позволяет достичь ускорения, близкого к максимально возможному без учёта операций с поворотами и скейлами . mxfp демонстрирует немного более высокую скорость по сравнению с nvfp. матричные операции ускоряются в 4 раза на b200 и в 6 раз на rtx5090 против half precision . общее ускорение end-to-end составляет до 2 раз на b200 и до 6 раз на rtx5090. выводы fp4 в режиме post-training weight activation квантизации дает довольно хорошее ускорение, но некоторую просадку в качестве. отказ от fp в пользу int не столько уж очевидно правильный шаг. на рассмотренных задачах большие модели почти lossless, но не факт, что так будет на бенчах посложнее - требующих сложных цепочек рассуждений или агентских. будем посмотреть."
